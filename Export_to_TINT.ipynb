{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38378a04",
   "metadata": {},
   "source": [
    "# Convert recording and sorting extractor data to TINT format\n",
    "\n",
    "The Hussaini lab uses the proprietary TINT software from Axona to analyze extracellular electrophysiology data. While we are already able to read various data formats from Axona (`raw` data or `unit` data) into spikeinterface, perform preprocessing, spike sorting and export the data to NWB, we also want to allow to export data to the TINT format. \n",
    "\n",
    "The TINT format is essentially the same as the `unit` data, including `.X` and `.pos` files, but also `.cut` or `.clu`. The latter two contain information about the spike sorted units.\n",
    "\n",
    "The conversion can be facilitated by using the existing tools from the Hussaini lab, which [convert `.bin` data to `.X` and `.pos`](https://github.com/HussainiLab/BinConverter/blob/master/BinConverter/core/ConversionFunctions.py). Some of this code is only relevant for using the GUI, which did not work for me. I cleared out GUI code and ran a conversion from `.bin` to `.X` and `.pos` in this notebook: [explore_hussaini_tools.ipynb](https://github.com/sbuergers/hussaini-lab-to-nwb-notebooks/blob/master/explore_hussaini_tools.ipynb).\n",
    "\n",
    "They also already wrote a [`write_cut()`](https://github.com/GeoffBarrett/gebaSpike/blob/967097ec28592182ef9783d2d391930e1c63ca58/gebaSpike/core/writeCut.py) function.\n",
    "\n",
    "We can test our solutions by reading data with these [Hussaini lab tools](https://github.com/HussainiLab/BinConverter/blob/master/BinConverter/core/Tint_Matlab.py). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8f7aaa",
   "metadata": {},
   "source": [
    "<a id='index'></a>\n",
    "## Index\n",
    "\n",
    "* [Testing functions](#testing_functions)\n",
    "* [Hussaini-lab functions](#hussaini-lab_functions)\n",
    "* [Convert Recording Extractor to TINT](#Convert_recording_extractor_to_tint)\n",
    "* [Convert Sorting Extractor to TINT](#Convert_sorting_extractor_to_tint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83cf3093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.5 (default, Sep  4 2020, 07:30:14) \n",
      "[GCC 7.3.0] linux /home/sbuergers/spikeinterface/spikeinterface_new_api/venv/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (16, 8)\n",
    "plt.rcParams.update({'font.size':14})\n",
    "%matplotlib inline\n",
    "\n",
    "import spikeextractors as se\n",
    "\n",
    "print(sys.version, sys.platform, sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1210bfc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input directory =  /mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/sample_bin_to_tint_no_bin\n",
      "Output directory =  /mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/sample_bin_to_tint_no_bin/conversion_to_tint\n"
     ]
    }
   ],
   "source": [
    "# Directories\n",
    "\n",
    "dir_name = Path('/mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/sample_bin_to_tint_no_bin')\n",
    "print('Input directory = ', dir_name)\n",
    "\n",
    "save_dir = dir_name / 'conversion_to_tint'\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "print('Output directory = ', save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aafca1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read cached spikeextractors data\n",
    "\n",
    "r_cache = se.load_extractor_from_pickle(os.path.join(dir_name, 'cached_unit_data_no_bin_preproc.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69609f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sbuergers/spikeinterface/spikeinterface_new_api/venv/lib/python3.8/site-packages/hdmf/common/table.py:442: UserWarning: An attribute 'name' already exists on DynamicTable 'electrodes' so this column cannot be accessed as an attribute, e.g., table.name; it can only be accessed using other methods, e.g., table['name'].\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Read NWB recording data\n",
    "\n",
    "nwb_dir = Path(dir_name, 'nwb')\n",
    "recording_nwb = se.NwbRecordingExtractor(nwb_dir / 'axona_tutorial_re2.nwb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29ca4d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read NWB sorting data\n",
    "\n",
    "sorting_nwb = se.NwbSortingExtractor(nwb_dir / 'axona_se_MS4.nwb', sampling_frequency=48000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43beb92a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spikeextractors.extractors.bindatrecordingextractor.bindatrecordingextractor.BinDatRecordingExtractor'>\n",
      "<class 'spikeextractors.extractors.nwbextractors.nwbextractors.NwbRecordingExtractor'>\n",
      "<class 'spikeextractors.extractors.nwbextractors.nwbextractors.NwbSortingExtractor'>\n"
     ]
    }
   ],
   "source": [
    "# Show data types of different objects\n",
    "\n",
    "print(type(r_cache))\n",
    "print(type(recording_nwb))\n",
    "print(type(sorting_nwb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b12d160",
   "metadata": {},
   "source": [
    "<a id=\"testing_functions\"></a>\n",
    "## Testing functions\n",
    "[back to index](#index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93029f86",
   "metadata": {},
   "source": [
    "As we start exporting to putative TINT format, we will want to check if we can read it back in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3d0bd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spikeextractors.extractors.axonaunitrecordingextractor import AxonaUnitRecordingExtractor\n",
    "import os\n",
    "\n",
    "\n",
    "def test_axonaunitrecordingextractor(filename):\n",
    "    '''Reads UNIT data with AxonaUnitRecordingExtractor and\n",
    "    performs some simple operations as a sanity check. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : str or Path\n",
    "        Full filename of `.set` file (could be any extension actually)\n",
    "    '''\n",
    "    re = AxonaUnitRecordingExtractor(filename=filename)\n",
    "    \n",
    "    # TEST AXONARECORDINGEXTRACTOR\n",
    "    # Retrieve some simple recording information and print it\n",
    "    recording = re\n",
    "    print('Channel ids = {}'.format(recording.get_channel_ids()))\n",
    "    print('Num. channels = {}'.format(len(recording.get_channel_ids())))\n",
    "    print('Sampling frequency = {} Hz'.format(recording.get_sampling_frequency()))\n",
    "    print('Num. timepoints = {}'.format(recording.get_num_frames()))\n",
    "    print('Stdev. on third channel = {}'.format(np.std(recording.get_traces(channel_ids=2))))\n",
    "    print('Location of third electrode = {}'.format(\n",
    "        recording.get_channel_property(channel_id=2, property_name='location')))\n",
    "    print('Channel groups = {}'.format(recording.get_channel_groups()))\n",
    "    \n",
    "    # TEST NEO_READER (axonaio)\n",
    "    print(recording.neo_reader.header['signal_channels'])\n",
    "    \n",
    "    \n",
    "def test_tetrode_files(filename):\n",
    "    '''Reads UNIT data with AxonaUnitRecordingExtractor and\n",
    "    performs some simple operations as a sanity check. \n",
    "    Will only test .X  and .set files (no .clu or .cut, no .pos).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : str or Path\n",
    "        Full filename of `.set` file (could be any extension actually)\n",
    "    '''\n",
    "    test_axonaunitrecordingextractor(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6333895",
   "metadata": {},
   "source": [
    "<a id=\"hussaini-lab_functions\"></a>\n",
    "## Hussaini-lab functions\n",
    "[back to index](#index)\n",
    "\n",
    "`gebaSpike` actually wants already existing `.cut` or `.clu` files, and allows modifying them. So these might not be all that useful for exporting to `.cut` or `.clu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c221133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From \n",
    "# https://github.com/GeoffBarrett/gebaSpike/blob/967097ec28592182ef9783d2d391930e1c63ca58/gebaSpike/main.py\n",
    "\n",
    "def save_function(self):\n",
    "    \"\"\"\n",
    "    this method will save the .cut file\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if self.cut_filename.text() == default_filename:\n",
    "        return\n",
    "\n",
    "    save_filename = os.path.realpath(self.cut_filename.text())\n",
    "\n",
    "    if os.path.exists(save_filename):\n",
    "        self.choice = None\n",
    "        self.LogError.signal.emit('OverwriteCut!%s' % save_filename)\n",
    "        while self.choice is None:\n",
    "            time.sleep(0.1)\n",
    "\n",
    "        if self.choice != QtWidgets.QMessageBox.Yes:\n",
    "            return\n",
    "\n",
    "    if len(self.tetrode_data) == 0:\n",
    "        return\n",
    "\n",
    "    # organize the cut data\n",
    "    n_spikes_expected = self.tetrode_data.shape[1]\n",
    "    n_spikes = len(np.asarray([item for sublist in self.cell_indices.values() for item in sublist]))\n",
    "\n",
    "    # check that with the manipulation of the spikes, that we still have the correct number of spikes\n",
    "    if n_spikes != n_spikes_expected:\n",
    "        self.choice = None\n",
    "        self.LogError.signal.emit('cutSizeError')\n",
    "        while self.choice is None:\n",
    "            time.sleep(0.1)\n",
    "        return\n",
    "\n",
    "    # we will check if we are missing some of the spikes somehow. If we kept track of them, then the indices from\n",
    "    # the spikes, when sorted, should produce an array from 0 -> N-1 spikes.\n",
    "    if not np.array_equal(np.sort(np.asarray([item for sublist in self.cell_indices.values() for item in sublist])),\n",
    "                      np.arange(len(self.cut_data_original))):\n",
    "        self.choice = None\n",
    "        self.LogError.signal.emit('cutIndexError')\n",
    "        while self.choice is None:\n",
    "            time.sleep(0.1)\n",
    "        return\n",
    "\n",
    "    cut_values = np.zeros(n_spikes)\n",
    "    for cell, cell_indices in self.cell_indices.items():\n",
    "        cut_values[cell_indices] = cell\n",
    "\n",
    "    if '.clu.' in save_filename:\n",
    "        # save the .clu filename\n",
    "        write_clu(save_filename, cut_values)\n",
    "        self.choice = None\n",
    "        self.LogError.signal.emit('saveCompleteClu')\n",
    "        while self.choice is None:\n",
    "            time.sleep(0.1)\n",
    "        self.actions_made = False\n",
    "\n",
    "    else:\n",
    "        # save the cut filename\n",
    "        write_cut(save_filename, cut_values)\n",
    "        self.choice = None\n",
    "        self.LogError.signal.emit('saveComplete')\n",
    "        while self.choice is None:\n",
    "            time.sleep(0.1)\n",
    "        self.actions_made = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01729f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From \n",
    "# https://github.com/GeoffBarrett/gebaSpike/blob/967097ec28592182ef9783d2d391930e1c63ca58/gebaSpike/core/writeCut.py\n",
    "\n",
    "def write_cut(cut_filename, cut, basename=None):\n",
    "    if basename is None:\n",
    "        basename = os.path.basename(os.path.splitext(cut_filename)[0])\n",
    "\n",
    "    unique_cells = np.unique(cut)\n",
    "\n",
    "    if 0 not in unique_cells:\n",
    "        # if it happens that there is no zero cell, add it anyways\n",
    "        unique_cells = np.insert(unique_cells, 0, 0)  # object, index, value to insert\n",
    "\n",
    "    n_clusters = len(np.unique(cut))\n",
    "    n_spikes = len(cut)\n",
    "\n",
    "    write_list = []  # the list of values to write\n",
    "\n",
    "    tab = '    '  # the spaces didn't line up with my tab so I just created a string with enough spaces\n",
    "    empty_space = '               '  # some of the empty spaces don't line up to x tabs\n",
    "\n",
    "    # we add 1 to n_clusters because zero is the garbage cell that no one uses\n",
    "    write_list.append('n_clusters: %d\\n' % (n_clusters))\n",
    "    write_list.append('n_channels: 4\\n')\n",
    "    write_list.append('n_params: 2\\n')\n",
    "    write_list.append('times_used_in_Vt:%s' % ((tab + '0') * 4 + '\\n'))\n",
    "\n",
    "    zero_string = (tab + '0') * 8 + '\\n'\n",
    "\n",
    "    for cell_i in np.arange(n_clusters):\n",
    "        write_list.append(' cluster: %d center:%s' % (cell_i, zero_string))\n",
    "        write_list.append('%smin:%s' % (empty_space, zero_string))\n",
    "        write_list.append('%smax:%s' % (empty_space, zero_string))\n",
    "    write_list.append('\\nExact_cut_for: %s spikes: %d\\n' % (basename, n_spikes))\n",
    "\n",
    "    # now the cut file lists 25 values per row\n",
    "    n_rows = int(np.floor(n_spikes / 25))  # number of full rows\n",
    "\n",
    "    remaining = int(n_spikes - n_rows * 25)\n",
    "    cut_string = ('%3u' * 25 + '\\n') * n_rows + '%3u' * remaining\n",
    "\n",
    "    write_list.append(cut_string % (tuple(cut)))\n",
    "\n",
    "    with open(cut_filename, 'w') as f:\n",
    "        f.writelines(write_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07cc054b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From \n",
    "# https://github.com/GeoffBarrett/gebaSpike/blob/967097ec28592182ef9783d2d391930e1c63ca58/gebaSpike/core/writeCut.py\n",
    "\n",
    "def write_clu(clu_filename, data):\n",
    "    # the .clu files and the .cut files are different since the .clu files are the .cut files (with no manual sorting)\n",
    "    # without the headers, and the values go from 1 -> N instead of 0 -> N, (1-based numbering instead of 0-based). Thus\n",
    "    # we add 1 to the .cut data to get the .clu data\n",
    "\n",
    "    data = np.asarray(data).astype(int)  # ensuring that the data is the integer data-type\n",
    "\n",
    "    data += 1  # making the data 1-based instead of 0-based\n",
    "\n",
    "    # calculating the number of clusters\n",
    "    n_clust = len(np.unique(data))\n",
    "\n",
    "    # ensuring that the cluster number is the 1st value\n",
    "    data = np.concatenate(([n_clust], data))\n",
    "\n",
    "    # saving the data as a column (delimter='\\n') and integer format.\n",
    "    np.savetxt(clu_filename, data, fmt='%d', delimiter='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230e729d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bfa970",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ff0753",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc976e55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc8c7b45",
   "metadata": {},
   "source": [
    "<a id=\"Convert_recording_extractor_to_tint\"></a>\n",
    "## Convert Recording extractor to TINT\n",
    "[back to index](#index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852eb64d",
   "metadata": {},
   "source": [
    "Hmm, since we are writing to TINT, thereby creating `.X` tetrode files, we throw away all information in-between spikes. There is no point to convert the fake continuous recording used for spike sorting to TINT at all. We really only want to export the spike sorting output!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90a3c847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anything to do here?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d90289",
   "metadata": {},
   "source": [
    "<a id=\"Convert_sorting_extractor_to_tint\"></a>\n",
    "## Convert Sorting extractor to TINT\n",
    "[back to index](#index)\n",
    "\n",
    "There are several points in the pipeline at which we might want to export to TINT. Ideally it should work for any `SortingExtractor` object!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7fe21ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where do we load data from?\n",
      "\n",
      " /mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/sample_bin_to_tint_no_bin\n"
     ]
    }
   ],
   "source": [
    "print('Where do we load data from?\\n\\n', dir_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae069b09",
   "metadata": {},
   "source": [
    "From a sorting extractor we can obtain a list unit spike sample arrays. We can convert this to the .clu or .cut type array of unit ID labels for each spike.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0fc1e882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20201004_Tint_1\n"
     ]
    }
   ],
   "source": [
    "cut_filename = Path('/mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/Axona_Tint_1ms/20201004_Tint_1.cut')\n",
    "\n",
    "basename = os.path.basename(os.path.splitext(cut_filename)[0])\n",
    "\n",
    "print(basename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8bae0e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/Axona_Tint_1ms/20201004_Tint.set\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('/mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/Axona_Tint_1ms/20201004_Tint_1.cut')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = Path('/mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/Axona_Tint_1ms/20201004_Tint.set')\n",
    "print(filename)\n",
    "\n",
    "Path(str(filename.with_suffix('')) + '_{}'.format(1) + '.cut')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2270a472",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_spike_train_to_label_array(spike_train):\n",
    "    '''Takes a list of arrays, where each array is a series of\n",
    "    sample points at which a spike occured for a given unit\n",
    "    (each list item is a unit). Converts to .cut array, i.e.\n",
    "    orders spike samples from all units and labels each sample\n",
    "    with the appropriate unit ID.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    spike_train : List of np.arrays\n",
    "        Output of `get_units_spike_train()` method of sorting extractor\n",
    "        \n",
    "    Return\n",
    "    ------\n",
    "    unit_labels_sorted : np.array\n",
    "        Each entry is the unit ID corresponding to the spike sample that\n",
    "        occured at this ordinal position\n",
    "    '''\n",
    "\n",
    "    # Generate Index array (indexing the unit for a given spike sample)\n",
    "    unit_labels = []\n",
    "    for i, l in enumerate(spike_train):\n",
    "        unit_labels.append(np.ones((len(l),), dtype=int) * i)\n",
    "    \n",
    "    # Flatten lists and sort them\n",
    "    spike_train_flat = np.concatenate(spike_train).ravel()\n",
    "    unit_labels_flat = np.concatenate(unit_labels).ravel()\n",
    "\n",
    "    sort_index = np.argsort(spike_train_flat)\n",
    "\n",
    "    unit_labels_sorted = unit_labels_flat[sort_index]\n",
    "\n",
    "    return unit_labels_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8215d02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_cut_file(cut_filename, unit_labels):\n",
    "    '''Write spike sorting output to .cut file.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    cut_filename : str or Path\n",
    "        Full filename of .cut file to write to. A given .cut file belongs\n",
    "        to a given tetrode file. For example, for tetrode `my_file.1`, the\n",
    "        corresponding cut_filename should be `my_file_1.cut`.\n",
    "    unit_labels : np.array\n",
    "        Vector of unit labels for each spike sample (ordered by time of \n",
    "        occurence)\n",
    "        \n",
    "    Example\n",
    "    -------\n",
    "    # Given a sortingextractor called sorting_nwb:\n",
    "    spike_train = sorting_nwb.get_units_spike_train()\n",
    "    unit_labels = convert_spike_train_to_label_array(spike_train)\n",
    "    write_to_cut_file(cut_filename, unit_labels)\n",
    "    \n",
    "    ---\n",
    "    Largely based on gebaSpike implementation by Geoff Barrett\n",
    "    https://github.com/GeoffBarrett/gebaSpike\n",
    "    '''\n",
    "\n",
    "    unique_cells = np.unique(unit_labels)\n",
    "\n",
    "    n_clusters = len(np.unique(unit_labels))\n",
    "    n_spikes = len(unit_labels)\n",
    "\n",
    "    write_list = []\n",
    "\n",
    "    tab = '    '\n",
    "    empty_space = '               '\n",
    "\n",
    "    write_list.append('n_clusters: %d\\n' % (n_clusters))\n",
    "    write_list.append('n_channels: 4\\n')\n",
    "    write_list.append('n_params: 2\\n')\n",
    "    write_list.append('times_used_in_Vt:%s' % ((tab + '0') * 4 + '\\n'))\n",
    "\n",
    "    zero_string = (tab + '0') * 8 + '\\n'\n",
    "\n",
    "    for cell_i in np.arange(n_clusters):\n",
    "        write_list.append(' cluster: %d center:%s' % (cell_i, zero_string))\n",
    "        write_list.append('%smin:%s' % (empty_space, zero_string))\n",
    "        write_list.append('%smax:%s' % (empty_space, zero_string))\n",
    "    write_list.append('\\nExact_cut_for: %s spikes: %d\\n' % (basename, n_spikes))\n",
    "\n",
    "    # The unit label array consists of 25 values per row in .cut file\n",
    "    n_rows = int(np.floor(n_spikes / 25))\n",
    "    remaining = int(n_spikes - n_rows * 25)\n",
    "\n",
    "    cut_string = ('%3u' * 25 + '\\n') * n_rows + '%3u' * remaining\n",
    "\n",
    "    write_list.append(cut_string % (tuple(unit_labels)))\n",
    "\n",
    "    with open(cut_filename, 'w') as f:\n",
    "        f.writelines(write_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2487be27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_clu_file(clu_filename, unit_labels):\n",
    "    ''' .clu files are pruned .cut files, containing only a long vector of unit\n",
    "    labels, which are 1-indexed, instead of 0-indexed. In addition, the very first\n",
    "    entry is the total number of units.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    clu_filename : str or Path\n",
    "        Full filename of .clu file to write to. A given .clu file belongs\n",
    "        to a given tetrode file. For example, for tetrode `my_file.1`, the\n",
    "        corresponding clu_filename should be `my_file_1.clu`.\n",
    "    unit_labels : np.array\n",
    "        Vector of unit labels for each spike sample (ordered by time of \n",
    "        occurence)\n",
    "        \n",
    "    ---\n",
    "    Largely based on gebaSpike implementation by Geoff Barrett\n",
    "    https://github.com/GeoffBarrett/gebaSpike\n",
    "    '''\n",
    "    unit_labels = np.asarray(unit_labels).astype(int)\n",
    "    unit_labels += 1\n",
    "\n",
    "    n_clust = len(np.unique(unit_labels))\n",
    "    unit_labels = np.concatenate(([n_clust], unit_labels))\n",
    "\n",
    "    np.savetxt(clu_filename, unit_labels, fmt='%d', delimiter='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "995d9aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_cut_filename_from_basename(filename, tetrode_id):\n",
    "    '''Given a str or Path object, assume the last entry after a slash\n",
    "    is a filename, strip any file suffix, add tetrode ID label, and\n",
    "    .cut suffix to name.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : str or Path\n",
    "    tetrode_id : int\n",
    "    '''\n",
    "    return Path(str(filename).split('.')[0] + '_{}'.format(tetrode_id) + '.cut')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fadbd312",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_unit_labels_to_file(sorting_extractor, filename):\n",
    "    '''Write spike sorting output to .cut file, separately for each\n",
    "    tetrode.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sorting_extractor : spikeextractors.SortingExtractor\n",
    "    filename : str or Path\n",
    "        Full filename of .set file or base-filename (i.e. the part of the\n",
    "        filename all Axona files have in common). A given .cut file belongs\n",
    "        to a given tetrode file. For example, for tetrode `my_file.1`, the\n",
    "        corresponding cut_filename should be `my_file_1.cut`. This will be\n",
    "        set automatically given the base-filename or set file.\n",
    "        \n",
    "    TODO: Any reason one might want to only convert some tetrodes or some\n",
    "    samples? Should those be parameters?\n",
    "    '''\n",
    "    tetrode_ids = sorting_extractor.get_units_property(property_name='group')\n",
    "    tetrode_ids = np.array(tetrode_ids)\n",
    "    \n",
    "    unit_ids = np.array(sorting_extractor.get_unit_ids())\n",
    "    \n",
    "    for i in np.unique(tetrode_ids):\n",
    "        \n",
    "        print('Converting Tetrode {}'.format(i))\n",
    "\n",
    "        spike_train = sorting_extractor.get_units_spike_train(unit_ids=unit_ids[tetrode_ids==i])\n",
    "        unit_labels = convert_spike_train_to_label_array(spike_train)\n",
    "\n",
    "        # We use Axona conventions for filenames (tetrodes are 1 indexed)\n",
    "        cut_filename = set_cut_filename_from_basename(filename, i + 1)\n",
    "        clu_filename = Path(str(cut_filename).replace('.cut', '.clu'))\n",
    "\n",
    "        write_to_cut_file(cut_filename, unit_labels)\n",
    "        write_to_clu_file(clu_filename, unit_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "001d00c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spikeextractors.extractors.nwbextractors.nwbextractors.NwbSortingExtractor'>\n"
     ]
    }
   ],
   "source": [
    "# We have sorting data exported in `.nwb` format\n",
    "\n",
    "nwb_dir = Path(dir_name, 'nwb')\n",
    "sorting_nwb = se.NwbSortingExtractor(nwb_dir / 'axona_se_MS4.nwb', sampling_frequency=48000)\n",
    "\n",
    "print(type(sorting_nwb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "45c9d08a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling frequency: 48000 Hz\n"
     ]
    }
   ],
   "source": [
    "print('Sampling frequency:', sorting_nwb.get_sampling_frequency(), 'Hz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "e0ddef20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting Tetrode 0\n",
      "Converting Tetrode 1\n",
      "Converting Tetrode 2\n",
      "Converting Tetrode 3\n"
     ]
    }
   ],
   "source": [
    "# Convert all tetrodes from sorting extractor to cut files\n",
    "write_unit_labels_to_file(sorting_nwb, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43f91b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223979f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd251800",
   "metadata": {},
   "source": [
    "## Convert recording extractor to tetrode files (.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "10194d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from BinConverter.core.ConvertTetrode import write_tetrode\n",
    "from BinConverter.core.readBin import (\n",
    "    get_bin_data, get_raw_pos, get_channel_from_tetrode, get_active_tetrode, get_active_eeg\n",
    ")\n",
    "from BinConverter.core.Tint_Matlab import int16toint8\n",
    "\n",
    "from spikeextractors.extractors.axonaunitrecordingextractor import AxonaUnitRecordingExtractor\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f5882ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_generic_header(filename):\n",
    "    \"\"\"\n",
    "    Given a binary file with phrases and line breaks, enters the\n",
    "    first word of a phrase as dictionary key and the following\n",
    "    string (without linebreaks) as value. Returns the dictionary.\n",
    "    \"\"\"\n",
    "    header = {}\n",
    "    with open(filename, 'rb') as f:\n",
    "        for bin_line in f:\n",
    "            if b'data_start' in bin_line:\n",
    "                break\n",
    "            line = bin_line.decode('cp1252').replace('\\r\\n', '').replace('\\r', '').strip()\n",
    "            parts = line.split(' ')\n",
    "            key = parts[0]\n",
    "            value = ' '.join(parts[1:])\n",
    "            header[key] = value\n",
    "            \n",
    "    return header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0ff4d999",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_channel_from_tetrode(tetrode):\n",
    "    \"\"\"\n",
    "    This function will take the tetrode number and return the Axona\n",
    "    channel numbers, i.e. Tetrode 1 = Ch0-Ch3, Tetrode 2 = Ch4-Ch7, etc.\n",
    "    \"\"\"\n",
    "    return np.arange(0, 4) + 4 * (int(tetrode) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df479c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_file = dir_name / 'axona_sample.set'\n",
    "set_filename = set_file\n",
    "print(set_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d87e2fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "re = AxonaUnitRecordingExtractor(filename=set_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f26aa3a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spikeextractors.extractors.axonaunitrecordingextractor.axonaunitrecordingextractor.AxonaUnitRecordingExtractor at 0x7fcb74500b50>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.neo_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "27349ae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/sample_bin_to_tint_no_bin')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "00de1c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "header = parse_generic_header(set_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bb4d4e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "40\n",
      "30\n",
      "43\n",
      "100\n",
      "-100\n"
     ]
    }
   ],
   "source": [
    "pre_spike_samples = int(header['pretrigSamps'])\n",
    "post_spike_samples = int(header['spikeLockout'])\n",
    "rejstart = int(header['rejstart'])\n",
    "rejthreshtail = int(header['rejthreshtail'])\n",
    "rejthreshupper = int(header['rejthreshupper'])\n",
    "rejthreshlower = int(header['rejthreshlower'])\n",
    "\n",
    "print(pre_spike_samples)\n",
    "print(post_spike_samples)\n",
    "print(rejstart)\n",
    "print(rejthreshtail)\n",
    "print(rejthreshupper)\n",
    "print(rejthreshlower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6f4d7186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spikeextractors.extractors.bindatrecordingextractor.bindatrecordingextractor.BinDatRecordingExtractor at 0x7fcb8ca94e50>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2b5068c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tetrode = 1\n",
    "set_file.stem + '.{}'.format(tetrode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b6c860aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_channel_from_tetrode' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-de74971df35b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtetrode_channels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_channel_from_tetrode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtetrode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'get_channel_from_tetrode' is not defined"
     ]
    }
   ],
   "source": [
    "tetrode_channels = get_channel_from_tetrode(tetrode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "06809590",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_bin_data??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "6dff6db5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tetrode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "c5082a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "import mmap\n",
    "\n",
    "def get_bin_data(bin_filename, channels=None, tetrode=None):\n",
    "    \"\"\"This function will be used to acquire the actual lfp data given the .bin filename,\n",
    "    and the tetrode or channels (from 1-64) that you want to get\"\"\"\n",
    "\n",
    "    if tetrode is not None:\n",
    "        channels = get_channel_from_tetrode(tetrode)\n",
    "    else:\n",
    "        channels = np.array(channels)  # just in case it isn't an np.array\n",
    "\n",
    "    bytes_per_iteration = 432\n",
    "\n",
    "    with open(bin_filename, 'rb') as f:\n",
    "        # pass\n",
    "        with contextlib.closing(mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ)) as m:\n",
    "            num_iterations = int(len(m)/bytes_per_iteration)\n",
    "\n",
    "            data = np.ndarray((num_iterations,), (np.int16, (1,192)), m, 32, (bytes_per_iteration,)).reshape((-1, 1)).flatten()\n",
    "            data = samples_to_array(data, channels=channels.tolist())\n",
    "\n",
    "    return data\n",
    "\n",
    "def samples_to_array(A, channels=[]):\n",
    "    \"\"\"This will take data matrix A, and convert it into a numpy array, there are three samples of\n",
    "    64 channels in this matrix, however their channels do need to be re-mapped\"\"\"\n",
    "\n",
    "    if channels == []:\n",
    "        channels = np.arange(64) + 1\n",
    "    else:\n",
    "        channels = np.asarray(channels)\n",
    "\n",
    "    A = np.asarray(A)\n",
    "\n",
    "    sample_num = int(len(A) / 64)  # get the sample numbers\n",
    "\n",
    "    sample_array = np.zeros((len(channels), sample_num))  # creating a 64x3 array of zeros (64 channels, 3 samples)\n",
    "\n",
    "    for i, channel in enumerate(channels):\n",
    "        sample_array[i, :] = A[get_sample_indices(channel, sample_num)]\n",
    "\n",
    "    return sample_array\n",
    "\n",
    "def get_sample_indices(channel_number, samples):\n",
    "    remap_channel = get_remap_chan(channel_number)\n",
    "\n",
    "    indices_scalar = np.multiply(np.arange(samples), 64)\n",
    "    sample_indices = indices_scalar + np.multiply(np.ones(samples), remap_channel)\n",
    "\n",
    "    # return np.array([remap_channel, 64 + remap_channel, 64*2 + remap_channel])\n",
    "    return (indices_scalar + np.multiply(np.ones(samples), remap_channel)).astype(int)\n",
    "\n",
    "def get_remap_chan(chan_num):\n",
    "    \"\"\"There is re-mapping, thus to get the correct channel data, you need to incorporate re-mapping\n",
    "    input will be a channel from 1 to 64, and will return the remapped channel\"\"\"\n",
    "\n",
    "    remap_channels = np.array([32, 33, 34, 35, 36, 37, 38, 39, 0, 1, 2, 3, 4, 5,\n",
    "                               6, 7, 40, 41, 42, 43, 44, 45, 46, 47, 8, 9, 10, 11,\n",
    "                               12, 13, 14, 15, 48, 49, 50, 51, 52, 53, 54, 55, 16, 17,\n",
    "                               18, 19, 20, 21, 22, 23, 56, 57, 58, 59, 60, 61, 62, 63,\n",
    "                               24, 25, 26, 27, 28, 29, 30, 31])\n",
    "\n",
    "    return remap_channels[chan_num - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "b7121c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_filename = Path('/mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/sample_bin_to_tint/axona_sample.bin')\n",
    "data = get_bin_data(bin_filename, tetrode=tetrode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "f4c610fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 57600)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "ddd6dab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -7572.,  -1930.,      0., ..., -26920., -22956., -17398.],\n",
       "       [ -5500.,   -206.,      0., ..., -18238., -17952., -15410.],\n",
       "       [   798.,  -9238.,      0., ..., -17090., -13344.,  -8568.],\n",
       "       [  2378.,  -6200.,      0., ..., -17912., -15948., -11356.]])"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "9b283da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo import AxonaIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "8b54a621",
   "metadata": {},
   "outputs": [],
   "source": [
    "neoio = AxonaIO(bin_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "3987e50c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3])"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channel_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "26f25a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_data_neo = neoio.get_analogsignal_chunk(channel_indexes=[12, 13, 14, 15]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "96f7082e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -7572,  -1930,      0, ..., -26920, -22956, -17398],\n",
       "       [ -5500,   -206,      0, ..., -18238, -17952, -15410],\n",
       "       [   798,  -9238,      0, ..., -17090, -13344,  -8568],\n",
       "       [  2378,  -6200,      0, ..., -17912, -15948, -11356]], dtype=int16)"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bin_data_neo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "7a47ead3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tet1_data_neo = neoio.get_spike_raw_waveforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "72f01e49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "memmap([[[  -7,    2,   18, ...,   33,   26,   15],\n",
       "         [  -6,   -2,    1, ...,   25,   25,   22],\n",
       "         [  -2,    3,   14, ...,   30,   24,   14],\n",
       "         [ -18,   -6,   13, ...,   22,   10,   -3]],\n",
       "\n",
       "        [[   4,   -3,  -11, ...,  -11,  -10,  -12],\n",
       "         [  28,   22,    8, ...,   -7,  -10,  -12],\n",
       "         [  22,   15,    1, ...,  -14,  -15,  -14],\n",
       "         [  40,   56,   57, ...,   -9,  -15,  -24]],\n",
       "\n",
       "        [[  15,    6,   -1, ...,  -40,  -25,   -8],\n",
       "         [  17,   14,    6, ...,  -29,  -29,  -25],\n",
       "         [  20,   15,    7, ...,  -34,  -24,  -11],\n",
       "         [   1,  -13,  -27, ...,  -53,  -31,   -5]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ -40,  -71, -108, ...,  -36,  -57,  -71],\n",
       "         [ -35,  -39,  -43, ...,   14,    0,  -20],\n",
       "         [ -15,  -17,  -22, ...,    5,  -15,  -46],\n",
       "         [ -20,  -20,  -26, ...,   12,   -6,  -32]],\n",
       "\n",
       "        [[   7,    5,    7, ...,    9,    6,    1],\n",
       "         [  -2,   -7,  -12, ...,  -15,  -13,  -16],\n",
       "         [  42,   44,   33, ...,   -1,   -5,   -9],\n",
       "         [  38,   44,   46, ...,   -8,  -10,  -10]],\n",
       "\n",
       "        [[ -28,  -29,  -19, ...,   17,   21,   26],\n",
       "         [  20,    8,    4, ...,    0,  -11,  -16],\n",
       "         [ -40,  -48,  -37, ...,   -5,   -5,    1],\n",
       "         [ -21,   -8,   12, ...,   22,   31,   36]]], dtype=int8)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tet1_data_neo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "5e0c8e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "fraw = 3682\n",
    "f = 4314.84375  # should be 3682"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "047981dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0b111001100010'"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bin(fraw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "8550067a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-219-57d3acba662e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "bin(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "9666751c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  4.42661252,  -9.72958561, -11.01196744, ..., -20.06961592,\n",
       "         -9.63475369,  -5.24076633],\n",
       "       [ -5.32732718, -13.26299552,  -6.53222017, ...,  -6.30657561,\n",
       "         -4.87058843,  -8.42493959],\n",
       "       [  2.52745487,  -4.65360377,  -7.52856443, ..., -32.52274916,\n",
       "        -17.37364009,  -5.33893052],\n",
       "       [ -4.76253126, -17.6824145 , -15.26715048, ..., -28.12541462,\n",
       "        -34.9801369 , -40.06043449]])"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traces * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "d1c49bfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.float64"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data[1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "49660066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>channel_ids</th>\n",
       "      <th>channel_groups</th>\n",
       "      <th>tetrode_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    channel_ids  channel_groups  tetrode_ids\n",
       "0             1               0            1\n",
       "1             2               0            1\n",
       "2             4               1            2\n",
       "3             6               1            2\n",
       "4             7               1            2\n",
       "5             8               2            3\n",
       "6             9               2            3\n",
       "7            10               2            3\n",
       "8            11               2            3\n",
       "9            12               3            4\n",
       "10           13               3            4\n",
       "11           14               3            4\n",
       "12           15               3            4"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    'channel_ids': r_cache.get_channel_ids(),\n",
    "    'channel_groups': r_cache.get_channel_groups(),\n",
    "    'tetrode_ids': r_cache.get_channel_groups() + 1\n",
    "})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "80d638ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tetrode_channels = df.loc[df['tetrode_ids'] == tetrode, 'channel_ids'].values + 1\n",
    "tetrode_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "b524bcf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tetrode_channels-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "3eb2357d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 57600)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A tetrode file expects 4 channels. Fill missing channels with zeros.\n",
    "\n",
    "traces = np.zeros((4, r_cache.get_num_frames()))\n",
    "traces[tetrode_channels-1, :] = r_cache.get_traces(channel_ids=tetrode_channels-1)\n",
    "traces.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a7decf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_cache.get_traces?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "9638e015",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tetrode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "3c94b3bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 7, 8])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tetrode_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "6fa77dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tetrode = int(tetrode)\n",
    "\n",
    "tetrode_filename = save_dir / Path(set_file.stem + '.{}'.format(tetrode))\n",
    "\n",
    "tetrode_channels = df.loc[df['tetrode_ids'] == tetrode, 'channel_ids'].values + 1\n",
    "\n",
    "traces = np.zeros((4, r_cache.get_num_frames()))\n",
    "traces[(tetrode_channels-1) % 4, :] = r_cache.get_traces(channel_ids=tetrode_channels-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "5295607d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Fs = r_cache.get_sampling_frequency()\n",
    "active_tetrodes = np.unique(r_cache.get_channel_groups()) + 1\n",
    "\n",
    "pre_spike_samples = int(header['pretrigSamps'])\n",
    "post_spike_samples = int(header['spikeLockout'])\n",
    "rejstart = int(header['rejstart'])\n",
    "rejthreshtail = int(header['rejthreshtail'])\n",
    "rejthreshupper = int(header['rejthreshupper'])\n",
    "rejthreshlower = int(header['rejthreshlower'])\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'channel_ids': r_cache.get_channel_ids(),\n",
    "    'channel_groups': r_cache.get_channel_groups(),\n",
    "    'tetrode_ids': r_cache.get_channel_groups() + 1\n",
    "})\n",
    "\n",
    "for tetrode in active_tetrodes:\n",
    "\n",
    "    tetrode = int(tetrode)\n",
    "    \n",
    "    tetrode_filename = save_dir / Path(set_file.stem + '.{}'.format(tetrode))\n",
    "    \n",
    "    tetrode_channels = df.loc[df['tetrode_ids'] == tetrode, 'channel_ids'].values + 1\n",
    "    \n",
    "    traces = np.zeros((4, r_cache.get_num_frames()))\n",
    "    traces[(tetrode_channels - 1) % 4, :] = r_cache.get_traces(channel_ids=tetrode_channels-1)\n",
    "\n",
    "    n_samples = traces.shape[1]\n",
    "\n",
    "    # create a time array that represents the 48kHz sampled data times\n",
    "    t = np.arange(0, n_samples) / Fs  # creates a time array of the signal starting from 0 (in seconds)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b678887",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "86a1fba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spikes(data, threshold):\n",
    "    all_spikes = np.array([])\n",
    "\n",
    "    for i, channel_data in enumerate(data):\n",
    "        spike_indices = np.where(channel_data >= threshold[i])[0]\n",
    "\n",
    "        if len(spike_indices) == 0:\n",
    "            continue\n",
    "\n",
    "        spike_indices = find_consec(spike_indices)\n",
    "\n",
    "        spike_indices = np.asarray([value[0] for value in spike_indices])\n",
    "\n",
    "        if len(all_spikes) == 0:\n",
    "            # this is the first iteration of the tetrode, no need to sort\n",
    "            unadded_spikes = spike_indices\n",
    "        else:\n",
    "            idx = matching_ind(all_spikes, spike_indices)\n",
    "            if len(idx) == 0:\n",
    "                unadded_spikes = spike_indices\n",
    "            else:\n",
    "                unadded_spikes = np.setdiff1d(spike_indices, all_spikes[idx])\n",
    "\n",
    "        if len(all_spikes) != 0:\n",
    "            all_spikes = np.sort(np.concatenate((all_spikes, unadded_spikes)))\n",
    "            unadded_spikes = None\n",
    "        else:\n",
    "            all_spikes = np.array(unadded_spikes)\n",
    "\n",
    "    return all_spikes\n",
    "\n",
    "def find_consec(data):\n",
    "    '''finds the consecutive numbers and outputs as a list'''\n",
    "    consecutive_values = []  # a list for the output\n",
    "    current_consecutive = [data[0]]\n",
    "\n",
    "    if len(data) == 1:\n",
    "        return [[data[0]]]\n",
    "\n",
    "    for index in range(1, len(data)):\n",
    "\n",
    "        if data[index] == data[index - 1] + 1:\n",
    "            current_consecutive.append(data[index])\n",
    "\n",
    "            if index == len(data) - 1:\n",
    "                consecutive_values.append(current_consecutive)\n",
    "\n",
    "        else:\n",
    "            consecutive_values.append(current_consecutive)\n",
    "            current_consecutive = [data[index]]\n",
    "\n",
    "            if index == len(data) - 1:\n",
    "                consecutive_values.append(current_consecutive)\n",
    "    return consecutive_values\n",
    "\n",
    "def matching_ind(haystack, needle):\n",
    "    idx = np.searchsorted(haystack, needle)\n",
    "    mask = idx < haystack.size\n",
    "    mask[mask] = haystack[idx[mask]] == needle[mask]\n",
    "    idx = idx[mask]\n",
    "    return idx\n",
    "\n",
    "def validate_spikes(tetrode, spikes, data, t, pre_spike_samples=10, post_spike_samples=40, rejstart=30,\n",
    "                    rejthreshtail=43, rejthreshupper=100, rejthreshlower=-100):\n",
    "    latest_spike = None\n",
    "\n",
    "    spike_count = 0\n",
    "    percentage_values = [int(value) for value in np.rint(np.linspace(0, len(spikes), num=21)).tolist()]\n",
    "\n",
    "    n_max = data.shape[1]\n",
    "\n",
    "    tetrode_spikes = {}\n",
    "\n",
    "    for spike in sorted(spikes):\n",
    "        # iterate through each spike and validate to ensure no spikes occur at the same time or within the\n",
    "        # refractory period\n",
    "\n",
    "        spike_count += 1\n",
    "\n",
    "        if spike_count in percentage_values:\n",
    "            pass\n",
    "\n",
    "        if spike - pre_spike_samples + 1 < 0:\n",
    "            continue\n",
    "\n",
    "        elif spike + post_spike_samples >= n_max:\n",
    "            continue\n",
    "\n",
    "        if latest_spike is not None:\n",
    "            if spike != latest_spike:\n",
    "                if spike in spike_refractory:\n",
    "                    # ensures no overlapping spikes\n",
    "                    continue\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        latest_spike = spike\n",
    "        spike_refractory = list(np.arange(spike + 1, spike + post_spike_samples + 1))\n",
    "\n",
    "        # spike_time = t[int(spike)]\n",
    "        spike_time = t[int(spike)]\n",
    "\n",
    "        # waveform_indices = np.where((t>=spike_time-250/1e6) & (t<=spike_time+850/1e6))[0]  # too slow\n",
    "        waveform_indices = np.arange(spike - pre_spike_samples + 1, spike + post_spike_samples + 1).astype(int)\n",
    "\n",
    "        # spike_t = t[waveform_indices] - spike_time  # making the times from -200 us to 800 us\n",
    "\n",
    "        # spike_waveform = np.zeros((len(tetrode_channels), 50))\n",
    "\n",
    "        spike_waveform = data[:, waveform_indices]\n",
    "\n",
    "        spike_time = spike_time * 96000  # multiply it by the timebase to get the frame count\n",
    "\n",
    "        spike_waveform = np.rint(spike_waveform)\n",
    "\n",
    "        # artifact rejection\n",
    "\n",
    "        if sum(spike_waveform[:, rejstart:].flatten() > rejthreshtail) > 0:\n",
    "            # this is 33% above baseline (0)\n",
    "            continue\n",
    "\n",
    "        # check if the first sample is well above or well below baseline\n",
    "        elif sum(spike_waveform[:, 0].flatten() > rejthreshupper) > 0:\n",
    "            # the first sample is >100\n",
    "            continue\n",
    "\n",
    "        elif sum(spike_waveform[:, 0].flatten() < rejthreshlower) > 0:\n",
    "            # or < -100\n",
    "            continue\n",
    "\n",
    "        tetrode_spikes[spike_time] = spike_waveform\n",
    "\n",
    "        # latest_spike = spike\n",
    "        # spike_refractory = list(np.arange(spike + 1, spike + post_spike_samples + 1))\n",
    "\n",
    "    return tetrode_spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "d57ab544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13, 14, 15, 16])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tetrode_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "ffbdb5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 3\n",
    "\n",
    "tetrode_spikes = {}  # creates an empty dictionary to hold the spike times\n",
    "# for each tetrode, find the spikes\n",
    "\n",
    "k = 0\n",
    "\n",
    "# data = int16toint8(data)  # converting the data into int8\n",
    "\n",
    "tetrode_thresholds = []\n",
    "for channel_index, channel in enumerate(tetrode_channels):\n",
    "    k += 1\n",
    "    '''\n",
    "    Auto thresholding technique incorporated by:\n",
    "    Quian Quiroga in 2014 - Unsupervised Spike Detection and Sorting with Wavelets and\n",
    "    Superparamagnetic Clustering\n",
    "    Thr = 4*sigma, sigma = median(abs(x)/0.6745)\n",
    "    '''\n",
    "    standard_deviations = float(threshold)\n",
    "\n",
    "    sigma_n = np.median(np.divide(np.abs(data[channel_index, :]), 0.6745))\n",
    "    # threshold = sigma_n / channel_max\n",
    "    # threshold = standard_deviations * sigma_n\n",
    "    tetrode_thresholds.append(standard_deviations * sigma_n)\n",
    "\n",
    "valid_spikes = get_spikes(data, tetrode_thresholds)\n",
    "\n",
    "# threshold is done in 16 bit values, but the rejection is done in 8bit, so we convert here\n",
    "# data = int16toint8(data)  # converting the data into int8\n",
    "\n",
    "data_int16 = int16toint8(data)  # converting the"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "717e709d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[23653.07635285397, 22896.960711638254, 26490.733876945887, 22834.692364714603]"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tetrode_thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "4dd12d8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  126,  1386,  1466,  1495,  1697,  1714,  1735,  2635,  2803,\n",
       "        2824,  2838,  2923,  2942,  4677,  4691,  4708,  5145,  5166,\n",
       "        5239,  5261,  5784,  5903,  5924,  6413,  6442,  6699,  6712,\n",
       "        6732,  6950,  7065,  7080,  7102,  7219,  7260,  8222,  8297,\n",
       "        8894,  8915,  9690,  9709,  9727, 10221, 10247, 10731, 10875,\n",
       "       11169, 11204, 11386, 11803, 11818, 11958, 11980, 12689, 12702,\n",
       "       12883, 12898, 12911, 14247, 14276, 14487, 14967, 15129, 15144,\n",
       "       15420, 15437, 15664, 15682, 16586, 16628, 16651, 16670, 16760,\n",
       "       16781, 16794, 16824, 16838, 16862, 17386, 17403, 17426, 17759,\n",
       "       18324, 18343, 18756, 19596, 19614, 19629, 19820, 19833, 19900,\n",
       "       20139, 20689, 20703, 21325, 21338, 21469, 21482, 21526, 21540,\n",
       "       22095, 22371, 22392, 22712, 22744, 23041, 23073, 23564, 23804,\n",
       "       23819, 23835, 23987, 24009, 24127, 24441, 25212, 25374, 25392,\n",
       "       25417, 25462, 25482, 26015, 26368, 26382, 27043, 27059, 27746,\n",
       "       27762, 29277, 29303, 30511, 32425, 33570, 33585, 33740, 35916,\n",
       "       36789, 37134, 37169, 37223, 37451, 37670, 38046, 38067, 38190,\n",
       "       38203, 39497, 39513, 39898, 40223, 40248, 42146, 42170, 42192,\n",
       "       42306, 42328, 42585, 42685, 42711, 43442, 43468, 43757, 44102,\n",
       "       44126, 44483, 44540, 44554, 44567, 44844, 44864, 44876, 44975,\n",
       "       45596, 45626, 46416, 46432, 47236, 47256, 47275, 47312, 47350,\n",
       "       47457, 48438, 48453, 48701, 49075, 49097, 49112, 49501, 49520,\n",
       "       50293, 51189, 51933, 51952, 51968, 51983, 52023, 52513, 52525,\n",
       "       52724, 55141, 55156, 55180, 56230, 56978, 57071, 57088, 57476,\n",
       "       57495])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tetrode_ids = sorting_nwb.get_units_property(property_name='group')\n",
    "tetrode_ids = np.array(tetrode_ids)\n",
    "\n",
    "unit_ids = np.array(sorting_nwb.get_unit_ids())\n",
    "spike_train = sorting_nwb.get_units_spike_train(unit_ids=unit_ids[tetrode_ids==3])\n",
    "\n",
    "sorted_spike_train = np.sort(np.concatenate(spike_train))\n",
    "\n",
    "sorted_spike_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "83b2554a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   94,  1366,  1367,  1472,  1703,  1705,  2634,  2635,  2636,\n",
       "        2801,  2906,  3276,  3277,  3652,  3653,  4000,  4001,  4002,\n",
       "        4352,  4353,  4403,  4674,  4675,  4935,  4936,  5023,  5024,\n",
       "        5128,  5129,  5130,  5232,  5233,  5247,  5569,  5570,  5784,\n",
       "        5786,  5888,  5974,  5975,  5976,  6260,  6261,  6289,  6409,\n",
       "        6411,  6608,  6706,  6707,  6937,  6982,  7065,  7208,  7210,\n",
       "        7255,  7256,  7257,  7686,  7688,  7886,  7887,  7888,  8222,\n",
       "        8223,  8435,  8436,  8756,  8757,  8878,  9310,  9311,  9556,\n",
       "        9557,  9699,  9700, 10216, 10218, 10723, 10724, 10868, 10869,\n",
       "       10870, 10940, 11167, 11380, 11454, 11455, 11456, 11511, 11787,\n",
       "       11832, 11955, 11997, 12053, 12453, 12455, 12483, 12688, 12758,\n",
       "       12759, 12760, 12890, 12891, 13643, 13644, 14112, 14241, 14242,\n",
       "       14483, 14484, 14681, 14682, 14683, 14853, 14854, 14855, 14966,\n",
       "       14967, 14968, 15123, 15405, 15449, 15450, 15484, 15486, 15659,\n",
       "       15673, 15674, 15675, 15829, 15830, 16108, 16109, 16110, 16186,\n",
       "       16188, 16227, 16264, 16324, 16405, 16520, 16521, 16582, 16635,\n",
       "       16756, 16757, 16832, 16834, 16835, 16984, 16985, 17023, 17044,\n",
       "       17204, 17205, 17395, 17396, 17397, 17655, 17748, 17908, 18257,\n",
       "       18258, 18259, 18320, 18321, 18581, 18756, 18757, 18861, 18862,\n",
       "       18863, 18958, 18959, 19119, 19120, 19216, 19217, 19343, 19344,\n",
       "       19592, 19593, 19695, 19696, 19824, 19825, 19899, 19900, 19901,\n",
       "       20139, 20140, 20214, 20215, 20280, 20281, 20282, 20552, 20553,\n",
       "       20568, 20575, 20696, 20697, 20847, 20849, 21082, 21307, 21321,\n",
       "       21407, 21440, 21441, 21458, 21523, 21524, 21758, 21760, 21919,\n",
       "       21920, 22089, 22367, 22368, 22711, 22712, 23039, 23168, 23169,\n",
       "       23299, 23300, 23544, 23798, 23799, 23983, 23984, 24064, 24065,\n",
       "       24083, 24084, 24170, 24415, 24416, 24476, 24477, 24478, 24860,\n",
       "       25143, 25144, 25206, 25381, 25459, 25460, 25461, 25618, 25619,\n",
       "       25620, 26021, 26022, 26090, 26091, 26349, 26350, 27022, 27204,\n",
       "       27318, 27750, 28491, 28512, 28639, 28641, 28642, 28677, 29281,\n",
       "       29283, 29330, 29332, 29871, 30034, 30035, 30099, 30100, 30101,\n",
       "       30733, 30734, 30886, 30887, 30934, 31964, 31965, 31966, 31982,\n",
       "       31983, 32113, 32245, 32259, 32271, 32861, 33227, 33551, 33553,\n",
       "       33987, 34159, 34160, 34766, 34768, 34769, 34882, 34883, 35028,\n",
       "       35029, 35333, 35334, 35606, 35607, 35608, 35914, 36074, 36352,\n",
       "       36353, 36378, 36379, 36469, 36518, 36669, 36670, 36981, 36982,\n",
       "       37133, 37203, 37204, 37228, 37434, 37536, 37537, 37669, 37670,\n",
       "       37671, 37895, 37986, 37987, 38054, 38176, 38178, 38365, 38401,\n",
       "       38494, 38496, 38607, 38791, 38861, 38862, 38863, 38884, 39047,\n",
       "       39048, 39354, 39355, 39380, 39381, 39403, 39404, 39432, 39493,\n",
       "       39495, 39680, 39681, 39767, 39768, 39769, 39873, 39918, 39919,\n",
       "       40092, 40093, 40218, 40219, 40453, 40454, 40596, 40597, 40636,\n",
       "       40637, 40638, 40897, 41068, 41077, 41084, 41093, 41321, 41322,\n",
       "       41354, 41418, 41419, 41586, 41743, 41754, 41817, 41899, 42028,\n",
       "       42029, 42155, 42241, 42242, 42299, 42457, 42458, 42459, 42557,\n",
       "       42558, 42561, 42679, 42680, 42995, 42996, 43017, 43018, 43019,\n",
       "       43037, 43347, 43437, 43439, 43529, 43531, 43596, 44094, 44097,\n",
       "       44479, 44480, 44531, 44852, 44895, 44938, 45524, 45589, 45850,\n",
       "       45851, 45955, 46265, 46266, 46306, 46307, 46423, 46525, 46951,\n",
       "       46952, 47001, 47243, 47319, 47453, 48433, 48435, 48667, 49083,\n",
       "       49494, 49495, 49537, 49538, 49586, 50055, 50261, 51166, 51767,\n",
       "       51768, 51769, 51948, 51949, 52023, 52024, 52025, 52311, 52519,\n",
       "       52520, 52521, 52991, 53594, 53595, 53596, 53769, 54330, 55150,\n",
       "       55623, 56004, 56205, 56207, 56208, 56209, 56571, 57053, 57237,\n",
       "       57238, 57389, 57464, 57473, 57571])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "a0c5e9e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00442661, -0.00972959, -0.01101197, ..., -0.02006962,\n",
       "        -0.00963475, -0.00524077],\n",
       "       [-0.00532733, -0.013263  , -0.00653222, ..., -0.00630658,\n",
       "        -0.00487059, -0.00842494],\n",
       "       [ 0.00252745, -0.0046536 , -0.00752856, ..., -0.03252275,\n",
       "        -0.01737364, -0.00533893],\n",
       "       [-0.00476253, -0.01768241, -0.01526715, ..., -0.02812541,\n",
       "        -0.03498014, -0.04006043]])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "b9bf372a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -29,   -7,    0, ..., -105,  -89,  -67],\n",
       "       [ -21,    0,    0, ...,  -71,  -70,  -60],\n",
       "       [   3,  -36,    0, ...,  -66,  -52,  -33],\n",
       "       [   9,  -24,    0, ...,  -69,  -62,  -44]])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_int16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "65ae01ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -7572.,  -1930.,      0., ..., -26920., -22956., -17398.],\n",
       "       [ -5500.,   -206.,      0., ..., -18238., -17952., -15410.],\n",
       "       [   798.,  -9238.,      0., ..., -17090., -13344.,  -8568.],\n",
       "       [  2378.,  -6200.,      0., ..., -17912., -15948., -11356.]])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "6c9a9445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=float64)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7285c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------Write Tetrode Data----------------------------------------\n",
    "\n",
    "Fs = get_Fs(set_filename)  # read the sampling frequency from the .set file, most like 48k\n",
    "\n",
    "active_tetrodes = get_active_tetrode(set_filename)\n",
    "\n",
    "# converts the data one tetrode at a time so we can eliminate memory errors\n",
    "\n",
    "pre_spike_samples = int(get_setfile_parameter('pretrigSamps', set_filename))\n",
    "post_spike_samples = int(get_setfile_parameter('spikeLockout', set_filename))\n",
    "rejstart = int(get_setfile_parameter('rejstart', set_filename))\n",
    "rejthreshtail = int(get_setfile_parameter('rejthreshtail', set_filename))\n",
    "rejthreshupper = int(get_setfile_parameter('rejthreshupper', set_filename))\n",
    "rejthreshlower = int(get_setfile_parameter('rejthreshlower', set_filename))\n",
    "\n",
    "for tetrode in active_tetrodes:\n",
    "\n",
    "    tetrode = int(tetrode)\n",
    "    # check if this tetrode exists already\n",
    "\n",
    "    tetrode_filename = os.path.join(directory, '%s.%d' % (tint_basename, tetrode))\n",
    "    if os.path.exists(tetrode_filename):\n",
    "        continue\n",
    "\n",
    "    tetrode_channels = get_channel_from_tetrode(tetrode)  # get the channels (from range of 1->64)\n",
    "\n",
    "    data = get_bin_data(bin_filename, tetrode=tetrode)  # 16bit, get data associated with the tetrode\n",
    "\n",
    "    # converting data to uV\n",
    "\n",
    "    n_samples = data.shape[1]\n",
    "    # create a time array that represents the 48kHz sampled data times\n",
    "    t = np.arange(0, n_samples) / Fs  # creates a time array of the signal starting from 0 (in seconds)\n",
    "\n",
    "    if not os.path.exists(tetrode_filename):\n",
    "\n",
    "        # ---------------------------Find the spikes in the unit data --------------------------------------\n",
    "\n",
    "        tetrode_spikes = {}  # creates an empty dictionary to hold the spike times\n",
    "        # for each tetrode, find the spikes\n",
    "\n",
    "        k = 0\n",
    "\n",
    "        # data = int16toint8(data)  # converting the data into int8\n",
    "\n",
    "        tetrode_thresholds = []\n",
    "        for channel_index, channel in enumerate(tetrode_channels):\n",
    "            k += 1\n",
    "            '''\n",
    "            Auto thresholding technique incorporated by:\n",
    "            Quian Quiroga in 2014 - Unsupervised Spike Detection and Sorting with Wavelets and\n",
    "            Superparamagnetic Clustering\n",
    "            Thr = 4*sigma, sigma = median(abs(x)/0.6745)\n",
    "            '''\n",
    "            standard_deviations = float(threshold)\n",
    "\n",
    "            sigma_n = np.median(np.divide(np.abs(data[channel_index, :]), 0.6745))\n",
    "            # threshold = sigma_n / channel_max\n",
    "            # threshold = standard_deviations * sigma_n\n",
    "            tetrode_thresholds.append(standard_deviations * sigma_n)\n",
    "\n",
    "        valid_spikes = get_spikes(data, tetrode_thresholds)\n",
    "\n",
    "        # threshold is done in 16 bit values, but the rejection is done in 8bit, so we convert here\n",
    "        # data = int16toint8(data)  # converting the data into int8\n",
    "\n",
    "        data = int16toint8(data)  # converting the data into int8\n",
    "\n",
    "        tetrode_spikes = validate_spikes(tetrode, valid_spikes, data, t, pre_spike_samples,\n",
    "                                         post_spike_samples, rejstart, rejthreshtail, rejthreshupper,\n",
    "                                         rejthreshlower)\n",
    "\n",
    "        # write the tetrode data to create the .N file\n",
    "        write_tetrode(tetrode_filename, tetrode_spikes, Fs)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    data = None\n",
    "    tetrode_spikes = None\n",
    "    valid_spikes = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09179af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfc6a77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "1dea0450",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_tetrode_files(sorting_extractor, save_dir):\n",
    "    '''Given a sorting extractor object create .X (tetrode) files.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sorting_extractor : spikeextractors.SortingExtractor\n",
    "    save_dir : str or Path\n",
    "        Directory where to save the output\n",
    "    '''\n",
    "    # TODO ...\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "1e81b2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_tint(sorting_extractor, filename):\n",
    "    '''Given a sorting extractor object, write appropriate data\n",
    "    to TINT format (from Axona). Will therefore create .X (tetrode),\n",
    "    .cut and .clu (spike sorting information) files.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sorting_extractor : spikeextractors.SortingExtractor\n",
    "    filename : str or Path\n",
    "        Full path and base filename shared by all output files \n",
    "        (e.g. my_dir/my_file will yield\n",
    "        my_dir/my_file.1, my_dir/my_file.2, ..., \n",
    "        my_dir/my_file_1.cut, my_dir/my_file_2.cut, ...,\n",
    "        my_dir/my_file_1.clu, my_dir/my_file_2.clu, ...)\n",
    "        If a file extension is given, it is simply ignored.\n",
    "        \n",
    "    Notes\n",
    "    -----\n",
    "    For details about the .X file format see:\n",
    "    http://space-memory-navigation.org/DacqUSBFileFormats.pdf\n",
    "    '''\n",
    "    # Make sure directory exists\n",
    "    filename.parent.absolute().mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # writes to .X files for each tetrode\n",
    "    # TODO...\n",
    "    write_to_tetrode_file(sorting_extractor, filename)\n",
    "    \n",
    "    # writes to .cut and .clu files for each tetrode\n",
    "    write_unit_labels_to_file(sorting_extractor, filename)\n",
    "    \n",
    "    # Position data?\n",
    "    # TODO ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "048d6928",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = Path(\n",
    "    '/mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/Axona_Tint_1ms/spikeextractors_to_tint/20201004_Tint'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "84ce7c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting Tetrode 0\n",
      "Converting Tetrode 1\n",
      "Converting Tetrode 2\n",
      "Converting Tetrode 3\n"
     ]
    }
   ],
   "source": [
    "write_to_tint(sorting_nwb, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15691ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4051f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490f3329",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5693d63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e684fd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db6c27f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spikeinterface",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
