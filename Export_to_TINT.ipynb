{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38378a04",
   "metadata": {},
   "source": [
    "# Convert recording and sorting extractor data to TINT format\n",
    "\n",
    "The Hussaini lab uses the proprietary TINT software from Axona to analyze extracellular electrophysiology data. While we are already able to read various data formats from Axona (`raw` data or `unit` data) into spikeinterface, perform preprocessing, spike sorting and export the data to NWB, we also want to allow to export data to the TINT format. \n",
    "\n",
    "The TINT format is essentially the same as the `unit` data, including `.X` and `.pos` files, but also `.cut` or `.clu`. The latter two contain information about the spike sorted units.\n",
    "\n",
    "The conversion can be facilitated by using the existing tools from the Hussaini lab, which [convert `.bin` data to `.X` and `.pos`](https://github.com/HussainiLab/BinConverter/blob/master/BinConverter/core/ConversionFunctions.py). Some of this code is only relevant for using the GUI, which did not work for me. I cleared out GUI code and ran a conversion from `.bin` to `.X` and `.pos` in this notebook: [explore_hussaini_tools.ipynb](https://github.com/sbuergers/hussaini-lab-to-nwb-notebooks/blob/master/explore_hussaini_tools.ipynb).\n",
    "\n",
    "They also already wrote a [`write_cut()`](https://github.com/GeoffBarrett/gebaSpike/blob/967097ec28592182ef9783d2d391930e1c63ca58/gebaSpike/core/writeCut.py) function.\n",
    "\n",
    "We can test our solutions by reading data with these [Hussaini lab tools](https://github.com/HussainiLab/BinConverter/blob/master/BinConverter/core/Tint_Matlab.py). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8f7aaa",
   "metadata": {},
   "source": [
    "<a id='index'></a>\n",
    "## Index\n",
    "\n",
    "* [Testing functions](#testing_functions)\n",
    "* [Hussaini-lab functions](#hussaini-lab_functions)\n",
    "* [Convert Sorting Extractor to TINT](#Convert_sorting_extractor_to_tint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83cf3093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.5 (default, Sep  4 2020, 07:30:14) \n",
      "[GCC 7.3.0] linux /home/sbuergers/spikeinterface/spikeinterface_new_api/venv/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (16, 8)\n",
    "plt.rcParams.update({'font.size':14})\n",
    "%matplotlib inline\n",
    "\n",
    "import spikeextractors as se\n",
    "import spiketoolkit as st\n",
    "\n",
    "print(sys.version, sys.platform, sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1210bfc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input directory =  /mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/sample_bin_to_tint_no_bin\n",
      "Output directory =  /mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/sample_bin_to_tint_no_bin/conversion_to_tint\n"
     ]
    }
   ],
   "source": [
    "# Directories\n",
    "\n",
    "dir_name = Path('/mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/sample_bin_to_tint_no_bin')\n",
    "print('Input directory = ', dir_name)\n",
    "\n",
    "save_dir = dir_name / 'conversion_to_tint'\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "print('Output directory = ', save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aafca1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read cached spikeextractors data\n",
    "\n",
    "r_cache = se.load_extractor_from_pickle(os.path.join(dir_name, 'cached_unit_data_no_bin_preproc.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69609f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sbuergers/spikeinterface/spikeinterface_new_api/venv/lib/python3.8/site-packages/hdmf/common/table.py:442: UserWarning: An attribute 'name' already exists on DynamicTable 'electrodes' so this column cannot be accessed as an attribute, e.g., table.name; it can only be accessed using other methods, e.g., table['name'].\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Read NWB recording data\n",
    "\n",
    "nwb_dir = Path(dir_name, 'nwb')\n",
    "recording_nwb = se.NwbRecordingExtractor(nwb_dir / 'axona_tutorial_re2.nwb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29ca4d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read NWB sorting data\n",
    "\n",
    "sorting_nwb = se.NwbSortingExtractor(nwb_dir / 'axona_se_MS4.nwb', sampling_frequency=48000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43beb92a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spikeextractors.extractors.bindatrecordingextractor.bindatrecordingextractor.BinDatRecordingExtractor'>\n",
      "<class 'spikeextractors.extractors.nwbextractors.nwbextractors.NwbRecordingExtractor'>\n",
      "<class 'spikeextractors.extractors.nwbextractors.nwbextractors.NwbSortingExtractor'>\n"
     ]
    }
   ],
   "source": [
    "# Show data types of different objects\n",
    "\n",
    "print(type(r_cache))\n",
    "print(type(recording_nwb))\n",
    "print(type(sorting_nwb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b12d160",
   "metadata": {},
   "source": [
    "<a id=\"testing_functions\"></a>\n",
    "## Testing functions\n",
    "[back to index](#index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93029f86",
   "metadata": {},
   "source": [
    "As we start exporting to putative TINT format, we will want to check if we can read it back in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3d0bd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spikeextractors.extractors.axonaunitrecordingextractor import AxonaUnitRecordingExtractor\n",
    "import os\n",
    "\n",
    "\n",
    "def test_axonaunitrecordingextractor(filename):\n",
    "    '''Reads UNIT data with AxonaUnitRecordingExtractor and\n",
    "    performs some simple operations as a sanity check. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : str or Path\n",
    "        Full filename of `.set` file (could be any extension actually)\n",
    "    '''\n",
    "    re = AxonaUnitRecordingExtractor(filename=filename)\n",
    "    \n",
    "    # TEST AXONARECORDINGEXTRACTOR\n",
    "    # Retrieve some simple recording information and print it\n",
    "    recording = re\n",
    "    print('Channel ids = {}'.format(recording.get_channel_ids()))\n",
    "    print('Num. channels = {}'.format(len(recording.get_channel_ids())))\n",
    "    print('Sampling frequency = {} Hz'.format(recording.get_sampling_frequency()))\n",
    "    print('Num. timepoints = {}'.format(recording.get_num_frames()))\n",
    "    print('Stdev. on third channel = {}'.format(np.std(recording.get_traces(channel_ids=2))))\n",
    "    print('Location of third electrode = {}'.format(\n",
    "        recording.get_channel_property(channel_id=2, property_name='location')))\n",
    "    print('Channel groups = {}'.format(recording.get_channel_groups()))\n",
    "    \n",
    "    # TEST NEO_READER (axonaio)\n",
    "    print(recording.neo_reader.header['signal_channels'])\n",
    "    \n",
    "    \n",
    "def test_tetrode_files(filename):\n",
    "    '''Reads UNIT data with AxonaUnitRecordingExtractor and\n",
    "    performs some simple operations as a sanity check. \n",
    "    Will only test .X  and .set files (no .clu or .cut, no .pos).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : str or Path\n",
    "        Full filename of `.set` file (could be any extension actually)\n",
    "    '''\n",
    "    test_axonaunitrecordingextractor(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6333895",
   "metadata": {},
   "source": [
    "<a id=\"hussaini-lab_functions\"></a>\n",
    "## Hussaini-lab functions\n",
    "[back to index](#index)\n",
    "\n",
    "`gebaSpike` actually wants already existing `.cut` or `.clu` files, and allows modifying them. So these might not be all that useful for exporting to `.cut` or `.clu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c221133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From \n",
    "# https://github.com/GeoffBarrett/gebaSpike/blob/967097ec28592182ef9783d2d391930e1c63ca58/gebaSpike/main.py\n",
    "\n",
    "def save_function(self):\n",
    "    \"\"\"\n",
    "    this method will save the .cut file\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if self.cut_filename.text() == default_filename:\n",
    "        return\n",
    "\n",
    "    save_filename = os.path.realpath(self.cut_filename.text())\n",
    "\n",
    "    if os.path.exists(save_filename):\n",
    "        self.choice = None\n",
    "        self.LogError.signal.emit('OverwriteCut!%s' % save_filename)\n",
    "        while self.choice is None:\n",
    "            time.sleep(0.1)\n",
    "\n",
    "        if self.choice != QtWidgets.QMessageBox.Yes:\n",
    "            return\n",
    "\n",
    "    if len(self.tetrode_data) == 0:\n",
    "        return\n",
    "\n",
    "    # organize the cut data\n",
    "    n_spikes_expected = self.tetrode_data.shape[1]\n",
    "    n_spikes = len(np.asarray([item for sublist in self.cell_indices.values() for item in sublist]))\n",
    "\n",
    "    # check that with the manipulation of the spikes, that we still have the correct number of spikes\n",
    "    if n_spikes != n_spikes_expected:\n",
    "        self.choice = None\n",
    "        self.LogError.signal.emit('cutSizeError')\n",
    "        while self.choice is None:\n",
    "            time.sleep(0.1)\n",
    "        return\n",
    "\n",
    "    # we will check if we are missing some of the spikes somehow. If we kept track of them, then the indices from\n",
    "    # the spikes, when sorted, should produce an array from 0 -> N-1 spikes.\n",
    "    if not np.array_equal(np.sort(np.asarray([item for sublist in self.cell_indices.values() for item in sublist])),\n",
    "                      np.arange(len(self.cut_data_original))):\n",
    "        self.choice = None\n",
    "        self.LogError.signal.emit('cutIndexError')\n",
    "        while self.choice is None:\n",
    "            time.sleep(0.1)\n",
    "        return\n",
    "\n",
    "    cut_values = np.zeros(n_spikes)\n",
    "    for cell, cell_indices in self.cell_indices.items():\n",
    "        cut_values[cell_indices] = cell\n",
    "\n",
    "    if '.clu.' in save_filename:\n",
    "        # save the .clu filename\n",
    "        write_clu(save_filename, cut_values)\n",
    "        self.choice = None\n",
    "        self.LogError.signal.emit('saveCompleteClu')\n",
    "        while self.choice is None:\n",
    "            time.sleep(0.1)\n",
    "        self.actions_made = False\n",
    "\n",
    "    else:\n",
    "        # save the cut filename\n",
    "        write_cut(save_filename, cut_values)\n",
    "        self.choice = None\n",
    "        self.LogError.signal.emit('saveComplete')\n",
    "        while self.choice is None:\n",
    "            time.sleep(0.1)\n",
    "        self.actions_made = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01729f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From \n",
    "# https://github.com/GeoffBarrett/gebaSpike/blob/967097ec28592182ef9783d2d391930e1c63ca58/gebaSpike/core/writeCut.py\n",
    "\n",
    "def write_cut(cut_filename, cut, basename=None):\n",
    "    if basename is None:\n",
    "        basename = os.path.basename(os.path.splitext(cut_filename)[0])\n",
    "\n",
    "    unique_cells = np.unique(cut)\n",
    "\n",
    "    if 0 not in unique_cells:\n",
    "        # if it happens that there is no zero cell, add it anyways\n",
    "        unique_cells = np.insert(unique_cells, 0, 0)  # object, index, value to insert\n",
    "\n",
    "    n_clusters = len(np.unique(cut))\n",
    "    n_spikes = len(cut)\n",
    "\n",
    "    write_list = []  # the list of values to write\n",
    "\n",
    "    tab = '    '  # the spaces didn't line up with my tab so I just created a string with enough spaces\n",
    "    empty_space = '               '  # some of the empty spaces don't line up to x tabs\n",
    "\n",
    "    # we add 1 to n_clusters because zero is the garbage cell that no one uses\n",
    "    write_list.append('n_clusters: %d\\n' % (n_clusters))\n",
    "    write_list.append('n_channels: 4\\n')\n",
    "    write_list.append('n_params: 2\\n')\n",
    "    write_list.append('times_used_in_Vt:%s' % ((tab + '0') * 4 + '\\n'))\n",
    "\n",
    "    zero_string = (tab + '0') * 8 + '\\n'\n",
    "\n",
    "    for cell_i in np.arange(n_clusters):\n",
    "        write_list.append(' cluster: %d center:%s' % (cell_i, zero_string))\n",
    "        write_list.append('%smin:%s' % (empty_space, zero_string))\n",
    "        write_list.append('%smax:%s' % (empty_space, zero_string))\n",
    "    write_list.append('\\nExact_cut_for: %s spikes: %d\\n' % (basename, n_spikes))\n",
    "\n",
    "    # now the cut file lists 25 values per row\n",
    "    n_rows = int(np.floor(n_spikes / 25))  # number of full rows\n",
    "\n",
    "    remaining = int(n_spikes - n_rows * 25)\n",
    "    cut_string = ('%3u' * 25 + '\\n') * n_rows + '%3u' * remaining\n",
    "\n",
    "    write_list.append(cut_string % (tuple(cut)))\n",
    "\n",
    "    with open(cut_filename, 'w') as f:\n",
    "        f.writelines(write_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07cc054b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From \n",
    "# https://github.com/GeoffBarrett/gebaSpike/blob/967097ec28592182ef9783d2d391930e1c63ca58/gebaSpike/core/writeCut.py\n",
    "\n",
    "def write_clu(clu_filename, data):\n",
    "    # the .clu files and the .cut files are different since the .clu files are the .cut files (with no manual sorting)\n",
    "    # without the headers, and the values go from 1 -> N instead of 0 -> N, (1-based numbering instead of 0-based). Thus\n",
    "    # we add 1 to the .cut data to get the .clu data\n",
    "\n",
    "    data = np.asarray(data).astype(int)  # ensuring that the data is the integer data-type\n",
    "\n",
    "    data += 1  # making the data 1-based instead of 0-based\n",
    "\n",
    "    # calculating the number of clusters\n",
    "    n_clust = len(np.unique(data))\n",
    "\n",
    "    # ensuring that the cluster number is the 1st value\n",
    "    data = np.concatenate(([n_clust], data))\n",
    "\n",
    "    # saving the data as a column (delimter='\\n') and integer format.\n",
    "    np.savetxt(clu_filename, data, fmt='%d', delimiter='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "230e729d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From \n",
    "# https://github.com/HussainiLab/BinConverter/blob/master/BinConverter/core/ConvertTetrode.py\n",
    "\n",
    "import os\n",
    "from BinConverter.core.conversion_utils import get_set_header\n",
    "import numpy as np\n",
    "import struct\n",
    "\n",
    "\n",
    "def write_tetrode(filepath, data, Fs):\n",
    "\n",
    "    session_path, session_filename = os.path.split(filepath)\n",
    "    tint_basename = os.path.splitext(session_filename)[0]\n",
    "    set_filename = os.path.join(session_path, '%s.set' % tint_basename)\n",
    "\n",
    "    n = len(data)\n",
    "\n",
    "    header = get_set_header(set_filename)\n",
    "\n",
    "    with open(filepath, 'w') as f:\n",
    "        num_chans = 'num_chans 4'\n",
    "        timebase_head = '\\ntimebase %d hz' % (96000)\n",
    "        bp_timestamp = '\\nbytes_per_timestamp %d' % (4)\n",
    "        # samps_per_spike = '\\nsamples_per_spike %d' % (int(Fs*1e-3))\n",
    "        samps_per_spike = '\\nsamples_per_spike %d' % (50)\n",
    "        sample_rate = '\\nsample_rate %d hz' % (Fs)\n",
    "        b_p_sample = '\\nbytes_per_sample %d' % (1)\n",
    "        # b_p_sample = '\\nbytes_per_sample %d' % (4)\n",
    "        spike_form = '\\nspike_format t,ch1,t,ch2,t,ch3,t,ch4'\n",
    "        num_spikes = '\\nnum_spikes %d' % (n)\n",
    "        start = '\\ndata_start'\n",
    "\n",
    "        write_order = [header, num_chans, timebase_head,\n",
    "                       bp_timestamp,\n",
    "                       samps_per_spike, sample_rate, b_p_sample, spike_form, num_spikes, start]\n",
    "\n",
    "        f.writelines(write_order)\n",
    "\n",
    "    # rearranging the data to have a flat array of t1, waveform1, t2, waveform2, t3, waveform3, etc....\n",
    "    spike_times = np.asarray(sorted(data.keys()))\n",
    "\n",
    "    # the spike times are repeated for each channel so lets tile this\n",
    "    spike_times = np.tile(spike_times, (4, 1))\n",
    "    spike_times = spike_times.flatten(order='F')\n",
    "\n",
    "    spike_values = np.asarray([value for (key, value) in sorted(data.items())])\n",
    "\n",
    "    # this will create a (n_samples, n_channels, n_samples_per_spike) => (n, 4, 50) sized matrix, we will create a\n",
    "    # matrix of all the samples and channels going from ch1 -> ch4 for each spike time\n",
    "    # time1 ch1_data\n",
    "    # time1 ch2_data\n",
    "    # time1 ch3_data\n",
    "    # time1 ch4_data\n",
    "    # time2 ch1_data\n",
    "    # time2 ch2_data\n",
    "    # .\n",
    "    # .\n",
    "    # .\n",
    "\n",
    "    spike_values = spike_values.reshape((n * 4, 50))  # create the 4nx50 channel data matrix\n",
    "\n",
    "    # make the first column the time values\n",
    "    spike_array = np.hstack((spike_times.reshape(len(spike_times), 1), spike_values))\n",
    "\n",
    "    data = None\n",
    "    spike_times = None\n",
    "    spike_values = None\n",
    "\n",
    "    spike_n = spike_array.shape[0]\n",
    "\n",
    "    t_packed = struct.pack('>%di' % spike_n, *spike_array[:, 0].astype(int))\n",
    "    spike_array = spike_array[:, 1:]  # removing time data from this matrix to save memory\n",
    "\n",
    "    spike_data_pack = struct.pack('<%db' % (spike_n*50), *spike_array.astype(int).flatten())\n",
    "\n",
    "    spike_array = None\n",
    "\n",
    "    # now we need to combine the lists by alternating\n",
    "\n",
    "    comb_list = [None] * (2*spike_n)\n",
    "    comb_list[::2] = [t_packed[i:i + 4] for i in range(0, len(t_packed), 4)]  # breaks up t_packed into a list,\n",
    "    # each timestamp is one 4 byte integer\n",
    "    comb_list[1::2] = [spike_data_pack[i:i + 50] for i in range(0, len(spike_data_pack), 50)]  # breaks up spike_data_\n",
    "    # pack and puts it into a list, each spike is 50 one byte integers\n",
    "\n",
    "    t_packed = None\n",
    "    spike_data_pack = None\n",
    "\n",
    "    write_order = []\n",
    "    with open(filepath, 'rb+') as f:\n",
    "\n",
    "        write_order.extend(comb_list)\n",
    "        write_order.append(bytes('\\r\\ndata_end\\r\\n', 'utf-8'))\n",
    "\n",
    "        f.seek(0, 2)\n",
    "        f.writelines(write_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d90289",
   "metadata": {},
   "source": [
    "<a id=\"Convert_sorting_extractor_to_tint\"></a>\n",
    "## Convert Sorting extractor to TINT\n",
    "[back to index](#index)\n",
    "\n",
    "There are several points in the pipeline at which we might want to export to TINT. Ideally it should work for any `SortingExtractor` object!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fe21ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where do we load data from?\n",
      " /mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/sample_bin_to_tint_no_bin\n"
     ]
    }
   ],
   "source": [
    "print('Where do we load data from?\\n', dir_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae069b09",
   "metadata": {},
   "source": [
    "From a sorting extractor we can obtain a list unit spike sample arrays. We can convert this to the .clu or .cut type array of unit ID labels for each spike.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0fc1e882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20201004_Tint_1\n"
     ]
    }
   ],
   "source": [
    "cut_filename = Path('/mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/Axona_Tint_1ms/20201004_Tint_1.cut')\n",
    "\n",
    "basename = os.path.basename(os.path.splitext(cut_filename)[0])\n",
    "\n",
    "print(basename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8bae0e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/Axona_Tint_1ms/20201004_Tint.set\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('/mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/Axona_Tint_1ms/20201004_Tint_1.cut')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = Path('/mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/Axona_Tint_1ms/20201004_Tint.set')\n",
    "print(filename)\n",
    "\n",
    "Path(str(filename.with_suffix('')) + '_{}'.format(1) + '.cut')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8855a9c0",
   "metadata": {},
   "source": [
    "### Write unit labels to .cut and .clu files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2270a472",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_spike_train_to_label_array(spike_train):\n",
    "    '''Takes a list of arrays, where each array is a series of\n",
    "    sample points at which a spike occured for a given unit\n",
    "    (each list item is a unit). Converts to .cut array, i.e.\n",
    "    orders spike samples from all units and labels each sample\n",
    "    with the appropriate unit ID.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    spike_train : List of np.arrays\n",
    "        Output of `get_units_spike_train()` method of sorting extractor\n",
    "        \n",
    "    Return\n",
    "    ------\n",
    "    unit_labels_sorted : np.array\n",
    "        Each entry is the unit ID corresponding to the spike sample that\n",
    "        occured at this ordinal position\n",
    "    '''\n",
    "\n",
    "    # Generate Index array (indexing the unit for a given spike sample)\n",
    "    unit_labels = []\n",
    "    for i, l in enumerate(spike_train):\n",
    "        unit_labels.append(np.ones((len(l),), dtype=int) * i)\n",
    "    \n",
    "    # Flatten lists and sort them\n",
    "    spike_train_flat = np.concatenate(spike_train).ravel()\n",
    "    unit_labels_flat = np.concatenate(unit_labels).ravel()\n",
    "\n",
    "    sort_index = np.argsort(spike_train_flat)\n",
    "\n",
    "    unit_labels_sorted = unit_labels_flat[sort_index]\n",
    "\n",
    "    return unit_labels_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8215d02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_cut_file(cut_filename, unit_labels):\n",
    "    '''Write spike sorting output to .cut file.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    cut_filename : str or Path\n",
    "        Full filename of .cut file to write to. A given .cut file belongs\n",
    "        to a given tetrode file. For example, for tetrode `my_file.1`, the\n",
    "        corresponding cut_filename should be `my_file_1.cut`.\n",
    "    unit_labels : np.array\n",
    "        Vector of unit labels for each spike sample (ordered by time of \n",
    "        occurence)\n",
    "        \n",
    "    Example\n",
    "    -------\n",
    "    # Given a sortingextractor called sorting_nwb:\n",
    "    spike_train = sorting_nwb.get_units_spike_train()\n",
    "    unit_labels = convert_spike_train_to_label_array(spike_train)\n",
    "    write_to_cut_file(cut_filename, unit_labels)\n",
    "    \n",
    "    ---\n",
    "    Largely based on gebaSpike implementation by Geoff Barrett\n",
    "    https://github.com/GeoffBarrett/gebaSpike\n",
    "    '''\n",
    "\n",
    "    unique_cells = np.unique(unit_labels)\n",
    "\n",
    "    n_clusters = len(np.unique(unit_labels))\n",
    "    n_spikes = len(unit_labels)\n",
    "\n",
    "    write_list = []\n",
    "\n",
    "    tab = '    '\n",
    "    empty_space = '               '\n",
    "\n",
    "    write_list.append('n_clusters: %d\\n' % (n_clusters))\n",
    "    write_list.append('n_channels: 4\\n')\n",
    "    write_list.append('n_params: 2\\n')\n",
    "    write_list.append('times_used_in_Vt:%s' % ((tab + '0') * 4 + '\\n'))\n",
    "\n",
    "    zero_string = (tab + '0') * 8 + '\\n'\n",
    "\n",
    "    for cell_i in np.arange(n_clusters):\n",
    "        write_list.append(' cluster: %d center:%s' % (cell_i, zero_string))\n",
    "        write_list.append('%smin:%s' % (empty_space, zero_string))\n",
    "        write_list.append('%smax:%s' % (empty_space, zero_string))\n",
    "    write_list.append('\\nExact_cut_for: %s spikes: %d\\n' % (basename, n_spikes))\n",
    "\n",
    "    # The unit label array consists of 25 values per row in .cut file\n",
    "    n_rows = int(np.floor(n_spikes / 25))\n",
    "    remaining = int(n_spikes - n_rows * 25)\n",
    "\n",
    "    cut_string = ('%3u' * 25 + '\\n') * n_rows + '%3u' * remaining\n",
    "\n",
    "    write_list.append(cut_string % (tuple(unit_labels)))\n",
    "\n",
    "    with open(cut_filename, 'w') as f:\n",
    "        f.writelines(write_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2487be27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_clu_file(clu_filename, unit_labels):\n",
    "    ''' .clu files are pruned .cut files, containing only a long vector of unit\n",
    "    labels, which are 1-indexed, instead of 0-indexed. In addition, the very first\n",
    "    entry is the total number of units.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    clu_filename : str or Path\n",
    "        Full filename of .clu file to write to. A given .clu file belongs\n",
    "        to a given tetrode file. For example, for tetrode `my_file.1`, the\n",
    "        corresponding clu_filename should be `my_file_1.clu`.\n",
    "    unit_labels : np.array\n",
    "        Vector of unit labels for each spike sample (ordered by time of \n",
    "        occurence)\n",
    "        \n",
    "    ---\n",
    "    Largely based on gebaSpike implementation by Geoff Barrett\n",
    "    https://github.com/GeoffBarrett/gebaSpike\n",
    "    '''\n",
    "    unit_labels = np.asarray(unit_labels).astype(int)\n",
    "    unit_labels += 1\n",
    "\n",
    "    n_clust = len(np.unique(unit_labels))\n",
    "    unit_labels = np.concatenate(([n_clust], unit_labels))\n",
    "\n",
    "    np.savetxt(clu_filename, unit_labels, fmt='%d', delimiter='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "995d9aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_cut_filename_from_basename(filename, tetrode_id):\n",
    "    '''Given a str or Path object, assume the last entry after a slash\n",
    "    is a filename, strip any file suffix, add tetrode ID label, and\n",
    "    .cut suffix to name.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : str or Path\n",
    "    tetrode_id : int\n",
    "    '''\n",
    "    return Path(str(filename).split('.')[0] + '_{}'.format(tetrode_id) + '.cut')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "fadbd312",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_unit_labels_to_file(sorting_extractor, filename):\n",
    "    '''Write spike sorting output to .cut and .clu file, separately for each\n",
    "    tetrode.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sorting_extractor : spikeextractors.SortingExtractor\n",
    "    filename : str or Path\n",
    "        Full filename of .set file or base-filename (i.e. the part of the\n",
    "        filename all Axona files have in common). A given .cut file belongs\n",
    "        to a given tetrode file. For example, for tetrode `my_file.1`, the\n",
    "        corresponding cut_filename should be `my_file_1.cut`. This will be\n",
    "        set automatically given the base-filename or set file.\n",
    "        \n",
    "    TODO: Any reason one might want to only convert some tetrodes or some\n",
    "    samples? Should those be parameters?\n",
    "    '''\n",
    "    tetrode_ids = sorting_extractor.get_units_property(property_name='group')\n",
    "    tetrode_ids = np.array(tetrode_ids)\n",
    "    \n",
    "    unit_ids = np.array(sorting_extractor.get_unit_ids())\n",
    "    \n",
    "    for i in np.unique(tetrode_ids):\n",
    "        \n",
    "        print('Write unit labels for tetrode {} to .cut and .clu'.format(i))\n",
    "\n",
    "        spike_train = sorting_extractor.get_units_spike_train(unit_ids=unit_ids[tetrode_ids==i])\n",
    "        unit_labels = convert_spike_train_to_label_array(spike_train)\n",
    "\n",
    "        # We use Axona conventions for filenames (tetrodes are 1 indexed)\n",
    "        cut_filename = set_cut_filename_from_basename(filename, i + 1)\n",
    "        clu_filename = Path(str(cut_filename).replace('.cut', '.clu'))\n",
    "\n",
    "        write_to_cut_file(cut_filename, unit_labels)\n",
    "        write_to_clu_file(clu_filename, unit_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "001d00c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spikeextractors.extractors.nwbextractors.nwbextractors.NwbSortingExtractor'>\n"
     ]
    }
   ],
   "source": [
    "# We have sorting data exported in `.nwb` format\n",
    "\n",
    "nwb_dir = Path(dir_name, 'nwb')\n",
    "sorting_nwb = se.NwbSortingExtractor(nwb_dir / 'axona_se_MS4.nwb', sampling_frequency=48000)\n",
    "\n",
    "print(type(sorting_nwb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "45c9d08a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling frequency: 48000 Hz\n"
     ]
    }
   ],
   "source": [
    "print('Sampling frequency:', sorting_nwb.get_sampling_frequency(), 'Hz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0ddef20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting Tetrode 0\n",
      "Converting Tetrode 1\n",
      "Converting Tetrode 2\n",
      "Converting Tetrode 3\n"
     ]
    }
   ],
   "source": [
    "# Convert all tetrodes from sorting extractor to cut files\n",
    "write_unit_labels_to_file(sorting_nwb, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f79ce30",
   "metadata": {},
   "source": [
    "### Write waveforms from `sortingextractor` to tetrode files (`.X`)\n",
    "\n",
    "Here, we need information that is available in the `.set` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2db589",
   "metadata": {},
   "outputs": [],
   "source": [
    "from BinConverter.core.ConvertTetrode import write_tetrode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ab1f7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_generic_header(filename):\n",
    "    \"\"\"\n",
    "    Given a binary file with phrases and line breaks, enters the\n",
    "    first word of a phrase as dictionary key and the following\n",
    "    string (without linebreaks) as value. Returns the dictionary.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : str or Path\n",
    "        Full filename.\n",
    "    \"\"\"\n",
    "    header = {}\n",
    "    with open(filename, 'rb') as f:\n",
    "        for bin_line in f:\n",
    "            if b'data_start' in bin_line:\n",
    "                break\n",
    "            line = bin_line.decode('cp1252').replace('\\r\\n', '').replace('\\r', '').strip()\n",
    "            parts = line.split(' ')\n",
    "            key = parts[0]\n",
    "            value = ' '.join(parts[1:])\n",
    "            header[key] = value\n",
    "            \n",
    "    return header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2df8d69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unit_group_ids(sorting):\n",
    "    '''Get group ids.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sorting : SortingExtractor\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    group_ids : List\n",
    "        List of groups ids for each Unit in `sorting`.\n",
    "    '''\n",
    "    unit_ids = sorting.get_unit_ids()\n",
    "    group_ids = [sorting.get_unit_property(\n",
    "        unit_id=unit_id, property_name='group') for unit_id in unit_ids\n",
    "    ]\n",
    "\n",
    "    return group_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4493ba1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_units_on_tetrode(group_spike_samples, group_waveforms):\n",
    "    '''Write all waveforms of given tetrode in dictionary with the\n",
    "    corresponding spike samples being the keys (1 sample for each\n",
    "    waveform).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    group_spike_samples : list\n",
    "        As returned by sortingextractor.get_units_spike_train()\n",
    "    group_waveforms : list\n",
    "        As returned by spiketoolkit.postprocessing.get_unit_waveforms()\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tetrode_spikes : dict\n",
    "        Keys are spike samples, values are waveforms (ntrls x nch x nsamp)\n",
    "    '''\n",
    "    tetrode_spikes = {}\n",
    "\n",
    "    for i, (samples, waveforms) in enumerate(zip(group_spike_samples, group_waveforms)):\n",
    "\n",
    "        for sample, waveform in zip(samples, waveforms):\n",
    "\n",
    "            tetrode_spikes[sample] = waveform\n",
    "            \n",
    "    return tetrode_spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "070cc763",
   "metadata": {},
   "outputs": [],
   "source": [
    "st.postprocessing.get_unit_waveforms??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "70dbf774",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_waveforms(recording, sorting, unit_ids, header):\n",
    "    '''Get waveforms for specific tetrode.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    recording : RecordingExtractor\n",
    "    sorting : SortingExtractor\n",
    "    unit_ids : List\n",
    "        List of unit ids to extract waveforms\n",
    "    header : dict\n",
    "        maps parameters from .set file to their values (as strings).\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    waveforms : List\n",
    "        List of np.array (n_spikes, n_channels, n_timepoints) with waveforms for each unit\n",
    "    '''\n",
    "    sampling_rate = recording.get_sampling_frequency()\n",
    "    samples_before = int(header['pretrigSamps'])\n",
    "    samples_after = int(header['spikeLockout'])\n",
    "\n",
    "    ms_before = samples_before / (sampling_rate / 1000) + 0.001\n",
    "    ms_after = samples_after / (sampling_rate / 1000) + 0.001\n",
    "\n",
    "    waveforms = st.postprocessing.get_unit_waveforms(\n",
    "        recording,\n",
    "        sorting,\n",
    "        unit_ids=unit_ids,\n",
    "        max_spikes_per_unit=None, \n",
    "        grouping_property='group',\n",
    "        recompute_info=True,\n",
    "        ms_before=ms_before,\n",
    "        ms_after=ms_after,\n",
    "        return_idxs=False,\n",
    "        return_scaled=False,\n",
    "        dtype=np.int8\n",
    "    )\n",
    "\n",
    "    return waveforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f268615f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_tetrode_files(recording, sorting, group_ids, set_file):\n",
    "    '''Get spike samples and waveforms for all tetrodes specified in\n",
    "    `group_ids`. Note that `group_ids` is 0-indexed, whereas tetrodes are\n",
    "    1-indexed (so if you want tetrodes 1+2, specify group_ids=[0, 1]).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    recording : RecordingExtractor\n",
    "    sorting : SortingExtractor\n",
    "    group_ids : array like\n",
    "        Tetrodes to include, but 0-indexed (i.e. tetrodeID - 1)\n",
    "    set_file : Path or str\n",
    "        .set file location. Used to determine how many samples prior to and\n",
    "        post spike sample should be cut out for each waveform. .X files will have\n",
    "        the same base filename as the .set file. So if you do not want to overwrite\n",
    "        existing .X files in your .set file directory, copy the .set file to a new\n",
    "        folder and give its new location. The new .X files will appear there.\n",
    "    '''\n",
    "    sampling_rate = recording.get_sampling_frequency()\n",
    "    group_ids = get_unit_group_ids(sorting)\n",
    "    header = parse_generic_header(set_file)\n",
    "\n",
    "    for group_id in np.unique(group_ids):\n",
    "\n",
    "        # get spike samples and waveforms of this group / tetrode\n",
    "        group_unit_ids = [i for i, gid in enumerate(group_ids) if gid==group_id]\n",
    "        group_waveforms = get_waveforms(recording, sorting, group_unit_ids, header)\n",
    "        group_spike_samples = sorting.get_units_spike_train(unit_ids=group_unit_ids)\n",
    "\n",
    "        # Assign each waveform to it's spike sample in a dictionary\n",
    "        spike_waveform_dict = combine_units_on_tetrode(group_spike_samples, group_waveforms)\n",
    "\n",
    "        # Set tetrode filename\n",
    "        tetrode_filename = str(set_file).split('.')[0] + '.{}'.format(group_id + 1)\n",
    "        print('Writing', Path(tetrode_filename).name)\n",
    "\n",
    "        # Use `BinConverter` function to write to tetrode file\n",
    "        write_tetrode(tetrode_filename, spike_waveform_dict, sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "4c8782d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/sample_bin_to_tint_no_bin/axona_sample.set\n"
     ]
    }
   ],
   "source": [
    "set_file = dir_name / 'axona_sample.set'\n",
    "print(set_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "26f6b729",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spikeextractors.extractors.axonaunitrecordingextractor import AxonaUnitRecordingExtractor\n",
    "\n",
    "recording = AxonaUnitRecordingExtractor(filename=set_file)\n",
    "signal = recording.get_traces(channel_ids=None, start_frame=None, end_frame=None, return_scaled=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c6117c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/sample_bin_to_tint_no_bin/conversion_to_tint/axona_sample.set\n"
     ]
    }
   ],
   "source": [
    "set_file_to_tint = dir_name / 'conversion_to_tint' / 'axona_sample.set'\n",
    "print(set_file_to_tint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a713f68e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>channel_ids</th>\n",
       "      <th>channel_groups</th>\n",
       "      <th>tetrode_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    channel_ids  channel_groups  tetrode_ids\n",
       "0             0               0            1\n",
       "1             1               0            1\n",
       "2             2               0            1\n",
       "3             3               0            1\n",
       "4             4               1            2\n",
       "5             5               1            2\n",
       "6             6               1            2\n",
       "7             7               1            2\n",
       "8             8               2            3\n",
       "9             9               2            3\n",
       "10           10               2            3\n",
       "11           11               2            3\n",
       "12           12               3            4\n",
       "13           13               3            4\n",
       "14           14               3            4\n",
       "15           15               3            4"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'channel_ids': recording.get_channel_ids(),\n",
    "    'channel_groups': recording.get_channel_groups(),\n",
    "    'tetrode_ids': recording.get_channel_groups() + 1\n",
    "})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "6ef24b4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channel_groups = recording.get_channel_groups()\n",
    "channel_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "de3461b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing axona_sample.1\n",
      "Writing axona_sample.2\n",
      "Writing axona_sample.3\n",
      "Writing axona_sample.4\n"
     ]
    }
   ],
   "source": [
    "write_to_tetrode_files(recording, sorting_nwb, channel_groups, set_file_to_tint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4c17b1bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ecbe4011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72,) (72, 4, 50)\n",
      "(101,) (101, 4, 50)\n",
      "(59,) (59, 4, 50)\n",
      "(15,) (15, 4, 50)\n",
      "(39,) (39, 4, 50)\n",
      "(51,) (51, 4, 50)\n",
      "(38,) (38, 4, 50)\n",
      "(50,) (50, 4, 50)\n",
      "(47,) (47, 4, 50)\n",
      "(73,) (73, 4, 50)\n",
      "(47,) (47, 4, 50)\n",
      "(42,) (42, 4, 50)\n",
      "(67,) (67, 4, 50)\n",
      "(52,) (52, 4, 50)\n"
     ]
    }
   ],
   "source": [
    "# On the .X file there will be no more unit information, we will write all units from a given\n",
    "# tetrode for each tetrode channel\n",
    "\n",
    "header = parse_generic_header(set_file_to_tint)\n",
    "group_unit_ids = None\n",
    "\n",
    "waveforms = get_waveforms(recording, sorting_nwb, group_unit_ids, header)\n",
    "spike_samples = sorting_nwb.get_units_spike_train()\n",
    "for spk_trn, wv in zip(spike_samples, waveforms):\n",
    "    print(spk_trn.shape, wv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "eb0ead79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "memmap([[-2, -2, -1, -4,  0,  3, -5,  6, -3,  0,  4,  0, -1,  2, -1, -2,\n",
       "         -3,  2,  3,  6,  1,  0, -1,  0,  0,  2, -3, -1, -1,  0,  0, -8,\n",
       "          1, -1,  1, -1,  0,  1,  0, -7,  1,  1, -1,  0,  1, -6,  1,  0,\n",
       "          0,  0],\n",
       "        [ 3, -1, -2, -3,  1,  0,  0,  0,  0,  3,  1,  5,  1, -4,  3,  2,\n",
       "         -4,  1,  0, -2, -3,  5,  0, -2, -5,  2, 13, -2,  3,  0,  0, -1,\n",
       "          3,  1, -1,  5, -1, -2,  1, -2, -8,  2,  0, -1,  0,  8,  2,  4,\n",
       "         -4, -4],\n",
       "        [ 3,  1,  1, -6, -4, -5, -1,  4,  0,  2, -1,  0, -3,  1,  2, -2,\n",
       "         -8,  2, -1,  5, -2,  0, -8,  0, -4, -2,  3, -1, -3,  3,  1, -1,\n",
       "         -1, -1,  0,  2,  0,  0,  0,  0, -6,  4,  0,  0,  1,  2,  7, -2,\n",
       "         -1,  1],\n",
       "        [ 1,  6,  1,  0,  1,  2,  1, -4,  2, -8,  6,  3,  3, -5,  1, -2,\n",
       "          2, -3,  3,  1,  0,  2,  3,  0, -4, -4, -2, -5, -2, -1,  0,  0,\n",
       "          6,  2,  0, -1, -1,  0, -1,  0, -1, -3,  1,  9,  1, -2,  1,  0,\n",
       "         -1,  0]], dtype=int8)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "waveforms[0][0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e5416dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get spike samples and waveforms of this group / tetrode\n",
    "group_unit_ids = [i for i, gid in enumerate(group_ids) if gid==group_id]\n",
    "group_waveforms = get_waveforms(recording, sorting, group_unit_ids, header)\n",
    "group_spike_samples = sorting.get_units_spike_train(unit_ids=group_unit_ids)\n",
    "\n",
    "# Assign each waveform to it's spike sample in a dictionary\n",
    "spike_waveform_dict = combine_units_on_tetrode(group_spike_samples, group_waveforms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e56fef51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_id = 0\n",
    "group_unit_ids = [i for i, gid in enumerate(group_ids) if gid==group_id]\n",
    "group_unit_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ccf36e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/sample_bin_to_tint_no_bin/conversion_to_tint/axona_sample.1\n",
      "(173, 4, 50)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "memmap([[-2, -2, -1, -4,  0,  3, -5,  6, -3,  0,  4,  0, -1,  2, -1, -2,\n",
       "         -3,  2,  3,  6,  1,  0, -1,  0,  0,  2, -3, -1, -1,  0,  0, -8,\n",
       "          1, -1,  1, -1,  0,  1,  0, -7,  1,  1, -1,  0,  1, -6,  1,  0,\n",
       "          0,  0],\n",
       "        [ 3, -1, -2, -3,  1,  0,  0,  0,  0,  3,  1,  5,  1, -4,  3,  2,\n",
       "         -4,  1,  0, -2, -3,  5,  0, -2, -5,  2, 13, -2,  3,  0,  0, -1,\n",
       "          3,  1, -1,  5, -1, -2,  1, -2, -8,  2,  0, -1,  0,  8,  2,  4,\n",
       "         -4, -4],\n",
       "        [ 3,  1,  1, -6, -4, -5, -1,  4,  0,  2, -1,  0, -3,  1,  2, -2,\n",
       "         -8,  2, -1,  5, -2,  0, -8,  0, -4, -2,  3, -1, -3,  3,  1, -1,\n",
       "         -1, -1,  0,  2,  0,  0,  0,  0, -6,  4,  0,  0,  1,  2,  7, -2,\n",
       "         -1,  1],\n",
       "        [ 1,  6,  1,  0,  1,  2,  1, -4,  2, -8,  6,  3,  3, -5,  1, -2,\n",
       "          2, -3,  3,  1,  0,  2,  3,  0, -4, -4, -2, -5, -2, -1,  0,  0,\n",
       "          6,  2,  0, -1, -1,  0, -1,  0, -1, -3,  1,  9,  1, -2,  1,  0,\n",
       "         -1,  0]], dtype=int8)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read data back in to see if it worked as expected\n",
    "\n",
    "tetrode_filename = \\\n",
    "    '/mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/sample_bin_to_tint_no_bin/conversion_to_tint/axona_sample.1'\n",
    "print(tetrode_filename)\n",
    "\n",
    "from neo import AxonaIO\n",
    "\n",
    "neoio = AxonaIO(tetrode_filename)\n",
    "\n",
    "waveforms = neoio.get_spike_raw_waveforms()\n",
    "print(waveforms.shape)\n",
    "waveforms[1, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1950dcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c2054a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0fa946",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7e6503",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587ac0c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8e3979",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "1e81b2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_tint(recording, sorting, set_file):\n",
    "    '''Given a sorting extractor object, write appropriate data\n",
    "    to TINT format (from Axona). Will therefore create .X (tetrode),\n",
    "    .cut and .clu (spike sorting information) files.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    recording : spikeextractors.RecordingExtractor\n",
    "    sorting : spikeextractors.SortingExtractor\n",
    "    set_file : Path or str\n",
    "        .set file location. Used to determine how many samples prior to and\n",
    "        post spike sample should be cut out for each waveform. .X files will have\n",
    "        the same base filename as the .set file. So if you do not want to overwrite\n",
    "        existing .X files in your .set file directory, copy the .set file to a new\n",
    "        folder and give its new location. The new files will appear there.\n",
    "        \n",
    "    Notes\n",
    "    -----\n",
    "    For details about the .X file format see:\n",
    "    http://space-memory-navigation.org/DacqUSBFileFormats.pdf\n",
    "    '''\n",
    "    \n",
    "    # writes to .X files for each tetrode\n",
    "    group_ids = recording.get_channel_groups()\n",
    "    write_to_tetrode_files(recording, sorting, group_ids, set_file)\n",
    "    \n",
    "    # writes to .cut and .clu files for each tetrode\n",
    "    write_unit_labels_to_file(sorting, set_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15691ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_tint(recording, sorting_nwb, set_file_to_tint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4051f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490f3329",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13b87295",
   "metadata": {},
   "source": [
    "## Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e684fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_values(x, maxabs, bound=127):\n",
    "    '''Scale signal `x` between -`bound` and +`bound`,\n",
    "    preserves 0 point.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : np.array\n",
    "    absmax : numeric\n",
    "        max(|min(x)|, |max(x)|)\n",
    "    bound : numeric\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    np.array\n",
    "    '''\n",
    "    return x / maxabs * bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db6c27f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spikeinterface",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
