{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38378a04",
   "metadata": {},
   "source": [
    "# Convert recording and sorting extractor data to TINT format\n",
    "\n",
    "The Hussaini lab uses the proprietary TINT software from Axona to analyze extracellular electrophysiology data. While we are already able to read various data formats from Axona (`raw` data or `unit` data) into spikeinterface, perform preprocessing, spike sorting and export the data to NWB, we also want to allow to export data to the TINT format. \n",
    "\n",
    "The TINT format is essentially the same as the `unit` data, including `.X` and `.pos` files, but also `.cut` or `.clu`. The latter two contain information about the spike sorted units.\n",
    "\n",
    "The conversion can be facilitated by using the existing tools from the Hussaini lab, which [convert `.bin` data to `.X` and `.pos`](https://github.com/HussainiLab/BinConverter/blob/master/BinConverter/core/ConversionFunctions.py). Some of this code is only relevant for using the GUI, which did not work for me. I cleared out GUI code and ran a conversion from `.bin` to `.X` and `.pos` in this notebook: [explore_hussaini_tools.ipynb](https://github.com/sbuergers/hussaini-lab-to-nwb-notebooks/blob/master/explore_hussaini_tools.ipynb).\n",
    "\n",
    "They also already wrote a [`write_cut()`](https://github.com/GeoffBarrett/gebaSpike/blob/967097ec28592182ef9783d2d391930e1c63ca58/gebaSpike/core/writeCut.py) function.\n",
    "\n",
    "We can test our solutions by reading data with these [Hussaini lab tools](https://github.com/HussainiLab/BinConverter/blob/master/BinConverter/core/Tint_Matlab.py). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8f7aaa",
   "metadata": {},
   "source": [
    "<a id='index'></a>\n",
    "## Index\n",
    "\n",
    "* [Testing functions](#testing_functions)\n",
    "* [Hussaini-lab functions](#hussaini-lab_functions)\n",
    "* [Convert Recording Extractor to TINT](#Convert_recording_extractor_to_tint)\n",
    "* [Convert Sorting Extractor to TINT](#Convert_sorting_extractor_to_tint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83cf3093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.5 (default, Sep  4 2020, 07:30:14) \n",
      "[GCC 7.3.0] linux /home/sbuergers/spikeinterface/spikeinterface_new_api/venv/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (16, 8)\n",
    "plt.rcParams.update({'font.size':14})\n",
    "%matplotlib inline\n",
    "\n",
    "import spikeextractors as se\n",
    "import spiketoolkit as st\n",
    "\n",
    "print(sys.version, sys.platform, sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1210bfc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input directory =  /mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/sample_bin_to_tint_no_bin\n",
      "Output directory =  /mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/sample_bin_to_tint_no_bin/conversion_to_tint\n"
     ]
    }
   ],
   "source": [
    "# Directories\n",
    "\n",
    "dir_name = Path('/mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/sample_bin_to_tint_no_bin')\n",
    "print('Input directory = ', dir_name)\n",
    "\n",
    "save_dir = dir_name / 'conversion_to_tint'\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "print('Output directory = ', save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aafca1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read cached spikeextractors data\n",
    "\n",
    "r_cache = se.load_extractor_from_pickle(os.path.join(dir_name, 'cached_unit_data_no_bin_preproc.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69609f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sbuergers/spikeinterface/spikeinterface_new_api/venv/lib/python3.8/site-packages/hdmf/common/table.py:442: UserWarning: An attribute 'name' already exists on DynamicTable 'electrodes' so this column cannot be accessed as an attribute, e.g., table.name; it can only be accessed using other methods, e.g., table['name'].\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Read NWB recording data\n",
    "\n",
    "nwb_dir = Path(dir_name, 'nwb')\n",
    "recording_nwb = se.NwbRecordingExtractor(nwb_dir / 'axona_tutorial_re2.nwb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29ca4d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read NWB sorting data\n",
    "\n",
    "sorting_nwb = se.NwbSortingExtractor(nwb_dir / 'axona_se_MS4.nwb', sampling_frequency=48000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43beb92a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spikeextractors.extractors.bindatrecordingextractor.bindatrecordingextractor.BinDatRecordingExtractor'>\n",
      "<class 'spikeextractors.extractors.nwbextractors.nwbextractors.NwbRecordingExtractor'>\n",
      "<class 'spikeextractors.extractors.nwbextractors.nwbextractors.NwbSortingExtractor'>\n"
     ]
    }
   ],
   "source": [
    "# Show data types of different objects\n",
    "\n",
    "print(type(r_cache))\n",
    "print(type(recording_nwb))\n",
    "print(type(sorting_nwb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b12d160",
   "metadata": {},
   "source": [
    "<a id=\"testing_functions\"></a>\n",
    "## Testing functions\n",
    "[back to index](#index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93029f86",
   "metadata": {},
   "source": [
    "As we start exporting to putative TINT format, we will want to check if we can read it back in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3d0bd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spikeextractors.extractors.axonaunitrecordingextractor import AxonaUnitRecordingExtractor\n",
    "import os\n",
    "\n",
    "\n",
    "def test_axonaunitrecordingextractor(filename):\n",
    "    '''Reads UNIT data with AxonaUnitRecordingExtractor and\n",
    "    performs some simple operations as a sanity check. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : str or Path\n",
    "        Full filename of `.set` file (could be any extension actually)\n",
    "    '''\n",
    "    re = AxonaUnitRecordingExtractor(filename=filename)\n",
    "    \n",
    "    # TEST AXONARECORDINGEXTRACTOR\n",
    "    # Retrieve some simple recording information and print it\n",
    "    recording = re\n",
    "    print('Channel ids = {}'.format(recording.get_channel_ids()))\n",
    "    print('Num. channels = {}'.format(len(recording.get_channel_ids())))\n",
    "    print('Sampling frequency = {} Hz'.format(recording.get_sampling_frequency()))\n",
    "    print('Num. timepoints = {}'.format(recording.get_num_frames()))\n",
    "    print('Stdev. on third channel = {}'.format(np.std(recording.get_traces(channel_ids=2))))\n",
    "    print('Location of third electrode = {}'.format(\n",
    "        recording.get_channel_property(channel_id=2, property_name='location')))\n",
    "    print('Channel groups = {}'.format(recording.get_channel_groups()))\n",
    "    \n",
    "    # TEST NEO_READER (axonaio)\n",
    "    print(recording.neo_reader.header['signal_channels'])\n",
    "    \n",
    "    \n",
    "def test_tetrode_files(filename):\n",
    "    '''Reads UNIT data with AxonaUnitRecordingExtractor and\n",
    "    performs some simple operations as a sanity check. \n",
    "    Will only test .X  and .set files (no .clu or .cut, no .pos).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : str or Path\n",
    "        Full filename of `.set` file (could be any extension actually)\n",
    "    '''\n",
    "    test_axonaunitrecordingextractor(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6333895",
   "metadata": {},
   "source": [
    "<a id=\"hussaini-lab_functions\"></a>\n",
    "## Hussaini-lab functions\n",
    "[back to index](#index)\n",
    "\n",
    "`gebaSpike` actually wants already existing `.cut` or `.clu` files, and allows modifying them. So these might not be all that useful for exporting to `.cut` or `.clu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c221133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From \n",
    "# https://github.com/GeoffBarrett/gebaSpike/blob/967097ec28592182ef9783d2d391930e1c63ca58/gebaSpike/main.py\n",
    "\n",
    "def save_function(self):\n",
    "    \"\"\"\n",
    "    this method will save the .cut file\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if self.cut_filename.text() == default_filename:\n",
    "        return\n",
    "\n",
    "    save_filename = os.path.realpath(self.cut_filename.text())\n",
    "\n",
    "    if os.path.exists(save_filename):\n",
    "        self.choice = None\n",
    "        self.LogError.signal.emit('OverwriteCut!%s' % save_filename)\n",
    "        while self.choice is None:\n",
    "            time.sleep(0.1)\n",
    "\n",
    "        if self.choice != QtWidgets.QMessageBox.Yes:\n",
    "            return\n",
    "\n",
    "    if len(self.tetrode_data) == 0:\n",
    "        return\n",
    "\n",
    "    # organize the cut data\n",
    "    n_spikes_expected = self.tetrode_data.shape[1]\n",
    "    n_spikes = len(np.asarray([item for sublist in self.cell_indices.values() for item in sublist]))\n",
    "\n",
    "    # check that with the manipulation of the spikes, that we still have the correct number of spikes\n",
    "    if n_spikes != n_spikes_expected:\n",
    "        self.choice = None\n",
    "        self.LogError.signal.emit('cutSizeError')\n",
    "        while self.choice is None:\n",
    "            time.sleep(0.1)\n",
    "        return\n",
    "\n",
    "    # we will check if we are missing some of the spikes somehow. If we kept track of them, then the indices from\n",
    "    # the spikes, when sorted, should produce an array from 0 -> N-1 spikes.\n",
    "    if not np.array_equal(np.sort(np.asarray([item for sublist in self.cell_indices.values() for item in sublist])),\n",
    "                      np.arange(len(self.cut_data_original))):\n",
    "        self.choice = None\n",
    "        self.LogError.signal.emit('cutIndexError')\n",
    "        while self.choice is None:\n",
    "            time.sleep(0.1)\n",
    "        return\n",
    "\n",
    "    cut_values = np.zeros(n_spikes)\n",
    "    for cell, cell_indices in self.cell_indices.items():\n",
    "        cut_values[cell_indices] = cell\n",
    "\n",
    "    if '.clu.' in save_filename:\n",
    "        # save the .clu filename\n",
    "        write_clu(save_filename, cut_values)\n",
    "        self.choice = None\n",
    "        self.LogError.signal.emit('saveCompleteClu')\n",
    "        while self.choice is None:\n",
    "            time.sleep(0.1)\n",
    "        self.actions_made = False\n",
    "\n",
    "    else:\n",
    "        # save the cut filename\n",
    "        write_cut(save_filename, cut_values)\n",
    "        self.choice = None\n",
    "        self.LogError.signal.emit('saveComplete')\n",
    "        while self.choice is None:\n",
    "            time.sleep(0.1)\n",
    "        self.actions_made = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01729f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From \n",
    "# https://github.com/GeoffBarrett/gebaSpike/blob/967097ec28592182ef9783d2d391930e1c63ca58/gebaSpike/core/writeCut.py\n",
    "\n",
    "def write_cut(cut_filename, cut, basename=None):\n",
    "    if basename is None:\n",
    "        basename = os.path.basename(os.path.splitext(cut_filename)[0])\n",
    "\n",
    "    unique_cells = np.unique(cut)\n",
    "\n",
    "    if 0 not in unique_cells:\n",
    "        # if it happens that there is no zero cell, add it anyways\n",
    "        unique_cells = np.insert(unique_cells, 0, 0)  # object, index, value to insert\n",
    "\n",
    "    n_clusters = len(np.unique(cut))\n",
    "    n_spikes = len(cut)\n",
    "\n",
    "    write_list = []  # the list of values to write\n",
    "\n",
    "    tab = '    '  # the spaces didn't line up with my tab so I just created a string with enough spaces\n",
    "    empty_space = '               '  # some of the empty spaces don't line up to x tabs\n",
    "\n",
    "    # we add 1 to n_clusters because zero is the garbage cell that no one uses\n",
    "    write_list.append('n_clusters: %d\\n' % (n_clusters))\n",
    "    write_list.append('n_channels: 4\\n')\n",
    "    write_list.append('n_params: 2\\n')\n",
    "    write_list.append('times_used_in_Vt:%s' % ((tab + '0') * 4 + '\\n'))\n",
    "\n",
    "    zero_string = (tab + '0') * 8 + '\\n'\n",
    "\n",
    "    for cell_i in np.arange(n_clusters):\n",
    "        write_list.append(' cluster: %d center:%s' % (cell_i, zero_string))\n",
    "        write_list.append('%smin:%s' % (empty_space, zero_string))\n",
    "        write_list.append('%smax:%s' % (empty_space, zero_string))\n",
    "    write_list.append('\\nExact_cut_for: %s spikes: %d\\n' % (basename, n_spikes))\n",
    "\n",
    "    # now the cut file lists 25 values per row\n",
    "    n_rows = int(np.floor(n_spikes / 25))  # number of full rows\n",
    "\n",
    "    remaining = int(n_spikes - n_rows * 25)\n",
    "    cut_string = ('%3u' * 25 + '\\n') * n_rows + '%3u' * remaining\n",
    "\n",
    "    write_list.append(cut_string % (tuple(cut)))\n",
    "\n",
    "    with open(cut_filename, 'w') as f:\n",
    "        f.writelines(write_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07cc054b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From \n",
    "# https://github.com/GeoffBarrett/gebaSpike/blob/967097ec28592182ef9783d2d391930e1c63ca58/gebaSpike/core/writeCut.py\n",
    "\n",
    "def write_clu(clu_filename, data):\n",
    "    # the .clu files and the .cut files are different since the .clu files are the .cut files (with no manual sorting)\n",
    "    # without the headers, and the values go from 1 -> N instead of 0 -> N, (1-based numbering instead of 0-based). Thus\n",
    "    # we add 1 to the .cut data to get the .clu data\n",
    "\n",
    "    data = np.asarray(data).astype(int)  # ensuring that the data is the integer data-type\n",
    "\n",
    "    data += 1  # making the data 1-based instead of 0-based\n",
    "\n",
    "    # calculating the number of clusters\n",
    "    n_clust = len(np.unique(data))\n",
    "\n",
    "    # ensuring that the cluster number is the 1st value\n",
    "    data = np.concatenate(([n_clust], data))\n",
    "\n",
    "    # saving the data as a column (delimter='\\n') and integer format.\n",
    "    np.savetxt(clu_filename, data, fmt='%d', delimiter='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "230e729d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From \n",
    "# https://github.com/HussainiLab/BinConverter/blob/master/BinConverter/core/ConvertTetrode.py\n",
    "\n",
    "import os\n",
    "from BinConverter.core.conversion_utils import get_set_header\n",
    "import numpy as np\n",
    "import struct\n",
    "\n",
    "\n",
    "def write_tetrode(filepath, data, Fs):\n",
    "\n",
    "    session_path, session_filename = os.path.split(filepath)\n",
    "    tint_basename = os.path.splitext(session_filename)[0]\n",
    "    set_filename = os.path.join(session_path, '%s.set' % tint_basename)\n",
    "\n",
    "    n = len(data)\n",
    "\n",
    "    header = get_set_header(set_filename)\n",
    "\n",
    "    with open(filepath, 'w') as f:\n",
    "        num_chans = 'num_chans 4'\n",
    "        timebase_head = '\\ntimebase %d hz' % (96000)\n",
    "        bp_timestamp = '\\nbytes_per_timestamp %d' % (4)\n",
    "        # samps_per_spike = '\\nsamples_per_spike %d' % (int(Fs*1e-3))\n",
    "        samps_per_spike = '\\nsamples_per_spike %d' % (50)\n",
    "        sample_rate = '\\nsample_rate %d hz' % (Fs)\n",
    "        b_p_sample = '\\nbytes_per_sample %d' % (1)\n",
    "        # b_p_sample = '\\nbytes_per_sample %d' % (4)\n",
    "        spike_form = '\\nspike_format t,ch1,t,ch2,t,ch3,t,ch4'\n",
    "        num_spikes = '\\nnum_spikes %d' % (n)\n",
    "        start = '\\ndata_start'\n",
    "\n",
    "        write_order = [header, num_chans, timebase_head,\n",
    "                       bp_timestamp,\n",
    "                       samps_per_spike, sample_rate, b_p_sample, spike_form, num_spikes, start]\n",
    "\n",
    "        f.writelines(write_order)\n",
    "\n",
    "    # rearranging the data to have a flat array of t1, waveform1, t2, waveform2, t3, waveform3, etc....\n",
    "    spike_times = np.asarray(sorted(data.keys()))\n",
    "\n",
    "    # the spike times are repeated for each channel so lets tile this\n",
    "    spike_times = np.tile(spike_times, (4, 1))\n",
    "    spike_times = spike_times.flatten(order='F')\n",
    "\n",
    "    spike_values = np.asarray([value for (key, value) in sorted(data.items())])\n",
    "\n",
    "    # this will create a (n_samples, n_channels, n_samples_per_spike) => (n, 4, 50) sized matrix, we will create a\n",
    "    # matrix of all the samples and channels going from ch1 -> ch4 for each spike time\n",
    "    # time1 ch1_data\n",
    "    # time1 ch2_data\n",
    "    # time1 ch3_data\n",
    "    # time1 ch4_data\n",
    "    # time2 ch1_data\n",
    "    # time2 ch2_data\n",
    "    # .\n",
    "    # .\n",
    "    # .\n",
    "\n",
    "    spike_values = spike_values.reshape((n * 4, 50))  # create the 4nx50 channel data matrix\n",
    "\n",
    "    # make the first column the time values\n",
    "    spike_array = np.hstack((spike_times.reshape(len(spike_times), 1), spike_values))\n",
    "\n",
    "    data = None\n",
    "    spike_times = None\n",
    "    spike_values = None\n",
    "\n",
    "    spike_n = spike_array.shape[0]\n",
    "\n",
    "    t_packed = struct.pack('>%di' % spike_n, *spike_array[:, 0].astype(int))\n",
    "    spike_array = spike_array[:, 1:]  # removing time data from this matrix to save memory\n",
    "\n",
    "    spike_data_pack = struct.pack('<%db' % (spike_n*50), *spike_array.astype(int).flatten())\n",
    "\n",
    "    spike_array = None\n",
    "\n",
    "    # now we need to combine the lists by alternating\n",
    "\n",
    "    comb_list = [None] * (2*spike_n)\n",
    "    comb_list[::2] = [t_packed[i:i + 4] for i in range(0, len(t_packed), 4)]  # breaks up t_packed into a list,\n",
    "    # each timestamp is one 4 byte integer\n",
    "    comb_list[1::2] = [spike_data_pack[i:i + 50] for i in range(0, len(spike_data_pack), 50)]  # breaks up spike_data_\n",
    "    # pack and puts it into a list, each spike is 50 one byte integers\n",
    "\n",
    "    t_packed = None\n",
    "    spike_data_pack = None\n",
    "\n",
    "    write_order = []\n",
    "    with open(filepath, 'rb+') as f:\n",
    "\n",
    "        write_order.extend(comb_list)\n",
    "        write_order.append(bytes('\\r\\ndata_end\\r\\n', 'utf-8'))\n",
    "\n",
    "        f.seek(0, 2)\n",
    "        f.writelines(write_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ff0753",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc976e55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc8c7b45",
   "metadata": {},
   "source": [
    "<a id=\"Convert_recording_extractor_to_tint\"></a>\n",
    "## Convert Recording extractor to TINT\n",
    "[back to index](#index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852eb64d",
   "metadata": {},
   "source": [
    "Hmm, since we are writing to TINT, thereby creating `.X` tetrode files, we throw away all information in-between spikes. There is no point to convert the fake continuous recording used for spike sorting to TINT at all. We really only want to export the spike sorting output!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90a3c847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anything to do here?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d90289",
   "metadata": {},
   "source": [
    "<a id=\"Convert_sorting_extractor_to_tint\"></a>\n",
    "## Convert Sorting extractor to TINT\n",
    "[back to index](#index)\n",
    "\n",
    "There are several points in the pipeline at which we might want to export to TINT. Ideally it should work for any `SortingExtractor` object!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fe21ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where do we load data from?\n",
      " /mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/sample_bin_to_tint_no_bin\n"
     ]
    }
   ],
   "source": [
    "print('Where do we load data from?\\n', dir_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae069b09",
   "metadata": {},
   "source": [
    "From a sorting extractor we can obtain a list unit spike sample arrays. We can convert this to the .clu or .cut type array of unit ID labels for each spike.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0fc1e882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20201004_Tint_1\n"
     ]
    }
   ],
   "source": [
    "cut_filename = Path('/mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/Axona_Tint_1ms/20201004_Tint_1.cut')\n",
    "\n",
    "basename = os.path.basename(os.path.splitext(cut_filename)[0])\n",
    "\n",
    "print(basename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8bae0e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/Axona_Tint_1ms/20201004_Tint.set\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('/mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/Axona_Tint_1ms/20201004_Tint_1.cut')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = Path('/mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/Axona_Tint_1ms/20201004_Tint.set')\n",
    "print(filename)\n",
    "\n",
    "Path(str(filename.with_suffix('')) + '_{}'.format(1) + '.cut')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8855a9c0",
   "metadata": {},
   "source": [
    "### Write unit labels to .cut and .clu files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2270a472",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_spike_train_to_label_array(spike_train):\n",
    "    '''Takes a list of arrays, where each array is a series of\n",
    "    sample points at which a spike occured for a given unit\n",
    "    (each list item is a unit). Converts to .cut array, i.e.\n",
    "    orders spike samples from all units and labels each sample\n",
    "    with the appropriate unit ID.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    spike_train : List of np.arrays\n",
    "        Output of `get_units_spike_train()` method of sorting extractor\n",
    "        \n",
    "    Return\n",
    "    ------\n",
    "    unit_labels_sorted : np.array\n",
    "        Each entry is the unit ID corresponding to the spike sample that\n",
    "        occured at this ordinal position\n",
    "    '''\n",
    "\n",
    "    # Generate Index array (indexing the unit for a given spike sample)\n",
    "    unit_labels = []\n",
    "    for i, l in enumerate(spike_train):\n",
    "        unit_labels.append(np.ones((len(l),), dtype=int) * i)\n",
    "    \n",
    "    # Flatten lists and sort them\n",
    "    spike_train_flat = np.concatenate(spike_train).ravel()\n",
    "    unit_labels_flat = np.concatenate(unit_labels).ravel()\n",
    "\n",
    "    sort_index = np.argsort(spike_train_flat)\n",
    "\n",
    "    unit_labels_sorted = unit_labels_flat[sort_index]\n",
    "\n",
    "    return unit_labels_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8215d02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_cut_file(cut_filename, unit_labels):\n",
    "    '''Write spike sorting output to .cut file.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    cut_filename : str or Path\n",
    "        Full filename of .cut file to write to. A given .cut file belongs\n",
    "        to a given tetrode file. For example, for tetrode `my_file.1`, the\n",
    "        corresponding cut_filename should be `my_file_1.cut`.\n",
    "    unit_labels : np.array\n",
    "        Vector of unit labels for each spike sample (ordered by time of \n",
    "        occurence)\n",
    "        \n",
    "    Example\n",
    "    -------\n",
    "    # Given a sortingextractor called sorting_nwb:\n",
    "    spike_train = sorting_nwb.get_units_spike_train()\n",
    "    unit_labels = convert_spike_train_to_label_array(spike_train)\n",
    "    write_to_cut_file(cut_filename, unit_labels)\n",
    "    \n",
    "    ---\n",
    "    Largely based on gebaSpike implementation by Geoff Barrett\n",
    "    https://github.com/GeoffBarrett/gebaSpike\n",
    "    '''\n",
    "\n",
    "    unique_cells = np.unique(unit_labels)\n",
    "\n",
    "    n_clusters = len(np.unique(unit_labels))\n",
    "    n_spikes = len(unit_labels)\n",
    "\n",
    "    write_list = []\n",
    "\n",
    "    tab = '    '\n",
    "    empty_space = '               '\n",
    "\n",
    "    write_list.append('n_clusters: %d\\n' % (n_clusters))\n",
    "    write_list.append('n_channels: 4\\n')\n",
    "    write_list.append('n_params: 2\\n')\n",
    "    write_list.append('times_used_in_Vt:%s' % ((tab + '0') * 4 + '\\n'))\n",
    "\n",
    "    zero_string = (tab + '0') * 8 + '\\n'\n",
    "\n",
    "    for cell_i in np.arange(n_clusters):\n",
    "        write_list.append(' cluster: %d center:%s' % (cell_i, zero_string))\n",
    "        write_list.append('%smin:%s' % (empty_space, zero_string))\n",
    "        write_list.append('%smax:%s' % (empty_space, zero_string))\n",
    "    write_list.append('\\nExact_cut_for: %s spikes: %d\\n' % (basename, n_spikes))\n",
    "\n",
    "    # The unit label array consists of 25 values per row in .cut file\n",
    "    n_rows = int(np.floor(n_spikes / 25))\n",
    "    remaining = int(n_spikes - n_rows * 25)\n",
    "\n",
    "    cut_string = ('%3u' * 25 + '\\n') * n_rows + '%3u' * remaining\n",
    "\n",
    "    write_list.append(cut_string % (tuple(unit_labels)))\n",
    "\n",
    "    with open(cut_filename, 'w') as f:\n",
    "        f.writelines(write_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2487be27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_clu_file(clu_filename, unit_labels):\n",
    "    ''' .clu files are pruned .cut files, containing only a long vector of unit\n",
    "    labels, which are 1-indexed, instead of 0-indexed. In addition, the very first\n",
    "    entry is the total number of units.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    clu_filename : str or Path\n",
    "        Full filename of .clu file to write to. A given .clu file belongs\n",
    "        to a given tetrode file. For example, for tetrode `my_file.1`, the\n",
    "        corresponding clu_filename should be `my_file_1.clu`.\n",
    "    unit_labels : np.array\n",
    "        Vector of unit labels for each spike sample (ordered by time of \n",
    "        occurence)\n",
    "        \n",
    "    ---\n",
    "    Largely based on gebaSpike implementation by Geoff Barrett\n",
    "    https://github.com/GeoffBarrett/gebaSpike\n",
    "    '''\n",
    "    unit_labels = np.asarray(unit_labels).astype(int)\n",
    "    unit_labels += 1\n",
    "\n",
    "    n_clust = len(np.unique(unit_labels))\n",
    "    unit_labels = np.concatenate(([n_clust], unit_labels))\n",
    "\n",
    "    np.savetxt(clu_filename, unit_labels, fmt='%d', delimiter='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "995d9aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_cut_filename_from_basename(filename, tetrode_id):\n",
    "    '''Given a str or Path object, assume the last entry after a slash\n",
    "    is a filename, strip any file suffix, add tetrode ID label, and\n",
    "    .cut suffix to name.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : str or Path\n",
    "    tetrode_id : int\n",
    "    '''\n",
    "    return Path(str(filename).split('.')[0] + '_{}'.format(tetrode_id) + '.cut')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fadbd312",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_unit_labels_to_file(sorting_extractor, filename):\n",
    "    '''Write spike sorting output to .cut file, separately for each\n",
    "    tetrode.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sorting_extractor : spikeextractors.SortingExtractor\n",
    "    filename : str or Path\n",
    "        Full filename of .set file or base-filename (i.e. the part of the\n",
    "        filename all Axona files have in common). A given .cut file belongs\n",
    "        to a given tetrode file. For example, for tetrode `my_file.1`, the\n",
    "        corresponding cut_filename should be `my_file_1.cut`. This will be\n",
    "        set automatically given the base-filename or set file.\n",
    "        \n",
    "    TODO: Any reason one might want to only convert some tetrodes or some\n",
    "    samples? Should those be parameters?\n",
    "    '''\n",
    "    tetrode_ids = sorting_extractor.get_units_property(property_name='group')\n",
    "    tetrode_ids = np.array(tetrode_ids)\n",
    "    \n",
    "    unit_ids = np.array(sorting_extractor.get_unit_ids())\n",
    "    \n",
    "    for i in np.unique(tetrode_ids):\n",
    "        \n",
    "        print('Converting Tetrode {}'.format(i))\n",
    "\n",
    "        spike_train = sorting_extractor.get_units_spike_train(unit_ids=unit_ids[tetrode_ids==i])\n",
    "        unit_labels = convert_spike_train_to_label_array(spike_train)\n",
    "\n",
    "        # We use Axona conventions for filenames (tetrodes are 1 indexed)\n",
    "        cut_filename = set_cut_filename_from_basename(filename, i + 1)\n",
    "        clu_filename = Path(str(cut_filename).replace('.cut', '.clu'))\n",
    "\n",
    "        write_to_cut_file(cut_filename, unit_labels)\n",
    "        write_to_clu_file(clu_filename, unit_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "001d00c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spikeextractors.extractors.nwbextractors.nwbextractors.NwbSortingExtractor'>\n"
     ]
    }
   ],
   "source": [
    "# We have sorting data exported in `.nwb` format\n",
    "\n",
    "nwb_dir = Path(dir_name, 'nwb')\n",
    "sorting_nwb = se.NwbSortingExtractor(nwb_dir / 'axona_se_MS4.nwb', sampling_frequency=48000)\n",
    "\n",
    "print(type(sorting_nwb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "45c9d08a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling frequency: 48000 Hz\n"
     ]
    }
   ],
   "source": [
    "print('Sampling frequency:', sorting_nwb.get_sampling_frequency(), 'Hz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0ddef20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting Tetrode 0\n",
      "Converting Tetrode 1\n",
      "Converting Tetrode 2\n",
      "Converting Tetrode 3\n"
     ]
    }
   ],
   "source": [
    "# Convert all tetrodes from sorting extractor to cut files\n",
    "write_unit_labels_to_file(sorting_nwb, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f79ce30",
   "metadata": {},
   "source": [
    "### Write waveforms to tetrode files (.X)\n",
    "\n",
    "Here, we need information that is available in the `.set` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ab1f7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_generic_header(filename):\n",
    "    \"\"\"\n",
    "    Given a binary file with phrases and line breaks, enters the\n",
    "    first word of a phrase as dictionary key and the following\n",
    "    string (without linebreaks) as value. Returns the dictionary.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : str or Path\n",
    "        Full filename.\n",
    "    \"\"\"\n",
    "    header = {}\n",
    "    with open(filename, 'rb') as f:\n",
    "        for bin_line in f:\n",
    "            if b'data_start' in bin_line:\n",
    "                break\n",
    "            line = bin_line.decode('cp1252').replace('\\r\\n', '').replace('\\r', '').strip()\n",
    "            parts = line.split(' ')\n",
    "            key = parts[0]\n",
    "            value = ' '.join(parts[1:])\n",
    "            header[key] = value\n",
    "            \n",
    "    return header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4c56562a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unit_group_ids(sorting):\n",
    "    '''Get group ids.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sorting : SortingExtractor\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    group_ids : List\n",
    "        List of groups ids for each Unit in `sorting`.\n",
    "    '''\n",
    "    unit_ids = sorting.get_unit_ids()\n",
    "    group_ids = [sorting.get_unit_property(\n",
    "        unit_id=unit_id, property_name='group') for unit_id in unit_ids\n",
    "    ]\n",
    "\n",
    "    return group_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4493ba1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_units_on_tetrode(group_spike_samples, group_waveforms):\n",
    "    '''Write all waveforms of given tetrode in dictionary with the\n",
    "    corresponding spike samples being the keys (1 sample for each\n",
    "    waveform).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    group_spike_samples : list\n",
    "        As returned by sortingextractor.get_units_spike_train()\n",
    "    group_waveforms : list\n",
    "        As returned by spiketoolkit.postprocessing.get_unit_waveforms()\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tetrode_spikes : dict\n",
    "        Keys are spike samples, values are waveforms (ntrls x nch x nsamp)\n",
    "    '''\n",
    "    tetrode_spikes = {}\n",
    "\n",
    "    for i, (samples, waveforms) in enumerate(zip(group_spike_samples, group_waveforms)):\n",
    "\n",
    "        for sample, waveform in zip(samples, waveforms):\n",
    "\n",
    "            tetrode_spikes[sample] = waveform\n",
    "            \n",
    "    return tetrode_spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "68b3a37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "st.postprocessing.get_unit_waveforms??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "70dbf774",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_waveforms(recording, sorting, unit_ids, header):\n",
    "    '''Get waveforms for specific tetrode.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    recording : RecordingExtractor\n",
    "    sorting : SortingExtractor\n",
    "    unit_ids : List\n",
    "        List of unit ids to extract waveforms\n",
    "    header : dict\n",
    "        maps parameters from .set file to their values (as strings).\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    waveforms : List\n",
    "        List of np.array (n_spikes, n_channels, n_timepoints) with waveforms for each unit\n",
    "    '''\n",
    "    sampling_rate = recording.get_sampling_frequency()\n",
    "    samples_before = int(header['pretrigSamps'])\n",
    "    samples_after = int(header['spikeLockout'])\n",
    "\n",
    "    ms_before = samples_before / (sampling_rate / 1000) + 0.001\n",
    "    ms_after = samples_after / (sampling_rate / 1000) + 0.001\n",
    "\n",
    "    waveforms = st.postprocessing.get_unit_waveforms(\n",
    "        recording,\n",
    "        sorting,\n",
    "        unit_ids=unit_ids,\n",
    "        max_spikes_per_unit=None, \n",
    "        grouping_property='group',\n",
    "        recompute_info=True,\n",
    "        ms_before=ms_before,\n",
    "        ms_after=ms_after,\n",
    "        return_idxs=False,\n",
    "        return_scaled=False,\n",
    "        dtype=np.int8\n",
    "    )\n",
    "\n",
    "    return waveforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f268615f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_tetrode_files(recording, sorting, group_ids, set_file):\n",
    "    '''Get spike samples and waveforms for all tetrodes specified in\n",
    "    `group_ids`. Note that `group_ids` is 0-indexed, whereas tetrodes are\n",
    "    1-indexed (so if you want tetrodes 1+2, specify group_ids=[0, 1]).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    recording : RecordingExtractor\n",
    "    sorting : SortingExtractor\n",
    "    group_ids : array like\n",
    "        Tetrodes to include, but 0-indexed (i.e. tetrodeID - 1)\n",
    "    set_file : Path or str\n",
    "        .set file location. Used to determine how many samples prior to and\n",
    "        post spike sample should be cut out for each waveform. .X files will have\n",
    "        the same base filename as the .set file. So if you do not want to overwrite\n",
    "        existing .X files in your .set file directory, copy the .set file to a new\n",
    "        folder and give its new location. The new .X files will appear there.\n",
    "    '''\n",
    "    sampling_rate = recording.get_sampling_frequency()\n",
    "    group_ids = get_unit_group_ids(sorting)\n",
    "    header = parse_generic_header(set_file)\n",
    "\n",
    "    for group_id in np.unique(group_ids):\n",
    "\n",
    "        # get spike samples and waveforms of this group / tetrode\n",
    "        group_unit_ids = [i for i, gid in enumerate(group_ids) if gid==group_id]\n",
    "        group_waveforms = get_waveforms(recording, sorting, group_unit_ids, header)\n",
    "        group_spike_samples = sorting.get_units_spike_train(unit_ids=group_unit_ids)\n",
    "\n",
    "        # Assign each waveform to it's spike sample in a dictionary\n",
    "        spike_waveform_dict = combine_units_on_tetrode(group_spike_samples, group_waveforms)\n",
    "\n",
    "        # Set tetrode filename\n",
    "        tetrode_filename = str(set_file).split('.')[0] + '.{}'.format(group_id + 1)\n",
    "        print(tetrode_filename)\n",
    "\n",
    "        # Use `BinConverter` function to write to tetrode file\n",
    "        write_tetrode(tetrode_filename, spike_waveform_dict, sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4c8782d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/sample_bin_to_tint_no_bin/axona_sample.set\n"
     ]
    }
   ],
   "source": [
    "set_file = dir_name / 'axona_sample.set'\n",
    "print(set_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "26f6b729",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spikeextractors.extractors.axonaunitrecordingextractor import AxonaUnitRecordingExtractor\n",
    "\n",
    "recording = AxonaUnitRecordingExtractor(filename=set_file)\n",
    "signal = recording.get_traces(channel_ids=None, start_frame=None, end_frame=None, return_scaled=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8d6ff0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/sample_bin_to_tint_no_bin/conversion_to_tint/axona_sample.set\n"
     ]
    }
   ],
   "source": [
    "set_file_to_tint = dir_name / 'conversion_to_tint' / 'axona_sample.set'\n",
    "print(set_file_to_tint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a713f68e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>channel_ids</th>\n",
       "      <th>channel_groups</th>\n",
       "      <th>tetrode_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    channel_ids  channel_groups  tetrode_ids\n",
       "0             0               0            1\n",
       "1             1               0            1\n",
       "2             2               0            1\n",
       "3             3               0            1\n",
       "4             4               1            2\n",
       "5             5               1            2\n",
       "6             6               1            2\n",
       "7             7               1            2\n",
       "8             8               2            3\n",
       "9             9               2            3\n",
       "10           10               2            3\n",
       "11           11               2            3\n",
       "12           12               3            4\n",
       "13           13               3            4\n",
       "14           14               3            4\n",
       "15           15               3            4"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'channel_ids': recording.get_channel_ids(),\n",
    "    'channel_groups': recording.get_channel_groups(),\n",
    "    'tetrode_ids': recording.get_channel_groups() + 1\n",
    "})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6ef24b4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channel_groups = recording.get_channel_groups()\n",
    "channel_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "76bc34a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/sample_bin_to_tint_no_bin/conversion_to_tint/axona_sample.1\n",
      "/mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/sample_bin_to_tint_no_bin/conversion_to_tint/axona_sample.2\n",
      "/mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/sample_bin_to_tint_no_bin/conversion_to_tint/axona_sample.3\n",
      "/mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/sample_bin_to_tint_no_bin/conversion_to_tint/axona_sample.4\n"
     ]
    }
   ],
   "source": [
    "write_to_tetrode_files(recording, sorting_nwb, channel_groups, set_file_to_tint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6599de38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ecbe4011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72,) (72, 4, 50)\n",
      "(101,) (101, 4, 50)\n",
      "(59,) (59, 4, 50)\n",
      "(15,) (15, 4, 50)\n",
      "(39,) (39, 4, 50)\n",
      "(51,) (51, 4, 50)\n",
      "(38,) (38, 4, 50)\n",
      "(50,) (50, 4, 50)\n",
      "(47,) (47, 4, 50)\n",
      "(73,) (73, 4, 50)\n",
      "(47,) (47, 4, 50)\n",
      "(42,) (42, 4, 50)\n",
      "(67,) (67, 4, 50)\n",
      "(52,) (52, 4, 50)\n"
     ]
    }
   ],
   "source": [
    "# On the .X file there will be no more unit information, we will write all units from a given\n",
    "# tetrode for each tetrode channel\n",
    "\n",
    "header = parse_generic_header(set_file_to_tint)\n",
    "group_unit_ids = None\n",
    "\n",
    "waveforms = get_waveforms(recording, sorting_nwb, group_unit_ids, header)\n",
    "spike_samples = sorting_nwb.get_units_spike_train()\n",
    "for spk_trn, wv in zip(spike_samples, waveforms):\n",
    "    print(spk_trn.shape, wv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4b013d83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "memmap([[-2, -2, -1, -4,  0,  3, -5,  6, -3,  0,  4,  0, -1,  2, -1, -2,\n",
       "         -3,  2,  3,  6,  1,  0, -1,  0,  0,  2, -3, -1, -1,  0,  0, -8,\n",
       "          1, -1,  1, -1,  0,  1,  0, -7,  1,  1, -1,  0,  1, -6,  1,  0,\n",
       "          0,  0],\n",
       "        [ 3, -1, -2, -3,  1,  0,  0,  0,  0,  3,  1,  5,  1, -4,  3,  2,\n",
       "         -4,  1,  0, -2, -3,  5,  0, -2, -5,  2, 13, -2,  3,  0,  0, -1,\n",
       "          3,  1, -1,  5, -1, -2,  1, -2, -8,  2,  0, -1,  0,  8,  2,  4,\n",
       "         -4, -4],\n",
       "        [ 3,  1,  1, -6, -4, -5, -1,  4,  0,  2, -1,  0, -3,  1,  2, -2,\n",
       "         -8,  2, -1,  5, -2,  0, -8,  0, -4, -2,  3, -1, -3,  3,  1, -1,\n",
       "         -1, -1,  0,  2,  0,  0,  0,  0, -6,  4,  0,  0,  1,  2,  7, -2,\n",
       "         -1,  1],\n",
       "        [ 1,  6,  1,  0,  1,  2,  1, -4,  2, -8,  6,  3,  3, -5,  1, -2,\n",
       "          2, -3,  3,  1,  0,  2,  3,  0, -4, -4, -2, -5, -2, -1,  0,  0,\n",
       "          6,  2,  0, -1, -1,  0, -1,  0, -1, -3,  1,  9,  1, -2,  1,  0,\n",
       "         -1,  0]], dtype=int8)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "waveforms[0][0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "95c02842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get spike samples and waveforms of this group / tetrode\n",
    "group_unit_ids = [i for i, gid in enumerate(group_ids) if gid==group_id]\n",
    "group_waveforms = get_waveforms(recording, sorting, group_unit_ids, header)\n",
    "group_spike_samples = sorting.get_units_spike_train(unit_ids=group_unit_ids)\n",
    "\n",
    "# Assign each waveform to it's spike sample in a dictionary\n",
    "spike_waveform_dict = combine_units_on_tetrode(group_spike_samples, group_waveforms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e27f3cf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_id = 0\n",
    "group_unit_ids = [i for i, gid in enumerate(group_ids) if gid==group_id]\n",
    "group_unit_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ccf36e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/sample_bin_to_tint_no_bin/conversion_to_tint/axona_sample.1\n",
      "(173, 4, 50)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "memmap([[-2, -2, -1, -4,  0,  3, -5,  6, -3,  0,  4,  0, -1,  2, -1, -2,\n",
       "         -3,  2,  3,  6,  1,  0, -1,  0,  0,  2, -3, -1, -1,  0,  0, -8,\n",
       "          1, -1,  1, -1,  0,  1,  0, -7,  1,  1, -1,  0,  1, -6,  1,  0,\n",
       "          0,  0],\n",
       "        [ 3, -1, -2, -3,  1,  0,  0,  0,  0,  3,  1,  5,  1, -4,  3,  2,\n",
       "         -4,  1,  0, -2, -3,  5,  0, -2, -5,  2, 13, -2,  3,  0,  0, -1,\n",
       "          3,  1, -1,  5, -1, -2,  1, -2, -8,  2,  0, -1,  0,  8,  2,  4,\n",
       "         -4, -4],\n",
       "        [ 3,  1,  1, -6, -4, -5, -1,  4,  0,  2, -1,  0, -3,  1,  2, -2,\n",
       "         -8,  2, -1,  5, -2,  0, -8,  0, -4, -2,  3, -1, -3,  3,  1, -1,\n",
       "         -1, -1,  0,  2,  0,  0,  0,  0, -6,  4,  0,  0,  1,  2,  7, -2,\n",
       "         -1,  1],\n",
       "        [ 1,  6,  1,  0,  1,  2,  1, -4,  2, -8,  6,  3,  3, -5,  1, -2,\n",
       "          2, -3,  3,  1,  0,  2,  3,  0, -4, -4, -2, -5, -2, -1,  0,  0,\n",
       "          6,  2,  0, -1, -1,  0, -1,  0, -1, -3,  1,  9,  1, -2,  1,  0,\n",
       "         -1,  0]], dtype=int8)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read data back in to see if it worked as expected\n",
    "\n",
    "tetrode_filename = \\\n",
    "    '/mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/sample_bin_to_tint_no_bin/conversion_to_tint/axona_sample.1'\n",
    "print(tetrode_filename)\n",
    "\n",
    "from neo import AxonaIO\n",
    "\n",
    "neoio = AxonaIO(tetrode_filename)\n",
    "\n",
    "waveforms = neoio.get_spike_raw_waveforms()\n",
    "print(waveforms.shape)\n",
    "waveforms[1, :, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd251800",
   "metadata": {},
   "source": [
    "## Convert recording extractor to tetrode files (.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "10194d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from BinConverter.core.ConvertTetrode import write_tetrode\n",
    "from BinConverter.core.readBin import (\n",
    "    get_bin_data, get_raw_pos, get_channel_from_tetrode, get_active_tetrode, get_active_eeg\n",
    ")\n",
    "from BinConverter.core.Tint_Matlab import int16toint8\n",
    "\n",
    "from spikeextractors.extractors.axonaunitrecordingextractor import AxonaUnitRecordingExtractor\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f5882ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_generic_header(filename):\n",
    "    \"\"\"\n",
    "    Given a binary file with phrases and line breaks, enters the\n",
    "    first word of a phrase as dictionary key and the following\n",
    "    string (without linebreaks) as value. Returns the dictionary.\n",
    "    \"\"\"\n",
    "    header = {}\n",
    "    with open(filename, 'rb') as f:\n",
    "        for bin_line in f:\n",
    "            if b'data_start' in bin_line:\n",
    "                break\n",
    "            line = bin_line.decode('cp1252').replace('\\r\\n', '').replace('\\r', '').strip()\n",
    "            parts = line.split(' ')\n",
    "            key = parts[0]\n",
    "            value = ' '.join(parts[1:])\n",
    "            header[key] = value\n",
    "            \n",
    "    return header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0ff4d999",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_channel_from_tetrode(tetrode):\n",
    "    \"\"\"\n",
    "    This function will take the tetrode number and return the Axona\n",
    "    channel numbers, i.e. Tetrode 1 = Ch0-Ch3, Tetrode 2 = Ch4-Ch7, etc.\n",
    "    \"\"\"\n",
    "    return np.arange(0, 4) + 4 * (int(tetrode) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "df479c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/sample_bin_to_tint_no_bin/axona_sample.set\n"
     ]
    }
   ],
   "source": [
    "set_file = dir_name / 'axona_sample.set'\n",
    "set_filename = set_file\n",
    "print(set_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d87e2fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "re = AxonaUnitRecordingExtractor(filename=set_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f26aa3a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AxonaRawIO: /mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/sample_bin_to_tint_no_bin/axona_sample\n",
       "nb_block: 1\n",
       "nb_segment:  [1]\n",
       "signal_streams: [stream 0 (chans: 16)]\n",
       "signal_channels: [1a, 1b, 1c, 1d ... 4a , 4b , 4c , 4d]\n",
       "spike_channels: [tetrode 1, tetrode 2, tetrode 3, tetrode 4]\n",
       "event_channels: []"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.neo_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "27349ae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/sample_bin_to_tint_no_bin')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "00de1c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "header = parse_generic_header(set_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bb4d4e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "40\n",
      "30\n",
      "43\n",
      "100\n",
      "-100\n"
     ]
    }
   ],
   "source": [
    "pre_spike_samples = int(header['pretrigSamps'])\n",
    "post_spike_samples = int(header['spikeLockout'])\n",
    "rejstart = int(header['rejstart'])\n",
    "rejthreshtail = int(header['rejthreshtail'])\n",
    "rejthreshupper = int(header['rejthreshupper'])\n",
    "rejthreshlower = int(header['rejthreshlower'])\n",
    "\n",
    "print(pre_spike_samples)\n",
    "print(post_spike_samples)\n",
    "print(rejstart)\n",
    "print(rejthreshtail)\n",
    "print(rejthreshupper)\n",
    "print(rejthreshlower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6f4d7186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spikeextractors.extractors.bindatrecordingextractor.bindatrecordingextractor.BinDatRecordingExtractor at 0x7fb2ff61db20>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2b5068c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'axona_sample.1'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tetrode = 1\n",
    "set_file.stem + '.{}'.format(tetrode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b6c860aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tetrode_channels = get_channel_from_tetrode(tetrode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "06809590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `get_bin_data` not found.\n"
     ]
    }
   ],
   "source": [
    "get_bin_data??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6dff6db5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tetrode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c5082a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "import mmap\n",
    "\n",
    "def get_bin_data(bin_filename, channels=None, tetrode=None):\n",
    "    \"\"\"This function will be used to acquire the actual lfp data given the .bin filename,\n",
    "    and the tetrode or channels (from 1-64) that you want to get\"\"\"\n",
    "\n",
    "    if tetrode is not None:\n",
    "        channels = get_channel_from_tetrode(tetrode)\n",
    "    else:\n",
    "        channels = np.array(channels)  # just in case it isn't an np.array\n",
    "\n",
    "    bytes_per_iteration = 432\n",
    "\n",
    "    with open(bin_filename, 'rb') as f:\n",
    "        # pass\n",
    "        with contextlib.closing(mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ)) as m:\n",
    "            num_iterations = int(len(m)/bytes_per_iteration)\n",
    "\n",
    "            data = np.ndarray((num_iterations,), (np.int16, (1,192)), m, 32, (bytes_per_iteration,)).reshape((-1, 1)).flatten()\n",
    "            data = samples_to_array(data, channels=channels.tolist())\n",
    "\n",
    "    return data\n",
    "\n",
    "def samples_to_array(A, channels=[]):\n",
    "    \"\"\"This will take data matrix A, and convert it into a numpy array, there are three samples of\n",
    "    64 channels in this matrix, however their channels do need to be re-mapped\"\"\"\n",
    "\n",
    "    if channels == []:\n",
    "        channels = np.arange(64) + 1\n",
    "    else:\n",
    "        channels = np.asarray(channels)\n",
    "\n",
    "    A = np.asarray(A)\n",
    "\n",
    "    sample_num = int(len(A) / 64)  # get the sample numbers\n",
    "\n",
    "    sample_array = np.zeros((len(channels), sample_num))  # creating a 64x3 array of zeros (64 channels, 3 samples)\n",
    "\n",
    "    for i, channel in enumerate(channels):\n",
    "        sample_array[i, :] = A[get_sample_indices(channel, sample_num)]\n",
    "\n",
    "    return sample_array\n",
    "\n",
    "def get_sample_indices(channel_number, samples):\n",
    "    remap_channel = get_remap_chan(channel_number)\n",
    "\n",
    "    indices_scalar = np.multiply(np.arange(samples), 64)\n",
    "    sample_indices = indices_scalar + np.multiply(np.ones(samples), remap_channel)\n",
    "\n",
    "    # return np.array([remap_channel, 64 + remap_channel, 64*2 + remap_channel])\n",
    "    return (indices_scalar + np.multiply(np.ones(samples), remap_channel)).astype(int)\n",
    "\n",
    "def get_remap_chan(chan_num):\n",
    "    \"\"\"There is re-mapping, thus to get the correct channel data, you need to incorporate re-mapping\n",
    "    input will be a channel from 1 to 64, and will return the remapped channel\"\"\"\n",
    "\n",
    "    remap_channels = np.array([32, 33, 34, 35, 36, 37, 38, 39, 0, 1, 2, 3, 4, 5,\n",
    "                               6, 7, 40, 41, 42, 43, 44, 45, 46, 47, 8, 9, 10, 11,\n",
    "                               12, 13, 14, 15, 48, 49, 50, 51, 52, 53, 54, 55, 16, 17,\n",
    "                               18, 19, 20, 21, 22, 23, 56, 57, 58, 59, 60, 61, 62, 63,\n",
    "                               24, 25, 26, 27, 28, 29, 30, 31])\n",
    "\n",
    "    return remap_channels[chan_num - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b7121c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_filename = Path('/mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/sample_bin_to_tint/axona_sample.bin')\n",
    "data = get_bin_data(bin_filename, tetrode=tetrode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f4c610fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 57600)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ddd6dab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  3574.,   -230.,      0., ...,      0.,      0.,      0.],\n",
       "       [  3682.,  -3870.,      0., ..., -13700., -14410., -11876.],\n",
       "       [  1714.,   -188.,      0., ...,  -9006.,  -9980.,  -9502.],\n",
       "       [ 10480.,   5308.,      0., ...,  -8460.,  -7266.,  -4906.]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9b283da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo import AxonaIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8b54a621",
   "metadata": {},
   "outputs": [],
   "source": [
    "neoio = AxonaIO(bin_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "26f25a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_data_neo = neoio.get_analogsignal_chunk(channel_indexes=[12, 13, 14, 15]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "96f7082e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -7572,  -1930,      0, ..., -26920, -22956, -17398],\n",
       "       [ -5500,   -206,      0, ..., -18238, -17952, -15410],\n",
       "       [   798,  -9238,      0, ..., -17090, -13344,  -8568],\n",
       "       [  2378,  -6200,      0, ..., -17912, -15948, -11356]], dtype=int16)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bin_data_neo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7a47ead3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tet1_data_neo = neoio.get_spike_raw_waveforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "72f01e49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "memmap([[[  -7,    2,   18, ...,   33,   26,   15],\n",
       "         [  -6,   -2,    1, ...,   25,   25,   22],\n",
       "         [  -2,    3,   14, ...,   30,   24,   14],\n",
       "         [ -18,   -6,   13, ...,   22,   10,   -3]],\n",
       "\n",
       "        [[   4,   -3,  -11, ...,  -11,  -10,  -12],\n",
       "         [  28,   22,    8, ...,   -7,  -10,  -12],\n",
       "         [  22,   15,    1, ...,  -14,  -15,  -14],\n",
       "         [  40,   56,   57, ...,   -9,  -15,  -24]],\n",
       "\n",
       "        [[  15,    6,   -1, ...,  -40,  -25,   -8],\n",
       "         [  17,   14,    6, ...,  -29,  -29,  -25],\n",
       "         [  20,   15,    7, ...,  -34,  -24,  -11],\n",
       "         [   1,  -13,  -27, ...,  -53,  -31,   -5]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ -40,  -71, -108, ...,  -36,  -57,  -71],\n",
       "         [ -35,  -39,  -43, ...,   14,    0,  -20],\n",
       "         [ -15,  -17,  -22, ...,    5,  -15,  -46],\n",
       "         [ -20,  -20,  -26, ...,   12,   -6,  -32]],\n",
       "\n",
       "        [[   7,    5,    7, ...,    9,    6,    1],\n",
       "         [  -2,   -7,  -12, ...,  -15,  -13,  -16],\n",
       "         [  42,   44,   33, ...,   -1,   -5,   -9],\n",
       "         [  38,   44,   46, ...,   -8,  -10,  -10]],\n",
       "\n",
       "        [[ -28,  -29,  -19, ...,   17,   21,   26],\n",
       "         [  20,    8,    4, ...,    0,  -11,  -16],\n",
       "         [ -40,  -48,  -37, ...,   -5,   -5,    1],\n",
       "         [ -21,   -8,   12, ...,   22,   31,   36]]], dtype=int8)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tet1_data_neo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5e0c8e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "fraw = 3682\n",
    "f = 4314.84375  # should be 3682"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "047981dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0b111001100010'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bin(fraw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8550067a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-57d3acba662e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "bin(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9666751c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'traces' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-aa2b57c00f22>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraces\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'traces' is not defined"
     ]
    }
   ],
   "source": [
    "traces * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d1c49bfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.float64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data[1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "49660066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>channel_ids</th>\n",
       "      <th>channel_groups</th>\n",
       "      <th>tetrode_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    channel_ids  channel_groups  tetrode_ids\n",
       "0             1               0            1\n",
       "1             2               0            1\n",
       "2             4               1            2\n",
       "3             6               1            2\n",
       "4             7               1            2\n",
       "5             8               2            3\n",
       "6             9               2            3\n",
       "7            10               2            3\n",
       "8            11               2            3\n",
       "9            12               3            4\n",
       "10           13               3            4\n",
       "11           14               3            4\n",
       "12           15               3            4"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    'channel_ids': r_cache.get_channel_ids(),\n",
    "    'channel_groups': r_cache.get_channel_groups(),\n",
    "    'tetrode_ids': r_cache.get_channel_groups() + 1\n",
    "})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "80d638ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tetrode_channels = df.loc[df['tetrode_ids'] == tetrode, 'channel_ids'].values + 1\n",
    "tetrode_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b524bcf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tetrode_channels-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3eb2357d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 57600)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A tetrode file expects 4 channels. Fill missing channels with zeros.\n",
    "\n",
    "traces = np.zeros((4, r_cache.get_num_frames()))\n",
    "traces[tetrode_channels-1, :] = r_cache.get_traces(channel_ids=tetrode_channels-1)\n",
    "traces.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a7decf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_cache.get_traces?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9638e015",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tetrode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3c94b3bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tetrode_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6fa77dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tetrode = int(tetrode)\n",
    "\n",
    "tetrode_filename = save_dir / Path(set_file.stem + '.{}'.format(tetrode))\n",
    "\n",
    "tetrode_channels = df.loc[df['tetrode_ids'] == tetrode, 'channel_ids'].values + 1\n",
    "\n",
    "traces = np.zeros((4, r_cache.get_num_frames()))\n",
    "traces[(tetrode_channels-1) % 4, :] = r_cache.get_traces(channel_ids=tetrode_channels-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5295607d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Fs = r_cache.get_sampling_frequency()\n",
    "active_tetrodes = np.unique(r_cache.get_channel_groups()) + 1\n",
    "\n",
    "pre_spike_samples = int(header['pretrigSamps'])\n",
    "post_spike_samples = int(header['spikeLockout'])\n",
    "rejstart = int(header['rejstart'])\n",
    "rejthreshtail = int(header['rejthreshtail'])\n",
    "rejthreshupper = int(header['rejthreshupper'])\n",
    "rejthreshlower = int(header['rejthreshlower'])\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'channel_ids': r_cache.get_channel_ids(),\n",
    "    'channel_groups': r_cache.get_channel_groups(),\n",
    "    'tetrode_ids': r_cache.get_channel_groups() + 1\n",
    "})\n",
    "\n",
    "for tetrode in active_tetrodes:\n",
    "\n",
    "    tetrode = int(tetrode)\n",
    "    \n",
    "    tetrode_filename = save_dir / Path(set_file.stem + '.{}'.format(tetrode))\n",
    "    \n",
    "    tetrode_channels = df.loc[df['tetrode_ids'] == tetrode, 'channel_ids'].values + 1\n",
    "    \n",
    "    traces = np.zeros((4, r_cache.get_num_frames()))\n",
    "    traces[(tetrode_channels - 1) % 4, :] = r_cache.get_traces(channel_ids=tetrode_channels-1)\n",
    "\n",
    "    n_samples = traces.shape[1]\n",
    "\n",
    "    # create a time array that represents the 48kHz sampled data times\n",
    "    t = np.arange(0, n_samples) / Fs  # creates a time array of the signal starting from 0 (in seconds)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6b678887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spikeextractors.extractors.bindatrecordingextractor.bindatrecordingextractor.BinDatRecordingExtractor at 0x7fb78c216730>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "86a1fba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spikes(data, threshold):\n",
    "    all_spikes = np.array([])\n",
    "\n",
    "    for i, channel_data in enumerate(data):\n",
    "        spike_indices = np.where(channel_data >= threshold[i])[0]\n",
    "\n",
    "        if len(spike_indices) == 0:\n",
    "            continue\n",
    "\n",
    "        spike_indices = find_consec(spike_indices)\n",
    "\n",
    "        spike_indices = np.asarray([value[0] for value in spike_indices])\n",
    "\n",
    "        if len(all_spikes) == 0:\n",
    "            # this is the first iteration of the tetrode, no need to sort\n",
    "            unadded_spikes = spike_indices\n",
    "        else:\n",
    "            idx = matching_ind(all_spikes, spike_indices)\n",
    "            if len(idx) == 0:\n",
    "                unadded_spikes = spike_indices\n",
    "            else:\n",
    "                unadded_spikes = np.setdiff1d(spike_indices, all_spikes[idx])\n",
    "\n",
    "        if len(all_spikes) != 0:\n",
    "            all_spikes = np.sort(np.concatenate((all_spikes, unadded_spikes)))\n",
    "            unadded_spikes = None\n",
    "        else:\n",
    "            all_spikes = np.array(unadded_spikes)\n",
    "\n",
    "    return all_spikes\n",
    "\n",
    "def find_consec(data):\n",
    "    '''finds the consecutive numbers and outputs as a list'''\n",
    "    consecutive_values = []  # a list for the output\n",
    "    current_consecutive = [data[0]]\n",
    "\n",
    "    if len(data) == 1:\n",
    "        return [[data[0]]]\n",
    "\n",
    "    for index in range(1, len(data)):\n",
    "\n",
    "        if data[index] == data[index - 1] + 1:\n",
    "            current_consecutive.append(data[index])\n",
    "\n",
    "            if index == len(data) - 1:\n",
    "                consecutive_values.append(current_consecutive)\n",
    "\n",
    "        else:\n",
    "            consecutive_values.append(current_consecutive)\n",
    "            current_consecutive = [data[index]]\n",
    "\n",
    "            if index == len(data) - 1:\n",
    "                consecutive_values.append(current_consecutive)\n",
    "    return consecutive_values\n",
    "\n",
    "def matching_ind(haystack, needle):\n",
    "    idx = np.searchsorted(haystack, needle)\n",
    "    mask = idx < haystack.size\n",
    "    mask[mask] = haystack[idx[mask]] == needle[mask]\n",
    "    idx = idx[mask]\n",
    "    return idx\n",
    "\n",
    "def validate_spikes(tetrode, spikes, data, t, pre_spike_samples=10, post_spike_samples=40, rejstart=30,\n",
    "                    rejthreshtail=43, rejthreshupper=100, rejthreshlower=-100):\n",
    "    latest_spike = None\n",
    "\n",
    "    spike_count = 0\n",
    "    percentage_values = [int(value) for value in np.rint(np.linspace(0, len(spikes), num=21)).tolist()]\n",
    "\n",
    "    n_max = data.shape[1]\n",
    "\n",
    "    tetrode_spikes = {}\n",
    "\n",
    "    for spike in sorted(spikes):\n",
    "        # iterate through each spike and validate to ensure no spikes occur at the same time or within the\n",
    "        # refractory period\n",
    "\n",
    "        spike_count += 1\n",
    "\n",
    "        if spike_count in percentage_values:\n",
    "            pass\n",
    "\n",
    "        if spike - pre_spike_samples + 1 < 0:\n",
    "            continue\n",
    "\n",
    "        elif spike + post_spike_samples >= n_max:\n",
    "            continue\n",
    "\n",
    "        if latest_spike is not None:\n",
    "            if spike != latest_spike:\n",
    "                if spike in spike_refractory:\n",
    "                    # ensures no overlapping spikes\n",
    "                    continue\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        latest_spike = spike\n",
    "        spike_refractory = list(np.arange(spike + 1, spike + post_spike_samples + 1))\n",
    "\n",
    "        # spike_time = t[int(spike)]\n",
    "        spike_time = t[int(spike)]\n",
    "\n",
    "        # waveform_indices = np.where((t>=spike_time-250/1e6) & (t<=spike_time+850/1e6))[0]  # too slow\n",
    "        waveform_indices = np.arange(spike - pre_spike_samples + 1, spike + post_spike_samples + 1).astype(int)\n",
    "\n",
    "        # spike_t = t[waveform_indices] - spike_time  # making the times from -200 us to 800 us\n",
    "\n",
    "        # spike_waveform = np.zeros((len(tetrode_channels), 50))\n",
    "\n",
    "        spike_waveform = data[:, waveform_indices]\n",
    "\n",
    "        spike_time = spike_time * 96000  # multiply it by the timebase to get the frame count\n",
    "\n",
    "        spike_waveform = np.rint(spike_waveform)\n",
    "\n",
    "        # artifact rejection\n",
    "\n",
    "        if sum(spike_waveform[:, rejstart:].flatten() > rejthreshtail) > 0:\n",
    "            # this is 33% above baseline (0)\n",
    "            continue\n",
    "\n",
    "        # check if the first sample is well above or well below baseline\n",
    "        elif sum(spike_waveform[:, 0].flatten() > rejthreshupper) > 0:\n",
    "            # the first sample is >100\n",
    "            continue\n",
    "\n",
    "        elif sum(spike_waveform[:, 0].flatten() < rejthreshlower) > 0:\n",
    "            # or < -100\n",
    "            continue\n",
    "\n",
    "        tetrode_spikes[spike_time] = spike_waveform\n",
    "\n",
    "        # latest_spike = spike\n",
    "        # spike_refractory = list(np.arange(spike + 1, spike + post_spike_samples + 1))\n",
    "\n",
    "    return tetrode_spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d57ab544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13, 14, 15, 16])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tetrode_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ffbdb5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 3\n",
    "\n",
    "tetrode_spikes = {}  # creates an empty dictionary to hold the spike times\n",
    "# for each tetrode, find the spikes\n",
    "\n",
    "k = 0\n",
    "\n",
    "# data = int16toint8(data)  # converting the data into int8\n",
    "\n",
    "tetrode_thresholds = []\n",
    "for channel_index, channel in enumerate(tetrode_channels):\n",
    "    k += 1\n",
    "    '''\n",
    "    Auto thresholding technique incorporated by:\n",
    "    Quian Quiroga in 2014 - Unsupervised Spike Detection and Sorting with Wavelets and\n",
    "    Superparamagnetic Clustering\n",
    "    Thr = 4*sigma, sigma = median(abs(x)/0.6745)\n",
    "    '''\n",
    "    standard_deviations = float(threshold)\n",
    "\n",
    "    sigma_n = np.median(np.divide(np.abs(data[channel_index, :]), 0.6745))\n",
    "    # threshold = sigma_n / channel_max\n",
    "    # threshold = standard_deviations * sigma_n\n",
    "    tetrode_thresholds.append(standard_deviations * sigma_n)\n",
    "\n",
    "valid_spikes = get_spikes(data, tetrode_thresholds)\n",
    "\n",
    "# threshold is done in 16 bit values, but the rejection is done in 8bit, so we convert here\n",
    "# data = int16toint8(data)  # converting the data into int8\n",
    "\n",
    "data_int16 = int16toint8(data)  # converting the"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "717e709d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 20993.32839140104, 15389.177168272796, 16474.425500370642]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tetrode_thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4dd12d8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  126,  1386,  1466,  1495,  1697,  1714,  1735,  2635,  2803,\n",
       "        2824,  2838,  2923,  2942,  4677,  4691,  4708,  5145,  5166,\n",
       "        5239,  5261,  5784,  5903,  5924,  6413,  6442,  6699,  6712,\n",
       "        6732,  6950,  7065,  7080,  7102,  7219,  7260,  8222,  8297,\n",
       "        8894,  8915,  9690,  9709,  9727, 10221, 10247, 10731, 10875,\n",
       "       11169, 11204, 11386, 11803, 11818, 11958, 11980, 12689, 12702,\n",
       "       12883, 12898, 12911, 14247, 14276, 14487, 14967, 15129, 15144,\n",
       "       15420, 15437, 15664, 15682, 16586, 16628, 16651, 16670, 16760,\n",
       "       16781, 16794, 16824, 16838, 16862, 17386, 17403, 17426, 17759,\n",
       "       18324, 18343, 18756, 19596, 19614, 19629, 19820, 19833, 19900,\n",
       "       20139, 20689, 20703, 21325, 21338, 21469, 21482, 21526, 21540,\n",
       "       22095, 22371, 22392, 22712, 22744, 23041, 23073, 23564, 23804,\n",
       "       23819, 23835, 23987, 24009, 24127, 24441, 25212, 25374, 25392,\n",
       "       25417, 25462, 25482, 26015, 26368, 26382, 27043, 27059, 27746,\n",
       "       27762, 29277, 29303, 30511, 32425, 33570, 33585, 33740, 35916,\n",
       "       36789, 37134, 37169, 37223, 37451, 37670, 38046, 38067, 38190,\n",
       "       38203, 39497, 39513, 39898, 40223, 40248, 42146, 42170, 42192,\n",
       "       42306, 42328, 42585, 42685, 42711, 43442, 43468, 43757, 44102,\n",
       "       44126, 44483, 44540, 44554, 44567, 44844, 44864, 44876, 44975,\n",
       "       45596, 45626, 46416, 46432, 47236, 47256, 47275, 47312, 47350,\n",
       "       47457, 48438, 48453, 48701, 49075, 49097, 49112, 49501, 49520,\n",
       "       50293, 51189, 51933, 51952, 51968, 51983, 52023, 52513, 52525,\n",
       "       52724, 55141, 55156, 55180, 56230, 56978, 57071, 57088, 57476,\n",
       "       57495])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tetrode_ids = sorting_nwb.get_units_property(property_name='group')\n",
    "tetrode_ids = np.array(tetrode_ids)\n",
    "\n",
    "unit_ids = np.array(sorting_nwb.get_unit_ids())\n",
    "spike_train = sorting_nwb.get_units_spike_train(unit_ids=unit_ids[tetrode_ids==3])\n",
    "\n",
    "sorted_spike_train = np.sort(np.concatenate(spike_train))\n",
    "\n",
    "sorted_spike_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "83b2554a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     2,   464,   584,   627,   691,   751,   787,   788,\n",
       "        1021,  1088,  1150,  1230,  1238,  1571,  1583,  1632,  1644,\n",
       "        1826,  2228,  2238,  2322,  2326,  2377,  2634,  2635,  2905,\n",
       "        2911,  2912,  3118,  3277,  3297,  3653,  3752,  3785,  3787,\n",
       "        3872,  3926,  4103,  4352,  4353,  4364,  4577,  4673,  4674,\n",
       "        4934,  4935,  5024,  5025,  5165,  5232,  5233,  5568,  5569,\n",
       "        5781,  5784,  5785,  5974,  5976,  6019,  6208,  6260,  6410,\n",
       "        6411,  6706,  6708,  6768,  6788,  6982,  6983,  7061,  7064,\n",
       "        7255,  7256,  7482,  7494,  7601,  7686,  7687,  7837,  8173,\n",
       "        8222,  8223,  8435,  8437,  8464,  8465,  8655,  8755,  8756,\n",
       "        9357,  9366,  9557,  9696, 10217, 10376, 10723, 10844, 10869,\n",
       "       10870, 11066, 11455, 11456, 11954, 11955, 12441, 12444, 12451,\n",
       "       12454, 12677, 12775, 13078, 13087, 13642, 13643, 13644, 14112,\n",
       "       14113, 14241, 14242, 14483, 14484, 14502, 14621, 14681, 14682,\n",
       "       14850, 14853, 14862, 14967, 14968, 15122, 15123, 15124, 15448,\n",
       "       15449, 15673, 15676, 15830, 15831, 15952, 16108, 16109, 16110,\n",
       "       16189, 16227, 16520, 16521, 16756, 16757, 16984, 17204, 17205,\n",
       "       17397, 17398, 17573, 17583, 17654, 17655, 17747, 17748, 17906,\n",
       "       17908, 18177, 18178, 18257, 18320, 18321, 18396, 18581, 18757,\n",
       "       18758, 18863, 18957, 18958, 19026, 19086, 19098, 19121, 19217,\n",
       "       19342, 19343, 19592, 19593, 19696, 19824, 19825, 19899, 19900,\n",
       "       20139, 20214, 20551, 20552, 20553, 20848, 20849, 21052, 21083,\n",
       "       21406, 21407, 21441, 21525, 21919, 21920, 21921, 22105, 22367,\n",
       "       22368, 22556, 22712, 22713, 22785, 23036, 23038, 23301, 23493,\n",
       "       23799, 23800, 24084, 24158, 24463, 24464, 24476, 24477, 24478,\n",
       "       24701, 24754, 25007, 25143, 25144, 25460, 25461, 25619, 25621,\n",
       "       25838, 26020, 26021, 26348, 26350, 27120, 27121, 27694, 27752,\n",
       "       28440, 28587, 28641, 28988, 29060, 29282, 29349, 29673, 29794,\n",
       "       29800, 30034, 30035, 30099, 30100, 30494, 30735, 30886, 30887,\n",
       "       31167, 31750, 31934, 31965, 32149, 32830, 33704, 34144, 34159,\n",
       "       34160, 34262, 34626, 34766, 34767, 34882, 35028, 35029, 35292,\n",
       "       35333, 35334, 35607, 35608, 35637, 35638, 36074, 36332, 36352,\n",
       "       36353, 36378, 36435, 36488, 36628, 36631, 36669, 36681, 36758,\n",
       "       36781, 36794, 36981, 36982, 37138, 37203, 37535, 37536, 37670,\n",
       "       37671, 37986, 38177, 38302, 38607, 39047, 39195, 39330, 39381,\n",
       "       39403, 39438, 39491, 39493, 39536, 39680, 39681, 39713, 39724,\n",
       "       39768, 39769, 39785, 39873, 39874, 39875, 39883, 39891, 39937,\n",
       "       40147, 40148, 40149, 40175, 40218, 40219, 40233, 40236, 40265,\n",
       "       40414, 40499, 40500, 40512, 40637, 40667, 40677, 40729, 40736,\n",
       "       40787, 40805, 40864, 40896, 40913, 40931, 40965, 40999, 41013,\n",
       "       41064, 41068, 41083, 41092, 41098, 41114, 41176, 41196, 41217,\n",
       "       41229, 41297, 41321, 41330, 41347, 41401, 41417, 41418, 41431,\n",
       "       41473, 41587, 41597, 41687, 41689, 41759, 41799, 41818, 41918,\n",
       "       41932, 41946, 42028, 42029, 42088, 42165, 42240, 42241, 42272,\n",
       "       42288, 42290, 42401, 42413, 42457, 42458, 42482, 42491, 42641,\n",
       "       42654, 42679, 42680, 43017, 43018, 43038, 43201, 43409, 43595,\n",
       "       43596, 43777, 43785, 43802, 43863, 43913, 44232, 44329, 44479,\n",
       "       44481, 44532, 44533, 44748, 44749, 44750, 44758, 44895, 44942,\n",
       "       45195, 45523, 45524, 45545, 45586, 45752, 45761, 45850, 45851,\n",
       "       45881, 45965, 45976, 46266, 46305, 46306, 46326, 46420, 46431,\n",
       "       46524, 46526, 46640, 46643, 46765, 46865, 46951, 47114, 47126,\n",
       "       47128, 47223, 47363, 47585, 47596, 47783, 47784, 48079, 48259,\n",
       "       48561, 48562, 48572, 48655, 48805, 48892, 49014, 49023, 49039,\n",
       "       49114, 49392, 49494, 49495, 49538, 49624, 49641, 49676, 49679,\n",
       "       49690, 49788, 50057, 50304, 50455, 50457, 50569, 50648, 51143,\n",
       "       51566, 52024, 52082, 52194, 52252, 52353, 52519, 52520, 52824,\n",
       "       52832, 53194, 53672, 53682, 53856, 54157, 55616, 55621, 55640,\n",
       "       55688, 55732, 55806, 55855, 55906, 55966, 56070, 56072, 56203,\n",
       "       56204, 56548, 56570, 56571, 57261])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a0c5e9e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00442661, -0.00972959, -0.01101197, ..., -0.02006962,\n",
       "        -0.00963475, -0.00524077],\n",
       "       [-0.00532733, -0.013263  , -0.00653222, ..., -0.00630658,\n",
       "        -0.00487059, -0.00842494],\n",
       "       [ 0.00252745, -0.0046536 , -0.00752856, ..., -0.03252275,\n",
       "        -0.01737364, -0.00533893],\n",
       "       [-0.00476253, -0.01768241, -0.01526715, ..., -0.02812541,\n",
       "        -0.03498014, -0.04006043]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b9bf372a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 13,   0,   0, ...,   0,   0,   0],\n",
       "       [ 14, -15,   0, ..., -53, -56, -46],\n",
       "       [  6,   0,   0, ..., -35, -38, -37],\n",
       "       [ 40,  20,   0, ..., -33, -28, -19]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_int16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "65ae01ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  3574.,   -230.,      0., ...,      0.,      0.,      0.],\n",
       "       [  3682.,  -3870.,      0., ..., -13700., -14410., -11876.],\n",
       "       [  1714.,   -188.,      0., ...,  -9006.,  -9980.,  -9502.],\n",
       "       [ 10480.,   5308.,      0., ...,  -8460.,  -7266.,  -4906.]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6c9a9445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     2,   464,   584,   627,   691,   751,   787,   788,\n",
       "        1021,  1088,  1150,  1230,  1238,  1571,  1583,  1632,  1644,\n",
       "        1826,  2228,  2238,  2322,  2326,  2377,  2634,  2635,  2905,\n",
       "        2911,  2912,  3118,  3277,  3297,  3653,  3752,  3785,  3787,\n",
       "        3872,  3926,  4103,  4352,  4353,  4364,  4577,  4673,  4674,\n",
       "        4934,  4935,  5024,  5025,  5165,  5232,  5233,  5568,  5569,\n",
       "        5781,  5784,  5785,  5974,  5976,  6019,  6208,  6260,  6410,\n",
       "        6411,  6706,  6708,  6768,  6788,  6982,  6983,  7061,  7064,\n",
       "        7255,  7256,  7482,  7494,  7601,  7686,  7687,  7837,  8173,\n",
       "        8222,  8223,  8435,  8437,  8464,  8465,  8655,  8755,  8756,\n",
       "        9357,  9366,  9557,  9696, 10217, 10376, 10723, 10844, 10869,\n",
       "       10870, 11066, 11455, 11456, 11954, 11955, 12441, 12444, 12451,\n",
       "       12454, 12677, 12775, 13078, 13087, 13642, 13643, 13644, 14112,\n",
       "       14113, 14241, 14242, 14483, 14484, 14502, 14621, 14681, 14682,\n",
       "       14850, 14853, 14862, 14967, 14968, 15122, 15123, 15124, 15448,\n",
       "       15449, 15673, 15676, 15830, 15831, 15952, 16108, 16109, 16110,\n",
       "       16189, 16227, 16520, 16521, 16756, 16757, 16984, 17204, 17205,\n",
       "       17397, 17398, 17573, 17583, 17654, 17655, 17747, 17748, 17906,\n",
       "       17908, 18177, 18178, 18257, 18320, 18321, 18396, 18581, 18757,\n",
       "       18758, 18863, 18957, 18958, 19026, 19086, 19098, 19121, 19217,\n",
       "       19342, 19343, 19592, 19593, 19696, 19824, 19825, 19899, 19900,\n",
       "       20139, 20214, 20551, 20552, 20553, 20848, 20849, 21052, 21083,\n",
       "       21406, 21407, 21441, 21525, 21919, 21920, 21921, 22105, 22367,\n",
       "       22368, 22556, 22712, 22713, 22785, 23036, 23038, 23301, 23493,\n",
       "       23799, 23800, 24084, 24158, 24463, 24464, 24476, 24477, 24478,\n",
       "       24701, 24754, 25007, 25143, 25144, 25460, 25461, 25619, 25621,\n",
       "       25838, 26020, 26021, 26348, 26350, 27120, 27121, 27694, 27752,\n",
       "       28440, 28587, 28641, 28988, 29060, 29282, 29349, 29673, 29794,\n",
       "       29800, 30034, 30035, 30099, 30100, 30494, 30735, 30886, 30887,\n",
       "       31167, 31750, 31934, 31965, 32149, 32830, 33704, 34144, 34159,\n",
       "       34160, 34262, 34626, 34766, 34767, 34882, 35028, 35029, 35292,\n",
       "       35333, 35334, 35607, 35608, 35637, 35638, 36074, 36332, 36352,\n",
       "       36353, 36378, 36435, 36488, 36628, 36631, 36669, 36681, 36758,\n",
       "       36781, 36794, 36981, 36982, 37138, 37203, 37535, 37536, 37670,\n",
       "       37671, 37986, 38177, 38302, 38607, 39047, 39195, 39330, 39381,\n",
       "       39403, 39438, 39491, 39493, 39536, 39680, 39681, 39713, 39724,\n",
       "       39768, 39769, 39785, 39873, 39874, 39875, 39883, 39891, 39937,\n",
       "       40147, 40148, 40149, 40175, 40218, 40219, 40233, 40236, 40265,\n",
       "       40414, 40499, 40500, 40512, 40637, 40667, 40677, 40729, 40736,\n",
       "       40787, 40805, 40864, 40896, 40913, 40931, 40965, 40999, 41013,\n",
       "       41064, 41068, 41083, 41092, 41098, 41114, 41176, 41196, 41217,\n",
       "       41229, 41297, 41321, 41330, 41347, 41401, 41417, 41418, 41431,\n",
       "       41473, 41587, 41597, 41687, 41689, 41759, 41799, 41818, 41918,\n",
       "       41932, 41946, 42028, 42029, 42088, 42165, 42240, 42241, 42272,\n",
       "       42288, 42290, 42401, 42413, 42457, 42458, 42482, 42491, 42641,\n",
       "       42654, 42679, 42680, 43017, 43018, 43038, 43201, 43409, 43595,\n",
       "       43596, 43777, 43785, 43802, 43863, 43913, 44232, 44329, 44479,\n",
       "       44481, 44532, 44533, 44748, 44749, 44750, 44758, 44895, 44942,\n",
       "       45195, 45523, 45524, 45545, 45586, 45752, 45761, 45850, 45851,\n",
       "       45881, 45965, 45976, 46266, 46305, 46306, 46326, 46420, 46431,\n",
       "       46524, 46526, 46640, 46643, 46765, 46865, 46951, 47114, 47126,\n",
       "       47128, 47223, 47363, 47585, 47596, 47783, 47784, 48079, 48259,\n",
       "       48561, 48562, 48572, 48655, 48805, 48892, 49014, 49023, 49039,\n",
       "       49114, 49392, 49494, 49495, 49538, 49624, 49641, 49676, 49679,\n",
       "       49690, 49788, 50057, 50304, 50455, 50457, 50569, 50648, 51143,\n",
       "       51566, 52024, 52082, 52194, 52252, 52353, 52519, 52520, 52824,\n",
       "       52832, 53194, 53672, 53682, 53856, 54157, 55616, 55621, 55640,\n",
       "       55688, 55732, 55806, 55855, 55906, 55966, 56070, 56072, 56203,\n",
       "       56204, 56548, 56570, 56571, 57261])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7285c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------Write Tetrode Data----------------------------------------\n",
    "\n",
    "Fs = get_Fs(set_filename)  # read the sampling frequency from the .set file, most like 48k\n",
    "\n",
    "active_tetrodes = get_active_tetrode(set_filename)\n",
    "\n",
    "# converts the data one tetrode at a time so we can eliminate memory errors\n",
    "\n",
    "pre_spike_samples = int(get_setfile_parameter('pretrigSamps', set_filename))\n",
    "post_spike_samples = int(get_setfile_parameter('spikeLockout', set_filename))\n",
    "rejstart = int(get_setfile_parameter('rejstart', set_filename))\n",
    "rejthreshtail = int(get_setfile_parameter('rejthreshtail', set_filename))\n",
    "rejthreshupper = int(get_setfile_parameter('rejthreshupper', set_filename))\n",
    "rejthreshlower = int(get_setfile_parameter('rejthreshlower', set_filename))\n",
    "\n",
    "for tetrode in active_tetrodes:\n",
    "\n",
    "    tetrode = int(tetrode)\n",
    "    # check if this tetrode exists already\n",
    "\n",
    "    tetrode_filename = os.path.join(directory, '%s.%d' % (tint_basename, tetrode))\n",
    "    if os.path.exists(tetrode_filename):\n",
    "        continue\n",
    "\n",
    "    tetrode_channels = get_channel_from_tetrode(tetrode)  # get the channels (from range of 1->64)\n",
    "\n",
    "    data = get_bin_data(bin_filename, tetrode=tetrode)  # 16bit, get data associated with the tetrode\n",
    "\n",
    "    # converting data to uV\n",
    "\n",
    "    n_samples = data.shape[1]\n",
    "    # create a time array that represents the 48kHz sampled data times\n",
    "    t = np.arange(0, n_samples) / Fs  # creates a time array of the signal starting from 0 (in seconds)\n",
    "\n",
    "    if not os.path.exists(tetrode_filename):\n",
    "\n",
    "        # ---------------------------Find the spikes in the unit data --------------------------------------\n",
    "\n",
    "        tetrode_spikes = {}  # creates an empty dictionary to hold the spike times\n",
    "        # for each tetrode, find the spikes\n",
    "\n",
    "        k = 0\n",
    "\n",
    "        # data = int16toint8(data)  # converting the data into int8\n",
    "\n",
    "        tetrode_thresholds = []\n",
    "        for channel_index, channel in enumerate(tetrode_channels):\n",
    "            k += 1\n",
    "            '''\n",
    "            Auto thresholding technique incorporated by:\n",
    "            Quian Quiroga in 2014 - Unsupervised Spike Detection and Sorting with Wavelets and\n",
    "            Superparamagnetic Clustering\n",
    "            Thr = 4*sigma, sigma = median(abs(x)/0.6745)\n",
    "            '''\n",
    "            standard_deviations = float(threshold)\n",
    "\n",
    "            sigma_n = np.median(np.divide(np.abs(data[channel_index, :]), 0.6745))\n",
    "            # threshold = sigma_n / channel_max\n",
    "            # threshold = standard_deviations * sigma_n\n",
    "            tetrode_thresholds.append(standard_deviations * sigma_n)\n",
    "\n",
    "        valid_spikes = get_spikes(data, tetrode_thresholds)\n",
    "\n",
    "        # threshold is done in 16 bit values, but the rejection is done in 8bit, so we convert here\n",
    "        # data = int16toint8(data)  # converting the data into int8\n",
    "\n",
    "        data = int16toint8(data)  # converting the data into int8\n",
    "\n",
    "        tetrode_spikes = validate_spikes(tetrode, valid_spikes, data, t, pre_spike_samples,\n",
    "                                         post_spike_samples, rejstart, rejthreshtail, rejthreshupper,\n",
    "                                         rejthreshlower)\n",
    "\n",
    "        # write the tetrode data to create the .N file\n",
    "        write_tetrode(tetrode_filename, tetrode_spikes, Fs)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    data = None\n",
    "    tetrode_spikes = None\n",
    "    valid_spikes = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09179af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfc6a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from .conversion_utils import get_set_header\n",
    "import numpy as np\n",
    "import struct\n",
    "\n",
    "\n",
    "def write_tetrode(filepath, data, Fs):\n",
    "\n",
    "    session_path, session_filename = os.path.split(filepath)\n",
    "    tint_basename = os.path.splitext(session_filename)[0]\n",
    "    set_filename = os.path.join(session_path, '%s.set' % tint_basename)\n",
    "\n",
    "    n = len(data)\n",
    "\n",
    "    header = get_set_header(set_filename)\n",
    "\n",
    "    with open(filepath, 'w') as f:\n",
    "        num_chans = 'num_chans 4'\n",
    "        timebase_head = '\\ntimebase %d hz' % (96000)\n",
    "        bp_timestamp = '\\nbytes_per_timestamp %d' % (4)\n",
    "        # samps_per_spike = '\\nsamples_per_spike %d' % (int(Fs*1e-3))\n",
    "        samps_per_spike = '\\nsamples_per_spike %d' % (50)\n",
    "        sample_rate = '\\nsample_rate %d hz' % (Fs)\n",
    "        b_p_sample = '\\nbytes_per_sample %d' % (1)\n",
    "        # b_p_sample = '\\nbytes_per_sample %d' % (4)\n",
    "        spike_form = '\\nspike_format t,ch1,t,ch2,t,ch3,t,ch4'\n",
    "        num_spikes = '\\nnum_spikes %d' % (n)\n",
    "        start = '\\ndata_start'\n",
    "\n",
    "        write_order = [header, num_chans, timebase_head,\n",
    "                       bp_timestamp,\n",
    "                       samps_per_spike, sample_rate, b_p_sample, spike_form, num_spikes, start]\n",
    "\n",
    "        f.writelines(write_order)\n",
    "\n",
    "    # rearranging the data to have a flat array of t1, waveform1, t2, waveform2, t3, waveform3, etc....\n",
    "    spike_times = np.asarray(sorted(data.keys()))\n",
    "\n",
    "    # the spike times are repeated for each channel so lets tile this\n",
    "    spike_times = np.tile(spike_times, (4, 1))\n",
    "    spike_times = spike_times.flatten(order='F')\n",
    "\n",
    "    spike_values = np.asarray([value for (key, value) in sorted(data.items())])\n",
    "\n",
    "    # this will create a (n_samples, n_channels, n_samples_per_spike) => (n, 4, 50) sized matrix, we will create a\n",
    "    # matrix of all the samples and channels going from ch1 -> ch4 for each spike time\n",
    "    # time1 ch1_data\n",
    "    # time1 ch2_data\n",
    "    # time1 ch3_data\n",
    "    # time1 ch4_data\n",
    "    # time2 ch1_data\n",
    "    # time2 ch2_data\n",
    "    # .\n",
    "    # .\n",
    "    # .\n",
    "\n",
    "    spike_values = spike_values.reshape((n * 4, 50))  # create the 4nx50 channel data matrix\n",
    "\n",
    "    # make the first column the time values\n",
    "    spike_array = np.hstack((spike_times.reshape(len(spike_times), 1), spike_values))\n",
    "\n",
    "    data = None\n",
    "    spike_times = None\n",
    "    spike_values = None\n",
    "\n",
    "    spike_n = spike_array.shape[0]\n",
    "\n",
    "    t_packed = struct.pack('>%di' % spike_n, *spike_array[:, 0].astype(int))\n",
    "    spike_array = spike_array[:, 1:]  # removing time data from this matrix to save memory\n",
    "\n",
    "    spike_data_pack = struct.pack('<%db' % (spike_n*50), *spike_array.astype(int).flatten())\n",
    "\n",
    "    spike_array = None\n",
    "\n",
    "    # now we need to combine the lists by alternating\n",
    "\n",
    "    comb_list = [None] * (2*spike_n)\n",
    "    comb_list[::2] = [t_packed[i:i + 4] for i in range(0, len(t_packed), 4)]  # breaks up t_packed into a list,\n",
    "    # each timestamp is one 4 byte integer\n",
    "    comb_list[1::2] = [spike_data_pack[i:i + 50] for i in range(0, len(spike_data_pack), 50)]  # breaks up spike_data_\n",
    "    # pack and puts it into a list, each spike is 50 one byte integers\n",
    "\n",
    "    t_packed = None\n",
    "    spike_data_pack = None\n",
    "\n",
    "    write_order = []\n",
    "    with open(filepath, 'rb+') as f:\n",
    "\n",
    "        write_order.extend(comb_list)\n",
    "        write_order.append(bytes('\\r\\ndata_end\\r\\n', 'utf-8'))\n",
    "\n",
    "        f.seek(0, 2)\n",
    "        f.writelines(write_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "1dea0450",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_tetrode_files(sorting_extractor, save_dir):\n",
    "    '''Given a sorting extractor object create .X (tetrode) files.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sorting_extractor : spikeextractors.SortingExtractor\n",
    "    save_dir : str or Path\n",
    "        Directory where to save the output\n",
    "    '''\n",
    "    # TODO ...\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "1e81b2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_tint(sorting_extractor, filename):\n",
    "    '''Given a sorting extractor object, write appropriate data\n",
    "    to TINT format (from Axona). Will therefore create .X (tetrode),\n",
    "    .cut and .clu (spike sorting information) files.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sorting_extractor : spikeextractors.SortingExtractor\n",
    "    filename : str or Path\n",
    "        Full path and base filename shared by all output files \n",
    "        (e.g. my_dir/my_file will yield\n",
    "        my_dir/my_file.1, my_dir/my_file.2, ..., \n",
    "        my_dir/my_file_1.cut, my_dir/my_file_2.cut, ...,\n",
    "        my_dir/my_file_1.clu, my_dir/my_file_2.clu, ...)\n",
    "        If a file extension is given, it is simply ignored.\n",
    "        \n",
    "    Notes\n",
    "    -----\n",
    "    For details about the .X file format see:\n",
    "    http://space-memory-navigation.org/DacqUSBFileFormats.pdf\n",
    "    '''\n",
    "    # Make sure directory exists\n",
    "    filename.parent.absolute().mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # writes to .X files for each tetrode\n",
    "    # TODO...\n",
    "    write_to_tetrode_file(sorting_extractor, filename)\n",
    "    \n",
    "    # writes to .cut and .clu files for each tetrode\n",
    "    write_unit_labels_to_file(sorting_extractor, filename)\n",
    "    \n",
    "    # Position data?\n",
    "    # TODO ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "048d6928",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = Path(\n",
    "    '/mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/Axona_Tint_1ms/spikeextractors_to_tint/20201004_Tint'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "84ce7c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting Tetrode 0\n",
      "Converting Tetrode 1\n",
      "Converting Tetrode 2\n",
      "Converting Tetrode 3\n"
     ]
    }
   ],
   "source": [
    "write_to_tint(sorting_nwb, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15691ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4051f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490f3329",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13b87295",
   "metadata": {},
   "source": [
    "## Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e684fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_values(x, maxabs, bound=127):\n",
    "    '''Scale signal `x` between -`bound` and +`bound`,\n",
    "    preserves 0 point.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : np.array\n",
    "    absmax : numeric\n",
    "        max(|min(x)|, |max(x)|)\n",
    "    bound : numeric\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    np.array\n",
    "    '''\n",
    "    return x / maxabs * bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db6c27f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spikeinterface",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
