{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38378a04",
   "metadata": {},
   "source": [
    "# Convert recording and sorting extractor data to TINT format\n",
    "\n",
    "The Hussaini lab uses the proprietary TINT software from Axona to analyze extracellular electrophysiology data. While we are already able to read various data formats from Axona (`raw` data or `unit` data) into spikeinterface, perform preprocessing, spike sorting and export the data to NWB, we also want to allow to export data to the TINT format. \n",
    "\n",
    "The TINT format is essentially the same as the `unit` data, including `.X` and `.pos` files, but also `.cut` or `.clu`. The latter two contain information about the spike sorted units.\n",
    "\n",
    "The conversion can be facilitated by using the existing tools from the Hussaini lab, which [convert `.bin` data to `.X` and `.pos`](https://github.com/HussainiLab/BinConverter/blob/master/BinConverter/core/ConversionFunctions.py). Some of this code is only relevant for using the GUI, which did not work for me. I cleared out GUI code and ran a conversion from `.bin` to `.X` and `.pos` in this notebook: [explore_hussaini_tools.ipynb](https://github.com/sbuergers/hussaini-lab-to-nwb-notebooks/blob/master/explore_hussaini_tools.ipynb).\n",
    "\n",
    "They also already wrote a [`write_cut()`](https://github.com/GeoffBarrett/gebaSpike/blob/967097ec28592182ef9783d2d391930e1c63ca58/gebaSpike/core/writeCut.py) function.\n",
    "\n",
    "We can test our solutions by reading data with these [Hussaini lab tools](https://github.com/HussainiLab/BinConverter/blob/master/BinConverter/core/Tint_Matlab.py). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8f7aaa",
   "metadata": {},
   "source": [
    "<a id='index'></a>\n",
    "## Index\n",
    "\n",
    "* [Testing functions](#testing_functions)\n",
    "* [Hussaini-lab functions](#hussaini-lab_functions)\n",
    "* [Convert Recording Extractor to TINT](#Convert_recording_extractor_to_tint)\n",
    "* [Convert Sorting Extractor to TINT](#Convert_sorting_extractor_to_tint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83cf3093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.5 (default, Sep  4 2020, 07:30:14) \n",
      "[GCC 7.3.0] linux /home/sbuergers/spikeinterface/spikeinterface_new_api/venv/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (16, 8)\n",
    "plt.rcParams.update({'font.size':14})\n",
    "%matplotlib inline\n",
    "\n",
    "import spikeextractors as se\n",
    "\n",
    "print(sys.version, sys.platform, sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1210bfc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input directory =  /mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/sample_bin_to_tint_no_bin\n",
      "Output directory =  /mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/sample_bin_to_tint_no_bin/conversion_to_tint\n"
     ]
    }
   ],
   "source": [
    "# Directories\n",
    "\n",
    "dir_name = Path('/mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/sample_bin_to_tint_no_bin')\n",
    "print('Input directory = ', dir_name)\n",
    "\n",
    "save_dir = dir_name / 'conversion_to_tint'\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "print('Output directory = ', save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aafca1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read cached spikeextractors data\n",
    "\n",
    "r_cache = se.load_extractor_from_pickle(os.path.join(dir_name, 'cached_unit_data_no_bin_preproc.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69609f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sbuergers/spikeinterface/spikeinterface_new_api/venv/lib/python3.8/site-packages/hdmf/common/table.py:442: UserWarning: An attribute 'name' already exists on DynamicTable 'electrodes' so this column cannot be accessed as an attribute, e.g., table.name; it can only be accessed using other methods, e.g., table['name'].\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Read NWB recording data\n",
    "\n",
    "nwb_dir = Path(dir_name, 'nwb')\n",
    "recording_nwb = se.NwbRecordingExtractor(nwb_dir / 'axona_tutorial_re2.nwb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29ca4d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read NWB sorting data\n",
    "\n",
    "sorting_nwb = se.NwbSortingExtractor(nwb_dir / 'axona_se_MS4.nwb', sampling_frequency=48000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43beb92a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spikeextractors.extractors.bindatrecordingextractor.bindatrecordingextractor.BinDatRecordingExtractor'>\n",
      "<class 'spikeextractors.extractors.nwbextractors.nwbextractors.NwbRecordingExtractor'>\n",
      "<class 'spikeextractors.extractors.nwbextractors.nwbextractors.NwbSortingExtractor'>\n"
     ]
    }
   ],
   "source": [
    "# Show data types of different objects\n",
    "\n",
    "print(type(r_cache))\n",
    "print(type(recording_nwb))\n",
    "print(type(sorting_nwb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b12d160",
   "metadata": {},
   "source": [
    "<a id=\"testing_functions\"></a>\n",
    "## Testing functions\n",
    "[back to index](#index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93029f86",
   "metadata": {},
   "source": [
    "As we start exporting to putative TINT format, we will want to check if we can read it back in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3d0bd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spikeextractors.extractors.axonaunitrecordingextractor import AxonaUnitRecordingExtractor\n",
    "import os\n",
    "\n",
    "\n",
    "def test_axonaunitrecordingextractor(filename):\n",
    "    '''Reads UNIT data with AxonaUnitRecordingExtractor and\n",
    "    performs some simple operations as a sanity check. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : str or Path\n",
    "        Full filename of `.set` file (could be any extension actually)\n",
    "    '''\n",
    "    re = AxonaUnitRecordingExtractor(filename=filename)\n",
    "    \n",
    "    # TEST AXONARECORDINGEXTRACTOR\n",
    "    # Retrieve some simple recording information and print it\n",
    "    recording = re\n",
    "    print('Channel ids = {}'.format(recording.get_channel_ids()))\n",
    "    print('Num. channels = {}'.format(len(recording.get_channel_ids())))\n",
    "    print('Sampling frequency = {} Hz'.format(recording.get_sampling_frequency()))\n",
    "    print('Num. timepoints = {}'.format(recording.get_num_frames()))\n",
    "    print('Stdev. on third channel = {}'.format(np.std(recording.get_traces(channel_ids=2))))\n",
    "    print('Location of third electrode = {}'.format(\n",
    "        recording.get_channel_property(channel_id=2, property_name='location')))\n",
    "    print('Channel groups = {}'.format(recording.get_channel_groups()))\n",
    "    \n",
    "    # TEST NEO_READER (axonaio)\n",
    "    print(recording.neo_reader.header['signal_channels'])\n",
    "    \n",
    "    \n",
    "def test_tetrode_files(filename):\n",
    "    '''Reads UNIT data with AxonaUnitRecordingExtractor and\n",
    "    performs some simple operations as a sanity check. \n",
    "    Will only test .X  and .set files (no .clu or .cut, no .pos).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : str or Path\n",
    "        Full filename of `.set` file (could be any extension actually)\n",
    "    '''\n",
    "    test_axonaunitrecordingextractor(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6333895",
   "metadata": {},
   "source": [
    "<a id=\"hussaini-lab_functions\"></a>\n",
    "## Hussaini-lab functions\n",
    "[back to index](#index)\n",
    "\n",
    "`gebaSpike` actually wants already existing `.cut` or `.clu` files, and allows modifying them. So these might not be all that useful for exporting to `.cut` or `.clu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4c221133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From \n",
    "# https://github.com/GeoffBarrett/gebaSpike/blob/967097ec28592182ef9783d2d391930e1c63ca58/gebaSpike/main.py\n",
    "\n",
    "def save_function(self):\n",
    "    \"\"\"\n",
    "    this method will save the .cut file\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if self.cut_filename.text() == default_filename:\n",
    "        return\n",
    "\n",
    "    save_filename = os.path.realpath(self.cut_filename.text())\n",
    "\n",
    "    if os.path.exists(save_filename):\n",
    "        self.choice = None\n",
    "        self.LogError.signal.emit('OverwriteCut!%s' % save_filename)\n",
    "        while self.choice is None:\n",
    "            time.sleep(0.1)\n",
    "\n",
    "        if self.choice != QtWidgets.QMessageBox.Yes:\n",
    "            return\n",
    "\n",
    "    if len(self.tetrode_data) == 0:\n",
    "        return\n",
    "\n",
    "    # organize the cut data\n",
    "    n_spikes_expected = self.tetrode_data.shape[1]\n",
    "    n_spikes = len(np.asarray([item for sublist in self.cell_indices.values() for item in sublist]))\n",
    "\n",
    "    # check that with the manipulation of the spikes, that we still have the correct number of spikes\n",
    "    if n_spikes != n_spikes_expected:\n",
    "        self.choice = None\n",
    "        self.LogError.signal.emit('cutSizeError')\n",
    "        while self.choice is None:\n",
    "            time.sleep(0.1)\n",
    "        return\n",
    "\n",
    "    # we will check if we are missing some of the spikes somehow. If we kept track of them, then the indices from\n",
    "    # the spikes, when sorted, should produce an array from 0 -> N-1 spikes.\n",
    "    if not np.array_equal(np.sort(np.asarray([item for sublist in self.cell_indices.values() for item in sublist])),\n",
    "                      np.arange(len(self.cut_data_original))):\n",
    "        self.choice = None\n",
    "        self.LogError.signal.emit('cutIndexError')\n",
    "        while self.choice is None:\n",
    "            time.sleep(0.1)\n",
    "        return\n",
    "\n",
    "    cut_values = np.zeros(n_spikes)\n",
    "    for cell, cell_indices in self.cell_indices.items():\n",
    "        cut_values[cell_indices] = cell\n",
    "\n",
    "    if '.clu.' in save_filename:\n",
    "        # save the .clu filename\n",
    "        write_clu(save_filename, cut_values)\n",
    "        self.choice = None\n",
    "        self.LogError.signal.emit('saveCompleteClu')\n",
    "        while self.choice is None:\n",
    "            time.sleep(0.1)\n",
    "        self.actions_made = False\n",
    "\n",
    "    else:\n",
    "        # save the cut filename\n",
    "        write_cut(save_filename, cut_values)\n",
    "        self.choice = None\n",
    "        self.LogError.signal.emit('saveComplete')\n",
    "        while self.choice is None:\n",
    "            time.sleep(0.1)\n",
    "        self.actions_made = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "01729f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From \n",
    "# https://github.com/GeoffBarrett/gebaSpike/blob/967097ec28592182ef9783d2d391930e1c63ca58/gebaSpike/core/writeCut.py\n",
    "\n",
    "def write_cut(cut_filename, cut, basename=None):\n",
    "    if basename is None:\n",
    "        basename = os.path.basename(os.path.splitext(cut_filename)[0])\n",
    "\n",
    "    unique_cells = np.unique(cut)\n",
    "\n",
    "    if 0 not in unique_cells:\n",
    "        # if it happens that there is no zero cell, add it anyways\n",
    "        unique_cells = np.insert(unique_cells, 0, 0)  # object, index, value to insert\n",
    "\n",
    "    n_clusters = len(np.unique(cut))\n",
    "    n_spikes = len(cut)\n",
    "\n",
    "    write_list = []  # the list of values to write\n",
    "\n",
    "    tab = '    '  # the spaces didn't line up with my tab so I just created a string with enough spaces\n",
    "    empty_space = '               '  # some of the empty spaces don't line up to x tabs\n",
    "\n",
    "    # we add 1 to n_clusters because zero is the garbage cell that no one uses\n",
    "    write_list.append('n_clusters: %d\\n' % (n_clusters))\n",
    "    write_list.append('n_channels: 4\\n')\n",
    "    write_list.append('n_params: 2\\n')\n",
    "    write_list.append('times_used_in_Vt:%s' % ((tab + '0') * 4 + '\\n'))\n",
    "\n",
    "    zero_string = (tab + '0') * 8 + '\\n'\n",
    "\n",
    "    for cell_i in np.arange(n_clusters):\n",
    "        write_list.append(' cluster: %d center:%s' % (cell_i, zero_string))\n",
    "        write_list.append('%smin:%s' % (empty_space, zero_string))\n",
    "        write_list.append('%smax:%s' % (empty_space, zero_string))\n",
    "    write_list.append('\\nExact_cut_for: %s spikes: %d\\n' % (basename, n_spikes))\n",
    "\n",
    "    # now the cut file lists 25 values per row\n",
    "    n_rows = int(np.floor(n_spikes / 25))  # number of full rows\n",
    "\n",
    "    remaining = int(n_spikes - n_rows * 25)\n",
    "    cut_string = ('%3u' * 25 + '\\n') * n_rows + '%3u' * remaining\n",
    "\n",
    "    write_list.append(cut_string % (tuple(cut)))\n",
    "\n",
    "    with open(cut_filename, 'w') as f:\n",
    "        f.writelines(write_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "07cc054b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From \n",
    "# https://github.com/GeoffBarrett/gebaSpike/blob/967097ec28592182ef9783d2d391930e1c63ca58/gebaSpike/core/writeCut.py\n",
    "\n",
    "def write_clu(clu_filename, data):\n",
    "    # the .clu files and the .cut files are different since the .clu files are the .cut files (with no manual sorting)\n",
    "    # without the headers, and the values go from 1 -> N instead of 0 -> N, (1-based numbering instead of 0-based). Thus\n",
    "    # we add 1 to the .cut data to get the .clu data\n",
    "\n",
    "    data = np.asarray(data).astype(int)  # ensuring that the data is the integer data-type\n",
    "\n",
    "    data += 1  # making the data 1-based instead of 0-based\n",
    "\n",
    "    # calculating the number of clusters\n",
    "    n_clust = len(np.unique(data))\n",
    "\n",
    "    # ensuring that the cluster number is the 1st value\n",
    "    data = np.concatenate(([n_clust], data))\n",
    "\n",
    "    # saving the data as a column (delimter='\\n') and integer format.\n",
    "    np.savetxt(clu_filename, data, fmt='%d', delimiter='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230e729d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bfa970",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ff0753",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc976e55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc8c7b45",
   "metadata": {},
   "source": [
    "<a id=\"Convert_recording_extractor_to_tint\"></a>\n",
    "## Convert Recording extractor to TINT\n",
    "[back to index](#index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852eb64d",
   "metadata": {},
   "source": [
    "Hmm, since we are writing to TINT, thereby creating `.X` tetrode files, we throw away all information in-between spikes. There is no point to convert the fake continuous recording used for spike sorting to TINT at all. We really only want to export the spike sorting output!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a3c847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anything to do here?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d90289",
   "metadata": {},
   "source": [
    "<a id=\"Convert_sorting_extractor_to_tint\"></a>\n",
    "## Convert Sorting extractor to TINT\n",
    "[back to index](#index)\n",
    "\n",
    "There are several points in the pipeline at which we might want to export to TINT. Ideally it should work for any `SortingExtractor` object!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7fe21ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where do we load data from?\n",
      "\n",
      " /mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/sample_bin_to_tint_no_bin\n"
     ]
    }
   ],
   "source": [
    "print('Where do we load data from?\\n\\n', dir_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae069b09",
   "metadata": {},
   "source": [
    "From a sorting extractor we can obtain a list unit spike sample arrays. We can convert this to the .clu or .cut type array of unit ID labels for each spike.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc1e882",
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_filename = Path('/mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/Axona_Tint_1ms/20201004_Tint_1.cut')\n",
    "\n",
    "basename = os.path.basename(os.path.splitext(cut_filename)[0])\n",
    "\n",
    "print(basename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "8bae0e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/Axona_Tint_1ms/20201004_Tint.set\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('/mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/Axona_Tint_1ms/20201004_Tint_1.cut')"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = Path('/mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/Axona_Tint_1ms/20201004_Tint.set')\n",
    "print(filename)\n",
    "\n",
    "Path(str(filename.with_suffix('')) + '_{}'.format(1) + '.cut')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de62d228",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2270a472",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_spike_train_to_label_array(spike_train):\n",
    "    '''Takes a list of arrays, where each array is a series of\n",
    "    sample points at which a spike occured for a given unit\n",
    "    (each list item is a unit). Converts to .cut array, i.e.\n",
    "    orders spike samples from all units and labels each sample\n",
    "    with the appropriate unit ID.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    spike_train : List of np.arrays\n",
    "        Output of `get_units_spike_train()` method of sorting extractor\n",
    "        \n",
    "    Return\n",
    "    ------\n",
    "    unit_labels_sorted : np.array\n",
    "        Each entry is the unit ID corresponding to the spike sample that\n",
    "        occured at this ordinal position\n",
    "    '''\n",
    "\n",
    "    # Generate Index array (indexing the unit for a given spike sample)\n",
    "    unit_labels = []\n",
    "    for i, l in enumerate(spike_train):\n",
    "        unit_labels.append(np.ones((len(l),), dtype=int) * i)\n",
    "    \n",
    "    # Flatten lists and sort them\n",
    "    spike_train_flat = np.concatenate(spike_train).ravel()\n",
    "    unit_labels_flat = np.concatenate(unit_labels).ravel()\n",
    "\n",
    "    sort_index = np.argsort(spike_train_flat)\n",
    "\n",
    "    unit_labels_sorted = unit_labels_flat[sort_index]\n",
    "\n",
    "    return unit_labels_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8215d02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_cut_file(cut_filename, unit_labels):\n",
    "    '''Write spike sorting output to .cut file.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    cut_filename : str or Path\n",
    "        Full filename of .cut file to write to. A given .cut file belongs\n",
    "        to a given tetrode file. For example, for tetrode `my_file.1`, the\n",
    "        corresponding cut_filename should be `my_file_1.cut`.\n",
    "    unit_labels : np.array\n",
    "        Vector of unit labels for each spike sample (ordered by time of \n",
    "        occurence)\n",
    "        \n",
    "    Example\n",
    "    -------\n",
    "    # Given a sortingextractor called sorting_nwb:\n",
    "    spike_train = sorting_nwb.get_units_spike_train()\n",
    "    unit_labels = convert_spike_train_to_label_array(spike_train)\n",
    "    write_to_cut_file(cut_filename, unit_labels)\n",
    "    \n",
    "    ---\n",
    "    Largely based on gebaSpike implementation by Geoff Barrett\n",
    "    https://github.com/GeoffBarrett/gebaSpike\n",
    "    '''\n",
    "\n",
    "    unique_cells = np.unique(unit_labels)\n",
    "\n",
    "    n_clusters = len(np.unique(unit_labels))\n",
    "    n_spikes = len(unit_labels)\n",
    "\n",
    "    write_list = []\n",
    "\n",
    "    tab = '    '\n",
    "    empty_space = '               '\n",
    "\n",
    "    write_list.append('n_clusters: %d\\n' % (n_clusters))\n",
    "    write_list.append('n_channels: 4\\n')\n",
    "    write_list.append('n_params: 2\\n')\n",
    "    write_list.append('times_used_in_Vt:%s' % ((tab + '0') * 4 + '\\n'))\n",
    "\n",
    "    zero_string = (tab + '0') * 8 + '\\n'\n",
    "\n",
    "    for cell_i in np.arange(n_clusters):\n",
    "        write_list.append(' cluster: %d center:%s' % (cell_i, zero_string))\n",
    "        write_list.append('%smin:%s' % (empty_space, zero_string))\n",
    "        write_list.append('%smax:%s' % (empty_space, zero_string))\n",
    "    write_list.append('\\nExact_cut_for: %s spikes: %d\\n' % (basename, n_spikes))\n",
    "\n",
    "    # The unit label array consists of 25 values per row in .cut file\n",
    "    n_rows = int(np.floor(n_spikes / 25))\n",
    "    remaining = int(n_spikes - n_rows * 25)\n",
    "\n",
    "    cut_string = ('%3u' * 25 + '\\n') * n_rows + '%3u' * remaining\n",
    "\n",
    "    write_list.append(cut_string % (tuple(unit_labels)))\n",
    "\n",
    "    with open(cut_filename, 'w') as f:\n",
    "        f.writelines(write_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "2487be27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_clu_file(clu_filename, unit_labels):\n",
    "    ''' .clu files are pruned .cut files, containing only a long vector of unit\n",
    "    labels, which are 1-indexed, instead of 0-indexed. In addition, the very first\n",
    "    entry is the total number of units.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    clu_filename : str or Path\n",
    "        Full filename of .clu file to write to. A given .clu file belongs\n",
    "        to a given tetrode file. For example, for tetrode `my_file.1`, the\n",
    "        corresponding clu_filename should be `my_file_1.clu`.\n",
    "    unit_labels : np.array\n",
    "        Vector of unit labels for each spike sample (ordered by time of \n",
    "        occurence)\n",
    "        \n",
    "    ---\n",
    "    Largely based on gebaSpike implementation by Geoff Barrett\n",
    "    https://github.com/GeoffBarrett/gebaSpike\n",
    "    '''\n",
    "    unit_labels = np.asarray(unit_labels).astype(int)\n",
    "    unit_labels += 1\n",
    "\n",
    "    n_clust = len(np.unique(unit_labels))\n",
    "    unit_labels = np.concatenate(([n_clust], unit_labels))\n",
    "\n",
    "    np.savetxt(clu_filename, unit_labels, fmt='%d', delimiter='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "995d9aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_cut_filename_from_basename(filename, tetrode_id):\n",
    "    '''Given a str or Path object, assume the last entry after a slash\n",
    "    is a filename, strip any file suffix, add tetrode ID label, and\n",
    "    .cut suffix to name.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : str or Path\n",
    "    tetrode_id : int\n",
    "    '''\n",
    "    return Path(str(filename).split('.')[0] + '_{}'.format(tetrode_id) + '.cut')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "fadbd312",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_unit_labels_to_file(sorting_extractor, filename):\n",
    "    '''Write spike sorting output to .cut file, separately for each\n",
    "    tetrode.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sorting_extractor : spikeextractors.SortingExtractor\n",
    "    filename : str or Path\n",
    "        Full filename of .set file or base-filename (i.e. the part of the\n",
    "        filename all Axona files have in common). A given .cut file belongs\n",
    "        to a given tetrode file. For example, for tetrode `my_file.1`, the\n",
    "        corresponding cut_filename should be `my_file_1.cut`. This will be\n",
    "        set automatically given the base-filename or set file.\n",
    "        \n",
    "    TODO: Any reason one might want to only convert some tetrodes or some\n",
    "    samples? Should those be parameters?\n",
    "    '''\n",
    "    tetrode_ids = sorting_extractor.get_units_property(property_name='group')\n",
    "    tetrode_ids = np.array(tetrode_ids)\n",
    "    \n",
    "    unit_ids = np.array(sorting_extractor.get_unit_ids())\n",
    "    \n",
    "    for i in np.unique(tetrode_ids):\n",
    "        \n",
    "        print('Converting Tetrode {}'.format(i))\n",
    "\n",
    "        spike_train = sorting_extractor.get_units_spike_train(unit_ids=unit_ids[tetrode_ids==i])\n",
    "        unit_labels = convert_spike_train_to_label_array(spike_train)\n",
    "\n",
    "        # We use Axona conventions for filenames (tetrodes are 1 indexed)\n",
    "        cut_filename = set_cut_filename_from_basename(filename, i + 1)\n",
    "        clu_filename = Path(str(cut_filename).replace('.cut', '.clu'))\n",
    "\n",
    "        write_to_cut_file(cut_filename, unit_labels)\n",
    "        write_to_clu_file(clu_filename, unit_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "001d00c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spikeextractors.extractors.nwbextractors.nwbextractors.NwbSortingExtractor'>\n"
     ]
    }
   ],
   "source": [
    "# We also have sorting data exported in `.nwb` format\n",
    "\n",
    "nwb_dir = Path(dir_name, 'nwb')\n",
    "sorting_nwb = se.NwbSortingExtractor(nwb_dir / 'axona_se_MS4.nwb', sampling_frequency=48000)\n",
    "\n",
    "print(type(sorting_nwb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "45c9d08a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling frequency: 48000 Hz\n"
     ]
    }
   ],
   "source": [
    "print('Sampling frequency:', sorting_nwb.get_sampling_frequency(), 'Hz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "e0ddef20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting Tetrode 0\n",
      "Converting Tetrode 1\n",
      "Converting Tetrode 2\n",
      "Converting Tetrode 3\n"
     ]
    }
   ],
   "source": [
    "# Convert all tetrodes from sorting extractor to cut files\n",
    "write_unit_labels_to_file(sorting_nwb, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "1dea0450",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_tetrode_file(sorting_extractor, save_dir):\n",
    "    '''Given a sorting extractor object create .X (tetrode) files.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sorting_extractor : spikeextractors.SortingExtractor\n",
    "    save_dir : str or Path\n",
    "        Directory where to save the output\n",
    "    '''\n",
    "    # TODO ...\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "1e81b2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_tint(sorting_extractor, filename):\n",
    "    '''Given a sorting extractor object, write appropriate data\n",
    "    to TINT format (from Axona). Will therefore create .X (tetrode),\n",
    "    .cut and .clu (spike sorting information) files.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sorting_extractor : spikeextractors.SortingExtractor\n",
    "    filename : str or Path\n",
    "        Full path and base filename shared by all output files \n",
    "        (e.g. my_dir/my_file will yield\n",
    "        my_dir/my_file.1, my_dir/my_file.2, ..., \n",
    "        my_dir/my_file_1.cut, my_dir/my_file_2.cut, ...,\n",
    "        my_dir/my_file_1.clu, my_dir/my_file_2.clu, ...)\n",
    "        If a file extension is given, it is simply ignored.\n",
    "        \n",
    "    Notes\n",
    "    -----\n",
    "    For details about the .X file format see:\n",
    "    http://space-memory-navigation.org/DacqUSBFileFormats.pdf\n",
    "    '''\n",
    "    # Make sure directory exists\n",
    "    filename.parent.absolute().mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # writes to .X files for each tetrode\n",
    "    # TODO...\n",
    "    write_to_tetrode_file(sorting_extractor, filename)\n",
    "    \n",
    "    # writes to .cut and .clu files for each tetrode\n",
    "    write_unit_labels_to_file(sorting_extractor, filename)\n",
    "    \n",
    "    # Position data?\n",
    "    # TODO ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "048d6928",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = Path(\n",
    "    '/mnt/d/freelance-work/catalyst-neuro/hussaini-lab-to-nwb/Axona_Tint_1ms/spikeextractors_to_tint/20201004_Tint'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "84ce7c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting Tetrode 0\n",
      "Converting Tetrode 1\n",
      "Converting Tetrode 2\n",
      "Converting Tetrode 3\n"
     ]
    }
   ],
   "source": [
    "write_to_tint(sorting_nwb, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15691ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4051f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490f3329",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5693d63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e684fd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db6c27f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spikeinterface",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
